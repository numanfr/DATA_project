{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d499846",
   "metadata": {},
   "source": [
    "# Joint Project Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0f1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea2514",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb36ba",
   "metadata": {},
   "source": [
    "Our project was created to solve the problem of how we can predict volatility accurately to be able to price options using the Black-Scholes model. This required us to innovate through the use of our domain knowledge in finance and the trade-off between accuracy and explainability in consideration of our target audience. \n",
    "\n",
    "The main findings were as follows. T_SNE reduced visualisation, with comparison of models using denormalised and normal data showing both limitations and a significant result as testing by t-test and anova post-hoc. Ensemble improves generalisation ability and limits error spread, evidenced by comparing the number of detected outliers.\n",
    "\n",
    "\n",
    "(FIGURE HERE)\n",
    "\n",
    "In terms of practical relevance, we believe that our analysis is insightful for academics and a more general audience such as fellow students and people without a finance background. However, we note that our results may be limited in its ability to provide insights for traders and people with a strong finance background or professionals in the field of finance, as the results we have yielded remain limited by the scope of our project having limited knowledge of the stocks we are analysing and the lack of applicability to real-world situations as the analysis is performed in a controlled environment where many factors determining volatility are not considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341f3ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbcf5358",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692a752",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "Volatility is a critical component of investing and trading, hence the importance of accurately forecasting volatility cannot be understated. Various papers have proposed different methodologies to predict volatility, with the need for a trade-off between accuracy and explainability considered in our report. Due to our target audience, it was important for us to not pick a method which would be highly accurate but difficult to convey to our audience on its conceptual and fundamental meaning, but also not to use over-simplistic methods which yield poor results.\n",
    "\n",
    "Various methods we considered are as follows. A novel deep neural network model with temporal embedding for volatility forecasting, termed HDNN, proposes an end-to-end learning model for volatility forecasting (Chen, Yao, & Shao, 2023). Despite its improvement from the ARCH model, the difficult interpretability of the model is a pitfall. Another model, using ARCH, criticises one of the most widely used risk measures: value-at-risk (VaR), as having a shortfall of focusing on short-run risk and is often applied to measuring the risk of long-horizon assets (Brownlees, Engle, & Kelly, 2011). We took this into consideration when selecting our proxies for volatility in our clustering. A general approach to testing volatility models in time series requires consistent parameter estimation, optimal volatility forecasting, valid hypothesis testing and economic interpretations, which rely on the correct specification of volatility models, hence justifying the importance of specificity in our model selection and why we should avoid an over-simplistic model (Hong & Lee, 2017). \n",
    "\n",
    "The application of our predicted volatility lies primarily in the Black-Scholes model, an option pricing model which uses a stock’s strike price, current price, time to maturity, the risk-free rate and the volatility to propose a fair price in the absence of market arbitrage opportunities or riskless profits. The model is formulated in continuous time and assumes that the risk-free interest rate, continuously compounded, is constant over time and that the price of the underlying stock follows a continuous stochastic process with a constant relative volatility. The volatility of a stock can be estimated from historical variations in the stock price and the estimate varies with the time period used in the estimation—both over short periods and long periods. Another measure of the volatility of a stock is its implied volatility (Munk, 2007). Given the current stock price and interest rate, an implied volatility of the stock for any exercise price and any maturity can be defined as the value of volatility plugged into the Black- Scholes formula to get a match with the observed market price of the option (Munk, 2007).\n",
    "\n",
    "One of our key selections of proxies for volatility is the beta of a stock, which is calculated as the covariance of the stock and the market divided by the variance of the market. The beta is an appropriate estimator as it is the measurement of the systematic risk of a stock relative to the market, in other words, how much riskier the stock is relative to the market. Stocks with a high beta will demand a higher premium due to its greater risk, while stocks with a low beta will have a lower required rate of return. Therefore, the use of beta, backed by our understanding of its financial theory, is a key component of our innovation in applying finance to a data science project.\n",
    "\n",
    "We created a project to predict volatility for a range of stocks based on specific characteristics. We sought to capture volatility across 126 different stocks, predicting volatility through many time intervals. Our clustering was done across numerous domain features including beta, DoM and spread. We visualised our results through a Shiny App for our target audience. Our motivation stemmed from our financial understanding of volatility and using this as our guide. This includes the beta, which captures the covariance of a stock with the market. We also studied a range of techniques for our prediction and evaluation and came up with the best ideas based on a tradeoff in accuracy and explainability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10baf8",
   "metadata": {},
   "source": [
    "###  Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0d22f",
   "metadata": {},
   "source": [
    "Our objective is four fold. Firstly, to optimise clustering by looking at the tradeoff between accuracy and explainability to create our features for the clusters. We sought to make the most accurate predictions, hence our prediction was done through an ARMA-GARCH model, linear regression and a LightGBM model. We represented communication through displaying both the stock features and their volatility predictions in the Shiny App.\n",
    "\n",
    "We created clusters from multiple clustering algorithms and used them as a feature for our LightGBM model. Our prediction shows that our linear regression and ARMA-GARCH models had very low predictive powers for such a short time horizon, and Light GBM had decent predictive abilities. We therefore weighted them accordingly based on their significance.\n",
    " \n",
    "We attempted to classify the time ids chronologically so that we could have more data to train the model. We did this by calculating the distance between the first second of each time_id and the last second of every other time id and connecting the time ids with the lower distance. The classification did not yield satisfactory results on testing until we found a way to denormalize the time ids using the assumption that the minimum deviation in each stock’s price is 1 cent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a548618",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e79fd4",
   "metadata": {},
   "source": [
    "We employ a nested cross validation approach for evaluating our models. The motivation is to produce a stable estimate in assessing performance that is not dependent upon how the data is split. In hold-out CV it is possible that test distributions differ from training distributions, which violates key assumptions that training, and test data come from the same distributions. Hence, this method ensures an unbiased estimate of performance. \n",
    " \n",
    "Our error metric is Root Mean Squared Percentage Error. Similar to RMSE, this metric weighs larger errors higher due to squaring before averaging, and by dividing the difference with true values to obtain the error ratio. This further penalizes errors and is suitable for when large errors are undesired. For inner fold validation, RMSPE is averaged across inner folds, and test performance is evaluated by generating predictions for all data in the out fold and directly calculating RMSPE. This is beneficial two-fold: (1) we infer a stable estimate of performance as seen by the proximity of calculated RMSPEs; and (2) we generate predictions on the entire dataset without training bias (which simulates performance on unseen data).\n",
    " \n",
    "Lastly, feature importance in LightGBM models is calculated with split (on right), by counting frequency each feature is used, or with gain (on left), by counting total gains of splits using each feature. Gains is an info-theoretic measure of difference in entropy after each split using a feature in the decision tree.\n",
    " \n",
    "Satisfyingly, many of our innovative features are used frequently – this includes beta, and labels from clustering on many engineered features (with K-means, DBSCAN and GMM). This shows the summative power of dimensionality reduction and how intrinsic assumptions belonging to each clustering algorithm, can help encode similarities or summarize relations between observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6d511",
   "metadata": {},
   "source": [
    "### Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2895c017",
   "metadata": {},
   "source": [
    "Limitations to our methods and results are as follows. Firstly, weaknesses in the ability of our model’s predictiveness can be improved through using a class of new diagnostic tests for volatility models in time series, where the volatility models can be linear or non-linear. This method can detect a wide range of model misspecifications in volatility models, while being robust to the higher order time‐varying moments of unknown form, such as skewness and kurtosis (Hong & Lee, 2017). The test can detect the neglected nonlinear volatility dynamics that existing tests can easily miss (Hong & Lee, 2017). However, while this method has been noted to be an improvement to the accuracy due to its incorporation of non-linear volatility factors, it still has a significant trade-off between explainability which makes it difficult to justify the use of the model in our methods, hence why we have framed the model around our discussion (Hong & Lee, 2017).\n",
    "\n",
    "Another key limitation of the model is the exclusion of white noise in constructing our model, a concept which states that each element has an identical, independent, mean-zero distribution which is used to model time series, being of significance because fluctuations in most economic time series tend to persist over time, so elements near each other in time are correlated (Neusser, 2016). \n",
    "\n",
    "Another limitation is the critique in the use of beta and the way we estimated our beta. A key message from Tofallis is that OLS regression lines are not intended to represent an underlying relationship between two variables, meaning the use of OLS regression in estimating the beta systematic risk coefficient may be inappropriate (Tofallis, n.d.). Instead, the paper proposes an estimator that is a measure of total risk which can be applied to all stocks and portfolios, allowing for consistency between its standard interpretation as relative volatility and the formula used for its calculation (Tofallis, n.d.). However, this method once again sacrifices explainability for a more accurate estimator of risk, and we have chosen not to incorporate Tofallis’s model in favour of a simpler, universally accepted method of estimated the beta using OLS regression (Tofallis, n.d.). \n",
    "\n",
    "The topic of beta also poses the discussion of whether betas are related to market volatility. The fundamental use of beta in finance is related to the capital asset pricing model (CAPM), which is a simple but widely accepted formula for determining the cost of equity using the risk free rate, market risk premium and beta, and which assumes the return on equity is directly proportional to the beta of the stock. A study by Chakrabarti and Das highlights the inefficacy of constant beta CAPM to capture the true market risk of assets as the betas vary over different phases of price movement of the asset concerned (Chakrabarti & Das, 2021). The model's inefficiency in dynamic forecasting further complicates the issue. Hence, while it is important to make asset betas time dependent, estimating the constant betas separately over different phases and their comparison would lead to further inefficiency, making an error in estimating the true market risk of the asset inevitable (Chakrabarti & Das, 2021). This limitation in the constant beta across time will yield weaker results, hence an improvement to our current model would be to substitute the constant beta with a time-dependent beta (Chakrabarti & Das, 2021). In our analysis, we also tried to reverse engineer the beta using the stock’s mean return, a proxy for risk-free rate being the 10-year government bond, and a proxy for the market return being the ASX300 to get to the market risk premium, but we found this method to be incapable of producing a robust and reasonable beta, therefore it was excluded from our clustering methods.\n",
    "\n",
    "In the future, we see three ways that our model can be improved. Firstly, the aforementioned use of a dynamic beta instead of a static beta, which is likely to yield more accurate results without a significant effect on explainability. We believe this method of using beta is more in line with a time-variant CAPM, in other words, that changing market conditions are better reflected in our proxy for risk. Another improvement to our model is to consider the inclusion of a white-noise process, which is extremely useful as a building block for modeling the time-series behavior of serially correlated processes. Although most economic time-series are not white noise, any series can be decomposed into predictable and unpredictable components, where the latter is the fundamental underlying white-noise process of the series. Finally, an improvement can be made to our predictive capabilities. As we are currently predicting a five minute time bucket for future volatility, we can expand our predictive capabilities across a longer time frame, such as extending to predicting a future 10-minute time interval, as well as creating predictions across different time IDs. This could be used to test the strength and robustness of our model, and further justify the importance of the prior suggested improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9f656",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6852dc",
   "metadata": {},
   "source": [
    "Our project was designed to predict volatility across different stocks based on specific characteristics, including the application of beta, a financial concept, in our clustering, along with DoM and spread as features of our LightGBM model. LightGBM had the best predictive abilities compared to ARMA-GARCH and linear regression. The clustering technique was optimised through a tradeoff between accuracy and explainability and our finance domain knowledge. We visualised our results through an interactive Shiny app which we believe is most appropriate for our intended audience. Many of our innovative features were used frequently which shows the summative power of dimensionality reduction.\n",
    "\n",
    "Based on our conclusion, future work could incorporate a better estimator of risk through a time-varying beta, and possibly exploring a different technique to estimate beta beyond OLS regression, which is simple to explain but less accurate. Our model can be extended to assess how it performs across not just different stocks, but different time IDs, which can be used to justify the robustness of our method and results. We also examined that the inclusion of white-noise processes could be used to model the time-series behaviour in unpredictable factors. Finally, we consider how we will be using this volatility in th Black-Scholes formula. As the Black-Scholes is only one of many methods which can be used to price options, the strength of our volatility predictions can be compared through analysing the implied volatility from other methods which are used to price options, such as the binomial model and Monte-Carlo simulation, to see if the volatility stands when the option pricing model changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4b811",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea3ca37",
   "metadata": {},
   "source": [
    "1. Brownlees, C., Engle, R., Kelly, B. (2011). A practical guide to volatility forecasting through calm and storm. The Journal of Risk, 14(2), 3-22. https://www.sas.upenn.edu/~fdiebold/papers/misc/Brownlees.pdf\n",
    "\n",
    "\n",
    "2. Chakrabarti, G., Dias, R. (2021). Time-varying beta, market volatility and stress:\n",
    "A comparison between the United States and India. IIMB Management Review, 33, 50-63. https://doi.org/10.1016/j.iimb.2021.03.003\n",
    "\n",
    "\n",
    "3. Chen, W., Yao, J., Shao, Y.  (2023). Volatility forecasting using deep neural network with time-series feature embedding. Economic Research-Ekonomska Istrazivanja, 36(1), 1377-1401. https://doi.org/10.1080/1331677X.2022.2089192\n",
    "\n",
    "\n",
    "4. Hong, Y., Lee, Y. (2017). A general approach to testing volatility models in time series. JMSE, 2(1), 1-33. doi:10.3724/SP.J.1383.201001\n",
    "\n",
    "\n",
    "5. Munk, C. (2007). Financial asset pricing theory. Oxford University Press.\n",
    "\n",
    "\n",
    "6. Neusser, K. (2016). Time Series Econometrics. Springer.\n",
    "\n",
    "\n",
    "7. Tofallis, C. (n.d.). Investment volatility: A critique of standard beta estimation and a simple way forward. Department of Management Systems, University of Hetfordshire, 1-16. https://arxiv.org/pdf/1109.4422.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b9af1",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f36668",
   "metadata": {},
   "source": [
    "## Student Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081870a",
   "metadata": {},
   "source": [
    "Student | Contribution\n",
    "_ _\n",
    "Michael |\n",
    "_ _\n",
    "\n",
    "Numan |\n",
    "_\n",
    "\n",
    "Harry |\n",
    "Harry was responsible for helping prepare and present weekly updates and working to brainstorm ideas with the group during weekly meetings. He has worked on designing and creating the Shiny app, along with incorporating the model, results and communication into a PowerPoint slide deck and a script for the final presentation. In the report, Harry was responsible for writing up the executive summary, background, discussion and conclusion, along with producing relevant graphics and integrating the final report through editing and reviewing.\n",
    "_\n",
    "Cody |\n",
    "\n",
    "_\n",
    "Yidan |\n",
    "_\n",
    "\n",
    "Yimeng |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d489956",
   "metadata": {},
   "source": [
    "| Student      | Unikey | Contribution     |\n",
    "| :---:        |    :----:   |          :--- |\n",
    "| Harry     |       | Harry was responsible for helping prepare and present weekly updates and working to brainstorm ideas with the group during weekly meetings. He has worked on designing and creating the Shiny app, along with incorporating the model, results and communication into a PowerPoint slide deck and a script for the final presentation. In the report, Harry was responsible for writing up the executive summary, background, discussion and conclusion, along with producing relevant graphics and integrating the final report through editing and reviewing.  | \n",
    "| Michael   | Text        | And more      |\n",
    "| Numan   | Text        | And more      |\n",
    "| Cody   | Text        | And more      |\n",
    "| Yidan   | Text        | And more      |\n",
    "| Yimeng   | Text        | And more      |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
