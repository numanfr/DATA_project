{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39499a8",
   "metadata": {},
   "source": [
    "### Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c533be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "import matplotlib\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "pd.options.display.max_rows = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21b116",
   "metadata": {},
   "source": [
    "### Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ff35391",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = pd.read_feather(\"beta/betas_condensed.fth\")\n",
    "spread_dom = pd.read_csv(\"all_times.csv\").drop([\"beta\"],axis=1)\n",
    "DENORM = pd.read_csv(\"denormalize_multipliers.csv\")\n",
    "\n",
    "def mad(data):\n",
    "    return np.mean(np.absolute(data - np.mean(data)))\n",
    "\n",
    "def import_beta(df):\n",
    "    global beta\n",
    "    global spread_dom\n",
    "    \n",
    "    df = pd.merge(df, beta, how=\"left\")\n",
    "    df = pd.merge(df, spread_dom, how=\"left\")\n",
    "    return df\n",
    "    \n",
    "def compute_wap(df):\n",
    "    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \\\n",
    "          /(df['bid_size1'] + df['ask_size1'])\n",
    "\n",
    "def compute_wap2(df):\n",
    "    return (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \\\n",
    "           /(df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "def compute_dom(df):\n",
    "    dom = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2']) \\\n",
    "            + (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])\n",
    "    return dom\n",
    "\n",
    "def compute_dom_diff(df):\n",
    "    dom_diff = abs((df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2']) \\\n",
    "            - (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2']))\n",
    "    return dom_diff\n",
    "\n",
    "def fill_seconds(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    index_range = pd.Index(range(600), name='seconds_in_bucket')\n",
    "    df = df.set_index('seconds_in_bucket').reindex(index_range)\n",
    "    \n",
    "    # Forward fill & back fill seconds\n",
    "    df = df.ffill().reset_index()\n",
    "    return df.bfill().iloc[:600]\n",
    "\n",
    "def beta_encoding(df):\n",
    "    beta1 = np.repeat(np.nan, df.shape[0])\n",
    "    beta2 = np.repeat(np.nan, df.shape[0])\n",
    "    kf = KFold(n_splits = 112, shuffle=True,random_state = 0)\n",
    "    for idx_1, idx_2 in kf.split(df):\n",
    "        bmean1 = df.iloc[idx_1].groupby('stock_id')['beta'].mean()\n",
    "        bmean2 = df.iloc[idx_1].groupby('stock_id')['beta2'].mean()\n",
    "        beta1[idx_2] = df['stock_id'].iloc[idx_2].map(bmean1)\n",
    "        beta2[idx_2] = df['stock_id'].iloc[idx_2].map(bmean2)\n",
    "        \n",
    "    df['encode_mean_beta'] = beta1\n",
    "    df['encode_mean_beta2'] = beta2\n",
    "    \n",
    "    return df.drop(['beta','beta2'], axis=1)\n",
    "\n",
    "def import_clusters(df):\n",
    "    reduc_feats_label = pd.read_csv(\"pca10_clusts.csv\").iloc[:,1:]\n",
    "    bds_cluster_label = pd.read_csv(\"cluster_labels.csv\").iloc[:,1:]\n",
    "    som_cluster_label = pd.read_csv(\"SOM_clusters.csv\")\n",
    "    #feature_leakage = pd.read_csv(\"feats_to_cluster.csv\").iloc[:,1:]\n",
    "    \n",
    "    df = df.merge(reduc_feats_label, how=\"left\")\n",
    "    df = df.merge(bds_cluster_label, how=\"left\")\n",
    "    df = df.merge(som_cluster_label, how=\"left\")\n",
    "    #df = df.merge(feature_leakage, how=\"left\")\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def remove_clusters(df, n=13):\n",
    "    if \"bds_gmm6\" in df.columns:\n",
    "        return df.drop(df.columns[-n:], axis=1)\n",
    "    return df\n",
    "\n",
    "def target_encoding(df):\n",
    "    tenc = np.repeat(np.nan, df.shape[0])\n",
    "    senc = np.repeat(np.nan, df.shape[0])\n",
    "    denc = np.repeat(np.nan, df.shape[0])\n",
    "    tencs = np.repeat(np.nan, df.shape[0])\n",
    "    sencs = np.repeat(np.nan, df.shape[0])\n",
    "    dencs = np.repeat(np.nan, df.shape[0])\n",
    "    kf = KFold(n_splits = 112, shuffle=True,random_state = 0)\n",
    "    for idx_1, idx_2 in kf.split(df):\n",
    "        tmean = df.iloc[idx_1].groupby('stock_id')['target_realized_volatility'].mean()\n",
    "        smean = df.iloc[idx_1].groupby('stock_id')['spread'].mean()\n",
    "        dmean = df.iloc[idx_1].groupby('stock_id')['dom'].mean()\n",
    "        tmeans = df.iloc[idx_1].groupby('stock_id')['target_realized_volatility'].std()\n",
    "        smeans = df.iloc[idx_1].groupby('stock_id')['spread'].std()\n",
    "        dmeans = df.iloc[idx_1].groupby('stock_id')['dom'].std()\n",
    "        tenc[idx_2] = df['stock_id'].iloc[idx_2].map(tmean)\n",
    "        senc[idx_2] = df['stock_id'].iloc[idx_2].map(smean)\n",
    "        denc[idx_2] = df['stock_id'].iloc[idx_2].map(dmean)\n",
    "        tencs[idx_2] = df['stock_id'].iloc[idx_2].map(tmeans)\n",
    "        sencs[idx_2] = df['stock_id'].iloc[idx_2].map(smeans)\n",
    "        dencs[idx_2] = df['stock_id'].iloc[idx_2].map(dmeans)\n",
    "    df['target_mean_enc'] = tenc\n",
    "    df['spread_mean_enc'] = senc\n",
    "    df['dom_mean_enc'] = denc\n",
    "    df['target_std_enc'] = tencs\n",
    "    df['spread_std_enc'] = sencs\n",
    "    df['dom_std_enc'] = dencs\n",
    "    \n",
    "    return df.drop(['dom','spread'], axis=1)\n",
    "\n",
    "def generate_feature_seconds(features, df, feats, seconds=[300], overlap=1):\n",
    "    if overlap:\n",
    "        for second in seconds:\n",
    "            res = pd.DataFrame(\n",
    "                df.query(f'{seconds[0]-second} <= seconds_in_bucket < {seconds[0]}')\n",
    "                .groupby(['time_id']).agg(feats)\n",
    "            ).reset_index()\n",
    "            res.columns = ['_'.join(feat).rstrip('_') for feat in res.columns]\n",
    "            res = res.add_suffix('_' + str(second))\n",
    "            features = pd.merge(\n",
    "                features, res, \n",
    "                how='left', \n",
    "                left_on='time_id', \n",
    "                right_on=f'time_id_{second}'\n",
    "            )\n",
    "            features = features.drop([f'time_id_{second}'], axis=1)  \n",
    "    else:\n",
    "        for i, second in enumerate(seconds):\n",
    "            last_second = 0 if not i else seconds[i-1]\n",
    "            res = pd.DataFrame(\n",
    "                df.query(f'{last_second} <= seconds_in_bucket < {second}')\n",
    "                .groupby(['time_id']).agg(feats)\n",
    "            ).reset_index()\n",
    "            res.columns = ['_'.join(feat).rstrip('_') for feat in res.columns]\n",
    "            res = res.add_suffix('_' + str(second))\n",
    "            features = pd.merge(\n",
    "                features, res, \n",
    "                how='left', \n",
    "                left_on='time_id', \n",
    "                right_on=f'time_id_{second}'\n",
    "            )\n",
    "            features = features.drop([f'time_id_{second}'], axis=1)\n",
    "    return features\n",
    "\n",
    "def generate_target(df, target, second=300):\n",
    "    features = pd.DataFrame(\n",
    "        df.query(f'seconds_in_bucket >= {second}')\n",
    "        .groupby(['time_id']).agg(target)\n",
    "    ).reset_index()\n",
    "    \n",
    "    features.columns = ['_'.join(feat).rstrip('_') for feat in features.columns]\n",
    "    return features\n",
    "\n",
    "def calc_slope(df):\n",
    "    v0 = (df['bid_size1']+df['ask_size1'])/2\n",
    "    p0 = (df['bid_price1']+df['ask_price1'])/2\n",
    "    slope_bid = ((df['bid_size1']/v0)-1)/abs((df['bid_price1']/p0)-1)+(\n",
    "                (df['bid_size2']/df['bid_size1'])-1)/abs((df['bid_price2']/df['bid_price1'])-1)\n",
    "    slope_ask = ((df['ask_size1']/v0)-1)/abs((df['ask_price1']/p0)-1)+(\n",
    "                (df['ask_size2']/df['ask_size1'])-1)/abs((df['ask_price2']/df['ask_price1'])-1)\n",
    "    return (slope_bid+slope_ask)/2, abs(slope_bid-slope_ask)\n",
    "\n",
    "\n",
    "def calc_dispersion(df):\n",
    "    bspread = df['bid_diff']\n",
    "    aspread = df['ask_diff']\n",
    "    bmid = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price1']\n",
    "    bmid2 = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price2']\n",
    "    amid = df['ask_price1'] - (df['bid_price1'] + df['ask_price1'])/2\n",
    "    amid2 = df['ask_price2'] - (df['bid_price1'] + df['ask_price1'])/2\n",
    "    bdisp = (df['bid_size1']*bmid + df['bid_size2']*bspread)/(df['bid_size1']+df['bid_size2'])\n",
    "    bdisp2 = (df['bid_size1']*bmid + df['bid_size2']*bmid2)/(df['bid_size1']+df['bid_size2'])\n",
    "    adisp = (df['ask_size1']*amid + df['ask_size2']*aspread)/(df['ask_size1']+df['ask_size2'])      \n",
    "    adisp2 = (df['ask_size1']*amid + df['ask_size2']*amid2)/(df['ask_size1']+df['ask_size2'])\n",
    "    return (bdisp + adisp)/2, (bdisp2 + adisp2)/2\n",
    "\n",
    "def calc_price_impact(df):\n",
    "    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n",
    "    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n",
    "    return (df['ask_price1'] - ask)/df['ask_price1'], (df['bid_price1'] - bid)/df['bid_price1']\n",
    "\n",
    "\n",
    "def calc_ofi(df):\n",
    "    a = df['bid_size1']*np.where(df['bid_price1'].diff()>=0,1,0)\n",
    "    b = df['bid_size1'].shift()*np.where(df['bid_price1'].diff()<=0,1,0)\n",
    "    c = df['ask_size1']*np.where(df['ask_price1'].diff()<=0,1,0)\n",
    "    d = df['ask_size1'].shift()*np.where(df['ask_price1'].diff()>=0,1,0)\n",
    "    return a - b - c + d\n",
    "\n",
    "def calc_tt1(df):\n",
    "    p1 = df['ask_price1'] * df['ask_size1'] + df['bid_price1'] * df['bid_size1']\n",
    "    p2 = df['ask_price2'] * df['ask_size2'] + df['bid_price2'] * df['bid_size2']      \n",
    "    return p2 - p1 \n",
    "\n",
    "def log_returns(waps):\n",
    "    return np.log(waps).diff() \n",
    "\n",
    "def realized_volatility(log_returns):\n",
    "    return np.sqrt(np.sum(log_returns**2))\n",
    "\n",
    "def weighted_volatility(log_returns):\n",
    "    return np.sqrt(np.sum(log_returns**2)/log_returns.count())\n",
    "\n",
    "def quarticity(s):\n",
    "    return (s.count()/3) * np.sum(s**4)\n",
    "\n",
    "def denormalize(df):\n",
    "    global DENORM\n",
    "\n",
    "    df = df.merge(DENORM, how=\"left\")\n",
    "    df['ask_price1'] *= df['multiplier']\n",
    "    df['ask_price2'] *= df['multiplier']\n",
    "    df['bid_price1'] *= df['multiplier']\n",
    "    df['bid_price2'] *= df['multiplier']\n",
    "    \n",
    "    return df.drop(['multiplier'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b8b67",
   "metadata": {},
   "source": [
    "### Generate Features for First 5 Minutes of Each Stock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "359c0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(stock_id, df, extend, norm=False):\n",
    "    \n",
    "    if norm: df.groupby(\"time_id\").apply(fill_seconds)\n",
    "    \n",
    "    # compute waps for first and second ask/bids \n",
    "    df['wap'] = compute_wap(df)\n",
    "    df['wap2'] = compute_wap2(df)\n",
    "    \n",
    "    # compute log returns for realized volatility\n",
    "    df['log_returns'] = df.groupby('time_id')['wap'].apply(log_returns)\n",
    "    df['log_returns2'] = df.groupby('time_id')['wap2'].apply(log_returns)\n",
    "    \n",
    "    # compute difference in waps and price as feature\n",
    "    df['wap_diff'] = abs(df['wap'] - df['wap2'])\n",
    "    \n",
    "    # price difference regularized\n",
    "    df['price_diff'] = (df['ask_price1'] - df['bid_price1']) \\\n",
    "                        / ((df['ask_price1'] + df['bid_price1'])/2)\n",
    "    df['price_diff2'] = (df['ask_price2'] - df['bid_price2']) \\\n",
    "                        / ((df['ask_price2'] + df['bid_price2'])/2)\n",
    "    \n",
    "    # difference between first and second bid/ask price\n",
    "    df['bid_diff'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_diff'] = df['ask_price1'] - df['ask_price2']\n",
    "    \n",
    "    # sum of first and second ask sizes and bid sizes\n",
    "    df['bid_ask_volume'] = (df['ask_size1'] + df['ask_size2']) \\\n",
    "                        + (df['bid_size1'] + df['bid_size2'])\n",
    "    \n",
    "    # diff between ask sizes and bid sizes \n",
    "    df['bid_ask_volume_diff'] = abs((df['ask_size1'] + df['ask_size2']) \\\n",
    "                                - (df['bid_size1'] + df['bid_size2']))\n",
    "\n",
    "    # depth of market and difference\n",
    "    df['dom'] = compute_dom(df)\n",
    "    df['dom_diff'] = compute_dom_diff(df)\n",
    "    df['dom_imbalance'] = abs(df['dom'] - df['dom_diff'])\n",
    "    \n",
    "    # bid ask spread from lecture/lab\n",
    "    df[\"bid_ask_spread1\"] = df['ask_price1'] / df['bid_price1'] - 1\n",
    "    df[\"bid_ask_spread2\"] = df['ask_price2'] / df['bid_price2'] - 1\n",
    "    df['bid_ask_spread3'] = abs(df['bid_diff'] - df['ask_diff'])\n",
    "    \n",
    "    df[\"slope\"], _ = calc_slope(df)\n",
    "    df[\"dispersion\"], _ = calc_dispersion(df)\n",
    "    df[\"price_impact\"], _ = calc_price_impact(df)\n",
    "    df[\"ofi\"] = calc_ofi(df)\n",
    "    df[\"turn_over\"] = calc_tt1(df)   \n",
    "    \n",
    "    df['target'] =  df.groupby('time_id')['wap'].apply(log_returns)\n",
    "    \n",
    "    target_dict = {'target':[realized_volatility]}\n",
    "    feature_dict = {\n",
    "        'wap':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'wap2':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'log_returns':[realized_volatility, weighted_volatility, \n",
    "                       quarticity, np.mean],# np.std, np.max],\n",
    "        'log_returns2':[realized_volatility, weighted_volatility, \n",
    "                       quarticity, np.mean],# np.std, np.max],\n",
    "        'wap_diff':[np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'price_diff':[np.max, np.std], #np.max, np.sum],#, mad, np.max, np.sum],\n",
    "        'price_diff2':[np.max, np.std],#, np.max, np.sum],#, mad, np.max, np.sum],\n",
    "        'bid_diff':[np.max, np.std],#, np.max, np.sum],#, mad, np.max, np.sum],\n",
    "        'ask_diff':[np.max, np.std],#, np.max, np.sum],#, mad, np.max, np.sum],\n",
    "        'bid_ask_volume':[np.mean, np.std, np.max],#, np.sum],#, mad, np.max, np.sum],\n",
    "        'bid_ask_volume_diff':[np.mean, np.max],#, mad, np.max, np.sum],\n",
    "        'dom':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'dom_diff':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'dom_imbalance':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'bid_ask_spread1':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'bid_ask_spread2':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'bid_ask_spread3':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'slope':[np.mean, np.max, np.std],#, mad, np.max, np.sum],\n",
    "        'dispersion':[np.mean, np.max],#, mad, np.max, np.sum],\n",
    "        'price_impact':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'ofi':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'turn_over':[np.mean, np.std, np.max]#, mad, np.max, np.sum],\n",
    "    }\n",
    "    \n",
    "    # target realized volatility for next 300 second (5 min window in time_id)\n",
    "    feature = generate_target(\n",
    "        df, target=target_dict, \n",
    "        second=900 if extend else 300  # target volatility is for next 300 second (no overlap)\n",
    "    )\n",
    "    \n",
    "    # obtain features over entire 5 mins or last 2.5 mins..\n",
    "    \n",
    "    feature = generate_feature_seconds(\n",
    "        feature, df, \n",
    "        feats=feature_dict, \n",
    "        seconds=[900, 600, 300, 150, 75] if extend else [300, 150, 75],  # features generated for each second interval (different widths)\n",
    "        overlap=1,\n",
    "    )\n",
    "    #create stock_id\n",
    "    feature['stock_id'] = int(stock_id)\n",
    "    feature['time_id'] = feature['time_id'].apply(int)\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def concat_features(stock_id, df, extend=False, norm=False):\n",
    "    \n",
    "    if norm:\n",
    "        stock_df = pd.read_csv(f\"individual_book_train/stock_{stock_id}.csv\")\n",
    "        return pd.concat([df, generate_features(stock_id, stock_df, extend=extend, norm=norm)])\n",
    "    \n",
    "    stock_df = pd.read_feather(\n",
    "        f\"extend_two/stock_{stock_id}.fth\") if extend else pd.read_feather(\n",
    "                                    f\"extend_one/stock_{stock_id}.fth\")   \n",
    "    \n",
    "    return pd.concat([df, generate_features(stock_id, stock_df, extend=extend)])\n",
    "\n",
    "def process_stocks(stock_ids, extend=False, norm=False):\n",
    "    df = pd.DataFrame()\n",
    "    df = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(concat_features)(stock_id, df, extend=extend, norm=norm) for stock_id in stock_ids\n",
    "    )\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    \n",
    "    # lastly import pre-computed beta coefficients for all stock_ids\n",
    "    df = import_beta(df)\n",
    "    df = target_encoding(df)\n",
    "    df = beta_encoding(df)\n",
    "    df = import_clusters(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db988330",
   "metadata": {},
   "source": [
    "## Process Features for All Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca431fc",
   "metadata": {},
   "source": [
    "Loading Length1 TimeID (simple) stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf94f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 22s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954    194.495455     0.164908   \n",
       "1            11                    0.000981    199.598260     0.031047   \n",
       "2            16                    0.001295    209.021831     0.092861   \n",
       "3            31                    0.001776    216.281256     0.183025   \n",
       "4            62                    0.001520    214.542788     0.051133   \n",
       "...         ...                         ...           ...          ...   \n",
       "428927    32751                    0.002899    306.672174     0.163919   \n",
       "428928    32753                    0.003454    291.124934     0.147318   \n",
       "428929    32758                    0.002792    202.972820     0.064758   \n",
       "428930    32763                    0.002379    152.478929     0.068413   \n",
       "428931    32767                    0.001414    194.982143     0.040268   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0          194.479014      0.196558                             0.003394   \n",
       "1          199.597336      0.036259                             0.000699   \n",
       "2          209.053034      0.098250                             0.001983   \n",
       "3          216.198136      0.164985                             0.001863   \n",
       "4          214.524415      0.071793                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428927     306.730549      0.206509                             0.002284   \n",
       "428928     291.137380      0.164838                             0.002217   \n",
       "428929     202.958539      0.080424                             0.001386   \n",
       "428930     152.480008      0.075165                             0.002783   \n",
       "428931     195.012846      0.055999                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000132                2.117007e-10   \n",
       "428928                             0.000128                9.484441e-11   \n",
       "428929                             0.000080                3.031629e-11   \n",
       "428930                             0.000161                1.960849e-10   \n",
       "428931                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0               7.138250e-06  ...            1              4   \n",
       "1               8.823633e-07  ...            1              4   \n",
       "2               1.729093e-06  ...            1              4   \n",
       "3              -5.516464e-06  ...            1              4   \n",
       "4              -2.164288e-06  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "428927         -2.858852e-06  ...            1              2   \n",
       "428928          3.674218e-06  ...            1              2   \n",
       "428929         -2.437732e-06  ...            1              2   \n",
       "428930          6.737309e-06  ...            1              2   \n",
       "428931         -1.059089e-06  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428932 rows x 195 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "simple = process_stocks(beta.stock_id.unique(), extend=False)\n",
    "simple = simple.reset_index(drop=True)\n",
    "simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd58ae",
   "metadata": {},
   "source": [
    "Loading Length2 TimeID (extend2) stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df98c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 15s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_900</th>\n",
       "      <th>wap_std_900</th>\n",
       "      <th>wap2_mean_900</th>\n",
       "      <th>wap2_std_900</th>\n",
       "      <th>log_returns_realized_volatility_900</th>\n",
       "      <th>log_returns_weighted_volatility_900</th>\n",
       "      <th>log_returns_quarticity_900</th>\n",
       "      <th>log_returns_mean_900</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.523405</td>\n",
       "      <td>0.137002</td>\n",
       "      <td>194.494943</td>\n",
       "      <td>0.153934</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>3.455208e-09</td>\n",
       "      <td>-9.753985e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.519609</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>209.558521</td>\n",
       "      <td>0.383354</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.099042e-07</td>\n",
       "      <td>-2.355713e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>191.553810</td>\n",
       "      <td>0.364277</td>\n",
       "      <td>191.534358</td>\n",
       "      <td>0.383770</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>1.346483e-07</td>\n",
       "      <td>-3.936685e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>192.765232</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>192.735618</td>\n",
       "      <td>0.657042</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>1.127322e-06</td>\n",
       "      <td>-6.358684e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>192.219228</td>\n",
       "      <td>5.420681</td>\n",
       "      <td>192.208617</td>\n",
       "      <td>5.426199</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>3.980222e-03</td>\n",
       "      <td>6.443939e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213355</th>\n",
       "      <td>32748</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>191.609684</td>\n",
       "      <td>1.258293</td>\n",
       "      <td>191.604521</td>\n",
       "      <td>1.254466</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>6.904742e-06</td>\n",
       "      <td>2.088613e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213356</th>\n",
       "      <td>32750</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>244.526554</td>\n",
       "      <td>1.607117</td>\n",
       "      <td>244.538262</td>\n",
       "      <td>1.625978</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>5.504136e-06</td>\n",
       "      <td>-1.615979e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213357</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>308.076728</td>\n",
       "      <td>1.006311</td>\n",
       "      <td>308.107535</td>\n",
       "      <td>0.989751</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>5.021137e-07</td>\n",
       "      <td>-9.098024e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213358</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>199.448299</td>\n",
       "      <td>2.496420</td>\n",
       "      <td>199.399570</td>\n",
       "      <td>2.520482</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>1.302272e-04</td>\n",
       "      <td>2.968647e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213359</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>195.658217</td>\n",
       "      <td>0.521323</td>\n",
       "      <td>195.671518</td>\n",
       "      <td>0.508953</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>6.154658e-08</td>\n",
       "      <td>-4.053552e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213360 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_900  wap_std_900  \\\n",
       "0             5                    0.002954    194.523405     0.137002   \n",
       "1            16                    0.001295    209.519609     0.379075   \n",
       "2            72                    0.006357    191.553810     0.364277   \n",
       "3           146                    0.002993    192.765232     0.641515   \n",
       "4           169                    0.001498    192.219228     5.420681   \n",
       "...         ...                         ...           ...          ...   \n",
       "213355    32748                    0.003057    191.609684     1.258293   \n",
       "213356    32750                    0.001827    244.526554     1.607117   \n",
       "213357    32751                    0.002899    308.076728     1.006311   \n",
       "213358    32758                    0.002792    199.448299     2.496420   \n",
       "213359    32767                    0.001414    195.658217     0.521323   \n",
       "\n",
       "        wap2_mean_900  wap2_std_900  log_returns_realized_volatility_900  \\\n",
       "0          194.494943      0.153934                             0.004921   \n",
       "1          209.558521      0.383354                             0.005600   \n",
       "2          191.534358      0.383770                             0.007720   \n",
       "3          192.735618      0.657042                             0.011335   \n",
       "4          192.208617      5.426199                             0.060467   \n",
       "...               ...           ...                                  ...   \n",
       "213355     191.604521      1.254466                             0.013254   \n",
       "213356     244.538262      1.625978                             0.012686   \n",
       "213357     308.107535      0.989751                             0.007706   \n",
       "213358     199.399570      2.520482                             0.026106   \n",
       "213359     195.671518      0.508953                             0.006634   \n",
       "\n",
       "        log_returns_weighted_volatility_900  log_returns_quarticity_900  \\\n",
       "0                                  0.000164                3.455208e-09   \n",
       "1                                  0.000187                1.099042e-07   \n",
       "2                                  0.000257                1.346483e-07   \n",
       "3                                  0.000378                1.127322e-06   \n",
       "4                                  0.002017                3.980222e-03   \n",
       "...                                     ...                         ...   \n",
       "213355                             0.000442                6.904742e-06   \n",
       "213356                             0.000423                5.504136e-06   \n",
       "213357                             0.000257                5.021137e-07   \n",
       "213358                             0.000871                1.302272e-04   \n",
       "213359                             0.000221                6.154658e-08   \n",
       "\n",
       "        log_returns_mean_900  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0              -9.753985e-07  ...            1              4   \n",
       "1              -2.355713e-06  ...            1              4   \n",
       "2              -3.936685e-06  ...            1              4   \n",
       "3              -6.358684e-06  ...            1              4   \n",
       "4               6.443939e-05  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "213355          2.088613e-05  ...            1              2   \n",
       "213356         -1.615979e-05  ...            1              2   \n",
       "213357         -9.098024e-06  ...            1              2   \n",
       "213358          2.968647e-05  ...            1              2   \n",
       "213359         -4.053552e-06  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "213355                -1         2          1          1           3   \n",
       "213356                -1         2          1          1           3   \n",
       "213357                -1         2          1          1           3   \n",
       "213358                -1         2          1          1           3   \n",
       "213359                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "213355           12         1          1  \n",
       "213356           12         1          1  \n",
       "213357           12         1          1  \n",
       "213358           12         1          1  \n",
       "213359           12         1          1  \n",
       "\n",
       "[213360 rows x 307 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extend = process_stocks(beta.stock_id.unique(), extend=True)\n",
    "extend = extend.reset_index(drop=True)\n",
    "extend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d9f03",
   "metadata": {},
   "source": [
    "#### Comment out to save data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d27f8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#simple.to_feather(\"simple.fth\")\n",
    "#extend.to_feather(\"extend.fth\")\n",
    "#del simple\n",
    "#del extend "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53d0ad",
   "metadata": {},
   "source": [
    "#### Comment out to read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e897f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple = pd.read_feather(\"simple.fth\")\n",
    "#extend = pd.read_feather(\"extend.fth\")\n",
    "\n",
    "#del beta\n",
    "#del spread_dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff423ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates = pd.merge(simple, extend, how='inner', \n",
    "#                       left_on=['stock_id','time_id'], \n",
    "#                       right_on=['stock_id','time_id'])\n",
    "\n",
    "# simple = simple.drop(duplicates.index).reset_index(drop=True)\n",
    "# simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30172175",
   "metadata": {},
   "source": [
    "### Check for NaN in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ee1295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple[simple.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd41f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extend[extend.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc93ad7",
   "metadata": {},
   "source": [
    "### Removing row with zero realized volatilities\n",
    "\n",
    "* Corrupts RMSPE and MAPE metric scores\n",
    "    * Denominator is y_true\n",
    "    * Added small epsilon to prevent zerodivision\n",
    "    * Results in very large error if y_true = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc82f9",
   "metadata": {},
   "source": [
    "Note: Proportion is similar to that with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0873b28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple[simple['target_realized_volatility']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5643cba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extend[extend['target_realized_volatility']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e65c29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954    194.495455     0.164908   \n",
       "1            11                    0.000981    199.598260     0.031047   \n",
       "2            16                    0.001295    209.021831     0.092861   \n",
       "3            31                    0.001776    216.281256     0.183025   \n",
       "4            62                    0.001520    214.542788     0.051133   \n",
       "...         ...                         ...           ...          ...   \n",
       "428927    32751                    0.002899    306.672174     0.163919   \n",
       "428928    32753                    0.003454    291.124934     0.147318   \n",
       "428929    32758                    0.002792    202.972820     0.064758   \n",
       "428930    32763                    0.002379    152.478929     0.068413   \n",
       "428931    32767                    0.001414    194.982143     0.040268   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0          194.479014      0.196558                             0.003394   \n",
       "1          199.597336      0.036259                             0.000699   \n",
       "2          209.053034      0.098250                             0.001983   \n",
       "3          216.198136      0.164985                             0.001863   \n",
       "4          214.524415      0.071793                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428927     306.730549      0.206509                             0.002284   \n",
       "428928     291.137380      0.164838                             0.002217   \n",
       "428929     202.958539      0.080424                             0.001386   \n",
       "428930     152.480008      0.075165                             0.002783   \n",
       "428931     195.012846      0.055999                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000132                2.117007e-10   \n",
       "428928                             0.000128                9.484441e-11   \n",
       "428929                             0.000080                3.031629e-11   \n",
       "428930                             0.000161                1.960849e-10   \n",
       "428931                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0               7.138250e-06  ...            1              4   \n",
       "1               8.823633e-07  ...            1              4   \n",
       "2               1.729093e-06  ...            1              4   \n",
       "3              -5.516464e-06  ...            1              4   \n",
       "4              -2.164288e-06  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "428927         -2.858852e-06  ...            1              2   \n",
       "428928          3.674218e-06  ...            1              2   \n",
       "428929         -2.437732e-06  ...            1              2   \n",
       "428930          6.737309e-06  ...            1              2   \n",
       "428931         -1.059089e-06  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428791 rows x 195 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple = simple[simple['target_realized_volatility'] != 0]#.sort_values(['stock_id','time_id']).reset_index(drop=True)\n",
    "simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "661babcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_900</th>\n",
       "      <th>wap_std_900</th>\n",
       "      <th>wap2_mean_900</th>\n",
       "      <th>wap2_std_900</th>\n",
       "      <th>log_returns_realized_volatility_900</th>\n",
       "      <th>log_returns_weighted_volatility_900</th>\n",
       "      <th>log_returns_quarticity_900</th>\n",
       "      <th>log_returns_mean_900</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.523405</td>\n",
       "      <td>0.137002</td>\n",
       "      <td>194.494943</td>\n",
       "      <td>0.153934</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>3.455208e-09</td>\n",
       "      <td>-9.753985e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.519609</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>209.558521</td>\n",
       "      <td>0.383354</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.099042e-07</td>\n",
       "      <td>-2.355713e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>191.553810</td>\n",
       "      <td>0.364277</td>\n",
       "      <td>191.534358</td>\n",
       "      <td>0.383770</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>1.346483e-07</td>\n",
       "      <td>-3.936685e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>192.765232</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>192.735618</td>\n",
       "      <td>0.657042</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>1.127322e-06</td>\n",
       "      <td>-6.358684e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>192.219228</td>\n",
       "      <td>5.420681</td>\n",
       "      <td>192.208617</td>\n",
       "      <td>5.426199</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>3.980222e-03</td>\n",
       "      <td>6.443939e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213355</th>\n",
       "      <td>32748</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>191.609684</td>\n",
       "      <td>1.258293</td>\n",
       "      <td>191.604521</td>\n",
       "      <td>1.254466</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>6.904742e-06</td>\n",
       "      <td>2.088613e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213356</th>\n",
       "      <td>32750</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>244.526554</td>\n",
       "      <td>1.607117</td>\n",
       "      <td>244.538262</td>\n",
       "      <td>1.625978</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>5.504136e-06</td>\n",
       "      <td>-1.615979e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213357</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>308.076728</td>\n",
       "      <td>1.006311</td>\n",
       "      <td>308.107535</td>\n",
       "      <td>0.989751</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>5.021137e-07</td>\n",
       "      <td>-9.098024e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213358</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>199.448299</td>\n",
       "      <td>2.496420</td>\n",
       "      <td>199.399570</td>\n",
       "      <td>2.520482</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>1.302272e-04</td>\n",
       "      <td>2.968647e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213359</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>195.658217</td>\n",
       "      <td>0.521323</td>\n",
       "      <td>195.671518</td>\n",
       "      <td>0.508953</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>6.154658e-08</td>\n",
       "      <td>-4.053552e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213297 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_900  wap_std_900  \\\n",
       "0             5                    0.002954    194.523405     0.137002   \n",
       "1            16                    0.001295    209.519609     0.379075   \n",
       "2            72                    0.006357    191.553810     0.364277   \n",
       "3           146                    0.002993    192.765232     0.641515   \n",
       "4           169                    0.001498    192.219228     5.420681   \n",
       "...         ...                         ...           ...          ...   \n",
       "213355    32748                    0.003057    191.609684     1.258293   \n",
       "213356    32750                    0.001827    244.526554     1.607117   \n",
       "213357    32751                    0.002899    308.076728     1.006311   \n",
       "213358    32758                    0.002792    199.448299     2.496420   \n",
       "213359    32767                    0.001414    195.658217     0.521323   \n",
       "\n",
       "        wap2_mean_900  wap2_std_900  log_returns_realized_volatility_900  \\\n",
       "0          194.494943      0.153934                             0.004921   \n",
       "1          209.558521      0.383354                             0.005600   \n",
       "2          191.534358      0.383770                             0.007720   \n",
       "3          192.735618      0.657042                             0.011335   \n",
       "4          192.208617      5.426199                             0.060467   \n",
       "...               ...           ...                                  ...   \n",
       "213355     191.604521      1.254466                             0.013254   \n",
       "213356     244.538262      1.625978                             0.012686   \n",
       "213357     308.107535      0.989751                             0.007706   \n",
       "213358     199.399570      2.520482                             0.026106   \n",
       "213359     195.671518      0.508953                             0.006634   \n",
       "\n",
       "        log_returns_weighted_volatility_900  log_returns_quarticity_900  \\\n",
       "0                                  0.000164                3.455208e-09   \n",
       "1                                  0.000187                1.099042e-07   \n",
       "2                                  0.000257                1.346483e-07   \n",
       "3                                  0.000378                1.127322e-06   \n",
       "4                                  0.002017                3.980222e-03   \n",
       "...                                     ...                         ...   \n",
       "213355                             0.000442                6.904742e-06   \n",
       "213356                             0.000423                5.504136e-06   \n",
       "213357                             0.000257                5.021137e-07   \n",
       "213358                             0.000871                1.302272e-04   \n",
       "213359                             0.000221                6.154658e-08   \n",
       "\n",
       "        log_returns_mean_900  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0              -9.753985e-07  ...            1              4   \n",
       "1              -2.355713e-06  ...            1              4   \n",
       "2              -3.936685e-06  ...            1              4   \n",
       "3              -6.358684e-06  ...            1              4   \n",
       "4               6.443939e-05  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "213355          2.088613e-05  ...            1              2   \n",
       "213356         -1.615979e-05  ...            1              2   \n",
       "213357         -9.098024e-06  ...            1              2   \n",
       "213358          2.968647e-05  ...            1              2   \n",
       "213359         -4.053552e-06  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "213355                -1         2          1          1           3   \n",
       "213356                -1         2          1          1           3   \n",
       "213357                -1         2          1          1           3   \n",
       "213358                -1         2          1          1           3   \n",
       "213359                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "213355           12         1          1  \n",
       "213356           12         1          1  \n",
       "213357           12         1          1  \n",
       "213358           12         1          1  \n",
       "213359           12         1          1  \n",
       "\n",
       "[213297 rows x 307 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extend = extend[extend['target_realized_volatility'] != 0]#.sort_values(['stock_id','time_id']).reset_index(drop=True)\n",
    "extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca9b31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extend[extend.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "180007e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple[simple.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6ca89",
   "metadata": {},
   "source": [
    " * Same as NaN rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e53e7",
   "metadata": {},
   "source": [
    "### Defining X for LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8572b99",
   "metadata": {},
   "source": [
    "NaN is ignored by StandardScaler\n",
    "* 4th point: https://scikit-learn.org/stable/whats_new/v0.20.html#id37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54684aef",
   "metadata": {},
   "source": [
    "#### Drop target \"label\" of realized volatility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bbf7710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simpleX = simple.drop(['time_id','target_realized_volatility'], axis=1)  # leave stock id as feature \n",
    "extendX = extend.drop(['time_id','target_realized_volatility'], axis=1)  # leave stock id as feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3cc96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>log_returns2_realized_volatility_300</th>\n",
       "      <th>log_returns2_weighted_volatility_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wap_mean_300  wap_std_300  wap2_mean_300  wap2_std_300  \\\n",
       "0         194.495455     0.164908     194.479014      0.196558   \n",
       "1         199.598260     0.031047     199.597336      0.036259   \n",
       "2         209.021831     0.092861     209.053034      0.098250   \n",
       "3         216.281256     0.183025     216.198136      0.164985   \n",
       "4         214.542788     0.051133     214.524415      0.071793   \n",
       "...              ...          ...            ...           ...   \n",
       "428927    306.672174     0.163919     306.730549      0.206509   \n",
       "428928    291.124934     0.147318     291.137380      0.164838   \n",
       "428929    202.972820     0.064758     202.958539      0.080424   \n",
       "428930    152.478929     0.068413     152.480008      0.075165   \n",
       "428931    194.982143     0.040268     195.012846      0.055999   \n",
       "\n",
       "        log_returns_realized_volatility_300  \\\n",
       "0                                  0.003394   \n",
       "1                                  0.000699   \n",
       "2                                  0.001983   \n",
       "3                                  0.001863   \n",
       "4                                  0.001131   \n",
       "...                                     ...   \n",
       "428927                             0.002284   \n",
       "428928                             0.002217   \n",
       "428929                             0.001386   \n",
       "428930                             0.002783   \n",
       "428931                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000132                2.117007e-10   \n",
       "428928                             0.000128                9.484441e-11   \n",
       "428929                             0.000080                3.031629e-11   \n",
       "428930                             0.000161                1.960849e-10   \n",
       "428931                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  log_returns2_realized_volatility_300  \\\n",
       "0               7.138250e-06                              0.005032   \n",
       "1               8.823633e-07                              0.001448   \n",
       "2               1.729093e-06                              0.003583   \n",
       "3              -5.516464e-06                              0.002422   \n",
       "4              -2.164288e-06                              0.002412   \n",
       "...                      ...                                   ...   \n",
       "428927         -2.858852e-06                              0.004503   \n",
       "428928          3.674218e-06                              0.003652   \n",
       "428929         -2.437732e-06                              0.002686   \n",
       "428930          6.737309e-06                              0.004316   \n",
       "428931         -1.059089e-06                              0.001784   \n",
       "\n",
       "        log_returns2_weighted_volatility_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0                                   0.000291  ...            1              4   \n",
       "1                                   0.000084  ...            1              4   \n",
       "2                                   0.000207  ...            1              4   \n",
       "3                                   0.000140  ...            1              4   \n",
       "4                                   0.000140  ...            1              4   \n",
       "...                                      ...  ...          ...            ...   \n",
       "428927                              0.000260  ...            1              2   \n",
       "428928                              0.000211  ...            1              2   \n",
       "428929                              0.000155  ...            1              2   \n",
       "428930                              0.000250  ...            1              2   \n",
       "428931                              0.000103  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428791 rows x 193 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5bab9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_mean_900</th>\n",
       "      <th>wap_std_900</th>\n",
       "      <th>wap2_mean_900</th>\n",
       "      <th>wap2_std_900</th>\n",
       "      <th>log_returns_realized_volatility_900</th>\n",
       "      <th>log_returns_weighted_volatility_900</th>\n",
       "      <th>log_returns_quarticity_900</th>\n",
       "      <th>log_returns_mean_900</th>\n",
       "      <th>log_returns2_realized_volatility_900</th>\n",
       "      <th>log_returns2_weighted_volatility_900</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194.523405</td>\n",
       "      <td>0.137002</td>\n",
       "      <td>194.494943</td>\n",
       "      <td>0.153934</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>3.455208e-09</td>\n",
       "      <td>-9.753985e-07</td>\n",
       "      <td>0.007827</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209.519609</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>209.558521</td>\n",
       "      <td>0.383354</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.099042e-07</td>\n",
       "      <td>-2.355713e-06</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191.553810</td>\n",
       "      <td>0.364277</td>\n",
       "      <td>191.534358</td>\n",
       "      <td>0.383770</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>1.346483e-07</td>\n",
       "      <td>-3.936685e-06</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.765232</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>192.735618</td>\n",
       "      <td>0.657042</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>1.127322e-06</td>\n",
       "      <td>-6.358684e-06</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.219228</td>\n",
       "      <td>5.420681</td>\n",
       "      <td>192.208617</td>\n",
       "      <td>5.426199</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>3.980222e-03</td>\n",
       "      <td>6.443939e-05</td>\n",
       "      <td>0.060721</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213355</th>\n",
       "      <td>191.609684</td>\n",
       "      <td>1.258293</td>\n",
       "      <td>191.604521</td>\n",
       "      <td>1.254466</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>6.904742e-06</td>\n",
       "      <td>2.088613e-05</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213356</th>\n",
       "      <td>244.526554</td>\n",
       "      <td>1.607117</td>\n",
       "      <td>244.538262</td>\n",
       "      <td>1.625978</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>5.504136e-06</td>\n",
       "      <td>-1.615979e-05</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213357</th>\n",
       "      <td>308.076728</td>\n",
       "      <td>1.006311</td>\n",
       "      <td>308.107535</td>\n",
       "      <td>0.989751</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>5.021137e-07</td>\n",
       "      <td>-9.098024e-06</td>\n",
       "      <td>0.010255</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213358</th>\n",
       "      <td>199.448299</td>\n",
       "      <td>2.496420</td>\n",
       "      <td>199.399570</td>\n",
       "      <td>2.520482</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>1.302272e-04</td>\n",
       "      <td>2.968647e-05</td>\n",
       "      <td>0.026246</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213359</th>\n",
       "      <td>195.658217</td>\n",
       "      <td>0.521323</td>\n",
       "      <td>195.671518</td>\n",
       "      <td>0.508953</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>6.154658e-08</td>\n",
       "      <td>-4.053552e-06</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213297 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wap_mean_900  wap_std_900  wap2_mean_900  wap2_std_900  \\\n",
       "0         194.523405     0.137002     194.494943      0.153934   \n",
       "1         209.519609     0.379075     209.558521      0.383354   \n",
       "2         191.553810     0.364277     191.534358      0.383770   \n",
       "3         192.765232     0.641515     192.735618      0.657042   \n",
       "4         192.219228     5.420681     192.208617      5.426199   \n",
       "...              ...          ...            ...           ...   \n",
       "213355    191.609684     1.258293     191.604521      1.254466   \n",
       "213356    244.526554     1.607117     244.538262      1.625978   \n",
       "213357    308.076728     1.006311     308.107535      0.989751   \n",
       "213358    199.448299     2.496420     199.399570      2.520482   \n",
       "213359    195.658217     0.521323     195.671518      0.508953   \n",
       "\n",
       "        log_returns_realized_volatility_900  \\\n",
       "0                                  0.004921   \n",
       "1                                  0.005600   \n",
       "2                                  0.007720   \n",
       "3                                  0.011335   \n",
       "4                                  0.060467   \n",
       "...                                     ...   \n",
       "213355                             0.013254   \n",
       "213356                             0.012686   \n",
       "213357                             0.007706   \n",
       "213358                             0.026106   \n",
       "213359                             0.006634   \n",
       "\n",
       "        log_returns_weighted_volatility_900  log_returns_quarticity_900  \\\n",
       "0                                  0.000164                3.455208e-09   \n",
       "1                                  0.000187                1.099042e-07   \n",
       "2                                  0.000257                1.346483e-07   \n",
       "3                                  0.000378                1.127322e-06   \n",
       "4                                  0.002017                3.980222e-03   \n",
       "...                                     ...                         ...   \n",
       "213355                             0.000442                6.904742e-06   \n",
       "213356                             0.000423                5.504136e-06   \n",
       "213357                             0.000257                5.021137e-07   \n",
       "213358                             0.000871                1.302272e-04   \n",
       "213359                             0.000221                6.154658e-08   \n",
       "\n",
       "        log_returns_mean_900  log_returns2_realized_volatility_900  \\\n",
       "0              -9.753985e-07                              0.007827   \n",
       "1              -2.355713e-06                              0.007525   \n",
       "2              -3.936685e-06                              0.010917   \n",
       "3              -6.358684e-06                              0.014531   \n",
       "4               6.443939e-05                              0.060721   \n",
       "...                      ...                                   ...   \n",
       "213355          2.088613e-05                              0.014580   \n",
       "213356         -1.615979e-05                              0.012666   \n",
       "213357         -9.098024e-06                              0.010255   \n",
       "213358          2.968647e-05                              0.026246   \n",
       "213359         -4.053552e-06                              0.008471   \n",
       "\n",
       "        log_returns2_weighted_volatility_900  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0                                   0.000261  ...            1              4   \n",
       "1                                   0.000251  ...            1              4   \n",
       "2                                   0.000364  ...            1              4   \n",
       "3                                   0.000485  ...            1              4   \n",
       "4                                   0.002025  ...            1              4   \n",
       "...                                      ...  ...          ...            ...   \n",
       "213355                              0.000486  ...            1              2   \n",
       "213356                              0.000422  ...            1              2   \n",
       "213357                              0.000342  ...            1              2   \n",
       "213358                              0.000875  ...            1              2   \n",
       "213359                              0.000283  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "213355                -1         2          1          1           3   \n",
       "213356                -1         2          1          1           3   \n",
       "213357                -1         2          1          1           3   \n",
       "213358                -1         2          1          1           3   \n",
       "213359                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "213355           12         1          1  \n",
       "213356           12         1          1  \n",
       "213357           12         1          1  \n",
       "213358           12         1          1  \n",
       "213359           12         1          1  \n",
       "\n",
       "[213297 rows x 305 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extendX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53c341",
   "metadata": {},
   "source": [
    "### Defining y for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ad82c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002954</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000981</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001295</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001776</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001520</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.002899</td>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.002792</td>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.002379</td>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>0.001414</td>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  stock_id  time_id\n",
       "0                         0.002954         0        5\n",
       "1                         0.000981         0       11\n",
       "2                         0.001295         0       16\n",
       "3                         0.001776         0       31\n",
       "4                         0.001520         0       62\n",
       "...                            ...       ...      ...\n",
       "428927                    0.002899       126    32751\n",
       "428928                    0.003454       126    32753\n",
       "428929                    0.002792       126    32758\n",
       "428930                    0.002379       126    32763\n",
       "428931                    0.001414       126    32767\n",
       "\n",
       "[428791 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleY = simple[['target_realized_volatility', 'stock_id', 'time_id']]\n",
    "simpleY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09cace17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002954</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001295</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006357</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001498</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213355</th>\n",
       "      <td>0.003057</td>\n",
       "      <td>126</td>\n",
       "      <td>32748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213356</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>126</td>\n",
       "      <td>32750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213357</th>\n",
       "      <td>0.002899</td>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213358</th>\n",
       "      <td>0.002792</td>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213359</th>\n",
       "      <td>0.001414</td>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213297 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  stock_id  time_id\n",
       "0                         0.002954         0        5\n",
       "1                         0.001295         0       16\n",
       "2                         0.006357         0       72\n",
       "3                         0.002993         0      146\n",
       "4                         0.001498         0      169\n",
       "...                            ...       ...      ...\n",
       "213355                    0.003057       126    32748\n",
       "213356                    0.001827       126    32750\n",
       "213357                    0.002899       126    32751\n",
       "213358                    0.002792       126    32758\n",
       "213359                    0.001414       126    32767\n",
       "\n",
       "[213297 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extendY = extend[['target_realized_volatility', 'stock_id', 'time_id']]\n",
    "extendY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559bedb",
   "metadata": {},
   "source": [
    "Note: Including stock_id and time_id for logging predictions for app (target_realized_volatility is only target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab8eff",
   "metadata": {},
   "source": [
    "## Training LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d57cb7",
   "metadata": {},
   "source": [
    "### Model Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae000099",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10 #\n",
    "\n",
    "def compute_importance(model, features, typ='gain'):\n",
    "    return pd.DataFrame(\n",
    "        model.feature_importance(importance_type=typ),\n",
    "        index=features,\n",
    "        columns=['importance']\n",
    "    ).sort_values('importance')\n",
    "    \n",
    "def rmspe(y_true, y_pred, n=6):\n",
    "    return  round(np.sqrt(np.mean(np.square((y_true - y_pred) / (y_true + EPSILON)))), n)\n",
    "\n",
    "def feval_RMSPE(preds, lgbm_train, n=4):\n",
    "    labels = lgbm_train.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds), n), False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b854d3d",
   "metadata": {},
   "source": [
    "### Tuning Parameters \n",
    "* Takes too much time to be effective\n",
    "* Better to choose GOSS for faster convergence and decent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c085cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import warnings\n",
    "# import optuna.integration.lightgbm as lgb\n",
    "\n",
    "# kfold = KFold(5, random_state=42, shuffle=True)\n",
    "# params = {'metric': 'rmse',\n",
    "#           'boosting_type': 'goss',\n",
    "#           'n_jobs': -1,\n",
    "#           \"force_col_wise\": True,\n",
    "#           'seed': 42} \n",
    "\n",
    "# param_tuner = optuna.create_study(direction='minimize')\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "# target = simpleY.reset_index(drop=True)['target_realized_volatility'].values.flatten()\n",
    "# lgbm_train = lgbm.Dataset(simpleX,target.flatten(),\n",
    "#                           weight=1/(np.square(target)+EPSILON))\n",
    "\n",
    "# tuner = lgb.LightGBMTunerCV(params, \n",
    "#                             lgbm_train, \n",
    "#                             study=param_tuner,\n",
    "#                             early_stopping_rounds=250,\n",
    "#                             time_budget=19800,\n",
    "#                             seed = 42,\n",
    "#                             folds=kfold,\n",
    "#                             num_boost_round=10000,\n",
    "#                             callbacks=[\n",
    "#                               lgb.reset_parameter(\n",
    "#                                   learning_rate = [0.005]*200 + [0.001]*9800\n",
    "#                               ),\n",
    "#                               lgb.log_evaluation(period=100)\n",
    "#                             ])\n",
    "# tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tuner.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_optimization_history(param_tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cdac0",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "\n",
    "### Train Simple - Note: Overlap with Extend (which improves prediction on harder time buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fa16bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "Outer Fold : 1\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274425, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001185\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487702\ttraining's RMSPE: 0.3331\tvalid_1's rmse: 0.000487704\tvalid_1's RMSPE: 0.3291\n",
      "[200]\ttraining's rmse: 0.00039457\ttraining's RMSPE: 0.2695\tvalid_1's rmse: 0.000399279\tvalid_1's RMSPE: 0.2694\n",
      "[300]\ttraining's rmse: 0.000372613\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000380251\tvalid_1's RMSPE: 0.2566\n",
      "[400]\ttraining's rmse: 0.000364958\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.000374473\tvalid_1's RMSPE: 0.2527\n",
      "[500]\ttraining's rmse: 0.000360761\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.000371661\tvalid_1's RMSPE: 0.2508\n",
      "[600]\ttraining's rmse: 0.000357735\ttraining's RMSPE: 0.2444\tvalid_1's rmse: 0.000369839\tvalid_1's RMSPE: 0.2496\n",
      "[700]\ttraining's rmse: 0.000355369\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000368462\tvalid_1's RMSPE: 0.2486\n",
      "[800]\ttraining's rmse: 0.000353347\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000367293\tvalid_1's RMSPE: 0.2478\n",
      "[900]\ttraining's rmse: 0.000351605\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000366343\tvalid_1's RMSPE: 0.2472\n",
      "[1000]\ttraining's rmse: 0.000350087\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000365562\tvalid_1's RMSPE: 0.2467\n",
      "[1100]\ttraining's rmse: 0.000348721\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.00036489\tvalid_1's RMSPE: 0.2462\n",
      "[1200]\ttraining's rmse: 0.000347484\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000364279\tvalid_1's RMSPE: 0.2458\n",
      "[1300]\ttraining's rmse: 0.000346368\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000363798\tvalid_1's RMSPE: 0.2455\n",
      "[1400]\ttraining's rmse: 0.000345355\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000363407\tvalid_1's RMSPE: 0.2452\n",
      "[1500]\ttraining's rmse: 0.000344395\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000363051\tvalid_1's RMSPE: 0.245\n",
      "[1600]\ttraining's rmse: 0.000343528\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000362698\tvalid_1's RMSPE: 0.2447\n",
      "[1700]\ttraining's rmse: 0.000342715\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000362408\tvalid_1's RMSPE: 0.2445\n",
      "[1800]\ttraining's rmse: 0.000341938\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000362163\tvalid_1's RMSPE: 0.2444\n",
      "[1900]\ttraining's rmse: 0.000341201\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000361931\tvalid_1's RMSPE: 0.2442\n",
      "[2000]\ttraining's rmse: 0.000340482\ttraining's RMSPE: 0.2326\tvalid_1's rmse: 0.000361778\tvalid_1's RMSPE: 0.2441\n",
      "[2100]\ttraining's rmse: 0.000339801\ttraining's RMSPE: 0.2321\tvalid_1's rmse: 0.0003616\tvalid_1's RMSPE: 0.244\n",
      "[2200]\ttraining's rmse: 0.000339139\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000361466\tvalid_1's RMSPE: 0.2439\n",
      "Early stopping, best iteration is:\n",
      "[2133]\ttraining's rmse: 0.000339579\ttraining's RMSPE: 0.232\tvalid_1's rmse: 0.000361521\tvalid_1's RMSPE: 0.2439\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2439\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274425, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487656\ttraining's RMSPE: 0.3327\tvalid_1's rmse: 0.000489692\tvalid_1's RMSPE: 0.3322\n",
      "[200]\ttraining's rmse: 0.000394471\ttraining's RMSPE: 0.2691\tvalid_1's rmse: 0.000400849\tvalid_1's RMSPE: 0.2719\n",
      "[300]\ttraining's rmse: 0.000372637\ttraining's RMSPE: 0.2542\tvalid_1's rmse: 0.000380997\tvalid_1's RMSPE: 0.2585\n",
      "[400]\ttraining's rmse: 0.000365096\ttraining's RMSPE: 0.2491\tvalid_1's rmse: 0.000374605\tvalid_1's RMSPE: 0.2541\n",
      "[500]\ttraining's rmse: 0.000360981\ttraining's RMSPE: 0.2463\tvalid_1's rmse: 0.000371427\tvalid_1's RMSPE: 0.252\n",
      "[600]\ttraining's rmse: 0.00035807\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000369423\tvalid_1's RMSPE: 0.2506\n",
      "[700]\ttraining's rmse: 0.000355748\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.00036792\tvalid_1's RMSPE: 0.2496\n",
      "[800]\ttraining's rmse: 0.000353772\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000366854\tvalid_1's RMSPE: 0.2489\n",
      "[900]\ttraining's rmse: 0.000352059\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000365946\tvalid_1's RMSPE: 0.2483\n",
      "[1000]\ttraining's rmse: 0.000350593\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.00036521\tvalid_1's RMSPE: 0.2478\n",
      "[1100]\ttraining's rmse: 0.000349264\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000364545\tvalid_1's RMSPE: 0.2473\n",
      "[1200]\ttraining's rmse: 0.000348108\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000364072\tvalid_1's RMSPE: 0.247\n",
      "[1300]\ttraining's rmse: 0.000347007\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000363572\tvalid_1's RMSPE: 0.2466\n",
      "[1400]\ttraining's rmse: 0.000346002\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000363197\tvalid_1's RMSPE: 0.2464\n",
      "[1500]\ttraining's rmse: 0.000345079\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000362872\tvalid_1's RMSPE: 0.2462\n",
      "[1600]\ttraining's rmse: 0.00034424\ttraining's RMSPE: 0.2348\tvalid_1's rmse: 0.000362576\tvalid_1's RMSPE: 0.246\n",
      "[1700]\ttraining's rmse: 0.00034343\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000362232\tvalid_1's RMSPE: 0.2457\n",
      "[1800]\ttraining's rmse: 0.000342649\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000361982\tvalid_1's RMSPE: 0.2456\n",
      "[1900]\ttraining's rmse: 0.000341923\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.00036174\tvalid_1's RMSPE: 0.2454\n",
      "[2000]\ttraining's rmse: 0.000341232\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000361551\tvalid_1's RMSPE: 0.2453\n",
      "[2100]\ttraining's rmse: 0.000340539\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000361406\tvalid_1's RMSPE: 0.2452\n",
      "Early stopping, best iteration is:\n",
      "[2012]\ttraining's rmse: 0.000341135\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000361517\tvalid_1's RMSPE: 0.2452\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2452\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001209\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487017\ttraining's RMSPE: 0.3292\tvalid_1's rmse: 0.000520724\tvalid_1's RMSPE: 0.3659\n",
      "[200]\ttraining's rmse: 0.00039639\ttraining's RMSPE: 0.268\tvalid_1's rmse: 0.000422192\tvalid_1's RMSPE: 0.2966\n",
      "[300]\ttraining's rmse: 0.000375072\ttraining's RMSPE: 0.2536\tvalid_1's rmse: 0.000395677\tvalid_1's RMSPE: 0.278\n",
      "[400]\ttraining's rmse: 0.000367608\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.000387114\tvalid_1's RMSPE: 0.272\n",
      "[500]\ttraining's rmse: 0.000363406\ttraining's RMSPE: 0.2457\tvalid_1's rmse: 0.000381229\tvalid_1's RMSPE: 0.2678\n",
      "[600]\ttraining's rmse: 0.000360466\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000377152\tvalid_1's RMSPE: 0.265\n",
      "[700]\ttraining's rmse: 0.000357928\ttraining's RMSPE: 0.242\tvalid_1's rmse: 0.000374793\tvalid_1's RMSPE: 0.2633\n",
      "[800]\ttraining's rmse: 0.000355911\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000372495\tvalid_1's RMSPE: 0.2617\n",
      "[900]\ttraining's rmse: 0.000354114\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000371191\tvalid_1's RMSPE: 0.2608\n",
      "[1000]\ttraining's rmse: 0.000352612\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000369723\tvalid_1's RMSPE: 0.2598\n",
      "[1100]\ttraining's rmse: 0.000351276\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000368949\tvalid_1's RMSPE: 0.2592\n",
      "[1200]\ttraining's rmse: 0.000350037\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000368107\tvalid_1's RMSPE: 0.2586\n",
      "[1300]\ttraining's rmse: 0.000348917\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000368205\tvalid_1's RMSPE: 0.2587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1208]\ttraining's rmse: 0.000349948\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000367964\tvalid_1's RMSPE: 0.2585\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2585\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001186\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486992\ttraining's RMSPE: 0.3324\tvalid_1's rmse: 0.000501354\tvalid_1's RMSPE: 0.3393\n",
      "[200]\ttraining's rmse: 0.000393581\ttraining's RMSPE: 0.2687\tvalid_1's rmse: 0.000422423\tvalid_1's RMSPE: 0.2859\n",
      "[300]\ttraining's rmse: 0.000372037\ttraining's RMSPE: 0.2539\tvalid_1's rmse: 0.000406916\tvalid_1's RMSPE: 0.2754\n",
      "[400]\ttraining's rmse: 0.00036471\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.00040198\tvalid_1's RMSPE: 0.2721\n",
      "[500]\ttraining's rmse: 0.000360663\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000399401\tvalid_1's RMSPE: 0.2703\n",
      "[600]\ttraining's rmse: 0.000357786\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.00039792\tvalid_1's RMSPE: 0.2693\n",
      "[700]\ttraining's rmse: 0.000355492\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.000396224\tvalid_1's RMSPE: 0.2682\n",
      "[800]\ttraining's rmse: 0.000353555\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000395387\tvalid_1's RMSPE: 0.2676\n",
      "[900]\ttraining's rmse: 0.000351823\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000393991\tvalid_1's RMSPE: 0.2667\n",
      "[1000]\ttraining's rmse: 0.000350333\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000393629\tvalid_1's RMSPE: 0.2664\n",
      "[1100]\ttraining's rmse: 0.000349015\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000392979\tvalid_1's RMSPE: 0.266\n",
      "[1200]\ttraining's rmse: 0.000347803\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000392543\tvalid_1's RMSPE: 0.2657\n",
      "[1300]\ttraining's rmse: 0.00034674\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000392317\tvalid_1's RMSPE: 0.2655\n",
      "[1400]\ttraining's rmse: 0.000345737\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000391948\tvalid_1's RMSPE: 0.2653\n",
      "[1500]\ttraining's rmse: 0.000344754\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000391511\tvalid_1's RMSPE: 0.265\n",
      "[1600]\ttraining's rmse: 0.00034392\ttraining's RMSPE: 0.2348\tvalid_1's rmse: 0.000391095\tvalid_1's RMSPE: 0.2647\n",
      "Early stopping, best iteration is:\n",
      "[1582]\ttraining's rmse: 0.000344063\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000391114\tvalid_1's RMSPE: 0.2647\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2647\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001185\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486287\ttraining's RMSPE: 0.3323\tvalid_1's rmse: 0.000498584\tvalid_1's RMSPE: 0.3361\n",
      "[200]\ttraining's rmse: 0.000393338\ttraining's RMSPE: 0.2687\tvalid_1's rmse: 0.000415043\tvalid_1's RMSPE: 0.2798\n",
      "[300]\ttraining's rmse: 0.000371944\ttraining's RMSPE: 0.2541\tvalid_1's rmse: 0.000397607\tvalid_1's RMSPE: 0.2681\n",
      "[400]\ttraining's rmse: 0.000364565\ttraining's RMSPE: 0.2491\tvalid_1's rmse: 0.00039214\tvalid_1's RMSPE: 0.2644\n",
      "[500]\ttraining's rmse: 0.000360567\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.000389629\tvalid_1's RMSPE: 0.2627\n",
      "[600]\ttraining's rmse: 0.000357686\ttraining's RMSPE: 0.2444\tvalid_1's rmse: 0.000387805\tvalid_1's RMSPE: 0.2615\n",
      "[700]\ttraining's rmse: 0.000355362\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000386308\tvalid_1's RMSPE: 0.2604\n",
      "[800]\ttraining's rmse: 0.000353359\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000385134\tvalid_1's RMSPE: 0.2597\n",
      "[900]\ttraining's rmse: 0.000351617\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000384242\tvalid_1's RMSPE: 0.2591\n",
      "[1000]\ttraining's rmse: 0.000350128\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000383516\tvalid_1's RMSPE: 0.2586\n",
      "[1100]\ttraining's rmse: 0.000348797\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000382886\tvalid_1's RMSPE: 0.2581\n",
      "[1200]\ttraining's rmse: 0.000347584\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000382377\tvalid_1's RMSPE: 0.2578\n",
      "[1300]\ttraining's rmse: 0.000346494\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000381899\tvalid_1's RMSPE: 0.2575\n",
      "[1400]\ttraining's rmse: 0.000345496\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000381507\tvalid_1's RMSPE: 0.2572\n",
      "[1500]\ttraining's rmse: 0.000344541\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000381151\tvalid_1's RMSPE: 0.257\n",
      "[1600]\ttraining's rmse: 0.000343639\ttraining's RMSPE: 0.2348\tvalid_1's rmse: 0.000380783\tvalid_1's RMSPE: 0.2567\n",
      "[1700]\ttraining's rmse: 0.000342836\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.00038053\tvalid_1's RMSPE: 0.2566\n",
      "[1800]\ttraining's rmse: 0.000342056\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000380267\tvalid_1's RMSPE: 0.2564\n",
      "[1900]\ttraining's rmse: 0.000341297\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000380046\tvalid_1's RMSPE: 0.2562\n",
      "Early stopping, best iteration is:\n",
      "[1881]\ttraining's rmse: 0.00034144\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000380078\tvalid_1's RMSPE: 0.2562\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2562\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 2\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001214\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000485192\ttraining's RMSPE: 0.3272\tvalid_1's rmse: 0.000495899\tvalid_1's RMSPE: 0.336\n",
      "[200]\ttraining's rmse: 0.000394592\ttraining's RMSPE: 0.2661\tvalid_1's rmse: 0.000410625\tvalid_1's RMSPE: 0.2782\n",
      "[300]\ttraining's rmse: 0.000374224\ttraining's RMSPE: 0.2524\tvalid_1's rmse: 0.000392906\tvalid_1's RMSPE: 0.2662\n",
      "[400]\ttraining's rmse: 0.000367349\ttraining's RMSPE: 0.2477\tvalid_1's rmse: 0.000387764\tvalid_1's RMSPE: 0.2627\n",
      "[500]\ttraining's rmse: 0.000363495\ttraining's RMSPE: 0.2451\tvalid_1's rmse: 0.000385021\tvalid_1's RMSPE: 0.2608\n",
      "[600]\ttraining's rmse: 0.000360737\ttraining's RMSPE: 0.2433\tvalid_1's rmse: 0.000383301\tvalid_1's RMSPE: 0.2597\n",
      "[700]\ttraining's rmse: 0.000358543\ttraining's RMSPE: 0.2418\tvalid_1's rmse: 0.000382059\tvalid_1's RMSPE: 0.2588\n",
      "[800]\ttraining's rmse: 0.000356691\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000381164\tvalid_1's RMSPE: 0.2582\n",
      "[900]\ttraining's rmse: 0.000355065\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000380275\tvalid_1's RMSPE: 0.2576\n",
      "[1000]\ttraining's rmse: 0.000353686\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.00037961\tvalid_1's RMSPE: 0.2572\n",
      "[1100]\ttraining's rmse: 0.000352415\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000379152\tvalid_1's RMSPE: 0.2569\n",
      "[1200]\ttraining's rmse: 0.000351257\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000378595\tvalid_1's RMSPE: 0.2565\n",
      "[1300]\ttraining's rmse: 0.000350184\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000378055\tvalid_1's RMSPE: 0.2561\n",
      "[1400]\ttraining's rmse: 0.00034925\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000377633\tvalid_1's RMSPE: 0.2558\n",
      "[1500]\ttraining's rmse: 0.000348346\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.00037734\tvalid_1's RMSPE: 0.2556\n",
      "[1600]\ttraining's rmse: 0.000347497\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000376999\tvalid_1's RMSPE: 0.2554\n",
      "[1700]\ttraining's rmse: 0.00034669\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000376826\tvalid_1's RMSPE: 0.2553\n",
      "Early stopping, best iteration is:\n",
      "[1621]\ttraining's rmse: 0.000347329\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000376911\tvalid_1's RMSPE: 0.2553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2553\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001210\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487117\ttraining's RMSPE: 0.3289\tvalid_1's rmse: 0.000485873\tvalid_1's RMSPE: 0.3273\n",
      "[200]\ttraining's rmse: 0.00039644\ttraining's RMSPE: 0.2677\tvalid_1's rmse: 0.00039815\tvalid_1's RMSPE: 0.2682\n",
      "[300]\ttraining's rmse: 0.000375311\ttraining's RMSPE: 0.2534\tvalid_1's rmse: 0.00037864\tvalid_1's RMSPE: 0.2551\n",
      "[400]\ttraining's rmse: 0.000368009\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.000372536\tvalid_1's RMSPE: 0.251\n",
      "[500]\ttraining's rmse: 0.000363911\ttraining's RMSPE: 0.2457\tvalid_1's rmse: 0.000369703\tvalid_1's RMSPE: 0.2491\n",
      "[600]\ttraining's rmse: 0.00036098\ttraining's RMSPE: 0.2438\tvalid_1's rmse: 0.000367919\tvalid_1's RMSPE: 0.2479\n",
      "[700]\ttraining's rmse: 0.000358683\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000366452\tvalid_1's RMSPE: 0.2469\n",
      "[800]\ttraining's rmse: 0.000356699\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000365331\tvalid_1's RMSPE: 0.2461\n",
      "[900]\ttraining's rmse: 0.000354983\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000364485\tvalid_1's RMSPE: 0.2456\n",
      "[1000]\ttraining's rmse: 0.00035349\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000363787\tvalid_1's RMSPE: 0.2451\n",
      "[1100]\ttraining's rmse: 0.000352127\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000363141\tvalid_1's RMSPE: 0.2447\n",
      "[1200]\ttraining's rmse: 0.000350952\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000362575\tvalid_1's RMSPE: 0.2443\n",
      "[1300]\ttraining's rmse: 0.000349908\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000362178\tvalid_1's RMSPE: 0.244\n",
      "[1400]\ttraining's rmse: 0.000348932\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000361732\tvalid_1's RMSPE: 0.2437\n",
      "[1500]\ttraining's rmse: 0.000347973\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000361407\tvalid_1's RMSPE: 0.2435\n",
      "[1600]\ttraining's rmse: 0.000347097\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.00036103\tvalid_1's RMSPE: 0.2432\n",
      "[1700]\ttraining's rmse: 0.000346291\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.00036075\tvalid_1's RMSPE: 0.243\n",
      "[1800]\ttraining's rmse: 0.000345556\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000360592\tvalid_1's RMSPE: 0.2429\n",
      "[1900]\ttraining's rmse: 0.00034483\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.00036043\tvalid_1's RMSPE: 0.2428\n",
      "[2000]\ttraining's rmse: 0.000344154\ttraining's RMSPE: 0.2324\tvalid_1's rmse: 0.000360222\tvalid_1's RMSPE: 0.2427\n",
      "Early stopping, best iteration is:\n",
      "[1952]\ttraining's rmse: 0.000344467\ttraining's RMSPE: 0.2326\tvalid_1's rmse: 0.000360304\tvalid_1's RMSPE: 0.2427\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2427\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001211\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486647\ttraining's RMSPE: 0.3286\tvalid_1's rmse: 0.000492255\tvalid_1's RMSPE: 0.3317\n",
      "[200]\ttraining's rmse: 0.000396009\ttraining's RMSPE: 0.2674\tvalid_1's rmse: 0.000403022\tvalid_1's RMSPE: 0.2716\n",
      "[300]\ttraining's rmse: 0.000374779\ttraining's RMSPE: 0.2531\tvalid_1's rmse: 0.00038303\tvalid_1's RMSPE: 0.2581\n",
      "[400]\ttraining's rmse: 0.000367478\ttraining's RMSPE: 0.2481\tvalid_1's rmse: 0.000376728\tvalid_1's RMSPE: 0.2539\n",
      "[500]\ttraining's rmse: 0.000363417\ttraining's RMSPE: 0.2454\tvalid_1's rmse: 0.000373811\tvalid_1's RMSPE: 0.2519\n",
      "[600]\ttraining's rmse: 0.000360499\ttraining's RMSPE: 0.2434\tvalid_1's rmse: 0.000371835\tvalid_1's RMSPE: 0.2506\n",
      "[700]\ttraining's rmse: 0.000358176\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000370393\tvalid_1's RMSPE: 0.2496\n",
      "[800]\ttraining's rmse: 0.000356202\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000369355\tvalid_1's RMSPE: 0.2489\n",
      "[900]\ttraining's rmse: 0.000354511\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000368667\tvalid_1's RMSPE: 0.2484\n",
      "[1000]\ttraining's rmse: 0.000353001\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000367846\tvalid_1's RMSPE: 0.2479\n",
      "[1100]\ttraining's rmse: 0.000351669\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000367269\tvalid_1's RMSPE: 0.2475\n",
      "[1200]\ttraining's rmse: 0.000350492\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.00036674\tvalid_1's RMSPE: 0.2471\n",
      "[1300]\ttraining's rmse: 0.000349394\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000366412\tvalid_1's RMSPE: 0.2469\n",
      "[1400]\ttraining's rmse: 0.000348377\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.00036619\tvalid_1's RMSPE: 0.2468\n",
      "[1500]\ttraining's rmse: 0.000347442\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000365822\tvalid_1's RMSPE: 0.2465\n",
      "[1600]\ttraining's rmse: 0.000346549\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000365563\tvalid_1's RMSPE: 0.2463\n",
      "Early stopping, best iteration is:\n",
      "[1592]\ttraining's rmse: 0.000346615\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000365572\tvalid_1's RMSPE: 0.2463\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2463\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001210\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000485651\ttraining's RMSPE: 0.328\tvalid_1's rmse: 0.000491778\tvalid_1's RMSPE: 0.3312\n",
      "[200]\ttraining's rmse: 0.00039485\ttraining's RMSPE: 0.2667\tvalid_1's rmse: 0.000410767\tvalid_1's RMSPE: 0.2767\n",
      "[300]\ttraining's rmse: 0.000374095\ttraining's RMSPE: 0.2526\tvalid_1's rmse: 0.000395055\tvalid_1's RMSPE: 0.2661\n",
      "[400]\ttraining's rmse: 0.00036706\ttraining's RMSPE: 0.2479\tvalid_1's rmse: 0.00039058\tvalid_1's RMSPE: 0.2631\n",
      "[500]\ttraining's rmse: 0.000363148\ttraining's RMSPE: 0.2452\tvalid_1's rmse: 0.000388203\tvalid_1's RMSPE: 0.2615\n",
      "[600]\ttraining's rmse: 0.000360463\ttraining's RMSPE: 0.2434\tvalid_1's rmse: 0.000386327\tvalid_1's RMSPE: 0.2602\n",
      "[700]\ttraining's rmse: 0.000358251\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000385087\tvalid_1's RMSPE: 0.2594\n",
      "[800]\ttraining's rmse: 0.000356429\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000383785\tvalid_1's RMSPE: 0.2585\n",
      "[900]\ttraining's rmse: 0.000354802\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000382853\tvalid_1's RMSPE: 0.2579\n",
      "[1000]\ttraining's rmse: 0.000353391\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000382152\tvalid_1's RMSPE: 0.2574\n",
      "[1100]\ttraining's rmse: 0.000352128\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000381589\tvalid_1's RMSPE: 0.257\n",
      "[1200]\ttraining's rmse: 0.000350958\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000381109\tvalid_1's RMSPE: 0.2567\n",
      "[1300]\ttraining's rmse: 0.000349909\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000380829\tvalid_1's RMSPE: 0.2565\n",
      "[1400]\ttraining's rmse: 0.000348959\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000380492\tvalid_1's RMSPE: 0.2563\n",
      "[1500]\ttraining's rmse: 0.000348021\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.00038005\tvalid_1's RMSPE: 0.256\n",
      "[1600]\ttraining's rmse: 0.000347137\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000379776\tvalid_1's RMSPE: 0.2558\n",
      "Early stopping, best iteration is:\n",
      "[1561]\ttraining's rmse: 0.000347484\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000379851\tvalid_1's RMSPE: 0.2558\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2558\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.001213\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487363\ttraining's RMSPE: 0.3288\tvalid_1's rmse: 0.000489835\tvalid_1's RMSPE: 0.3312\n",
      "[200]\ttraining's rmse: 0.000396987\ttraining's RMSPE: 0.2678\tvalid_1's rmse: 0.000399179\tvalid_1's RMSPE: 0.2699\n",
      "[300]\ttraining's rmse: 0.000375801\ttraining's RMSPE: 0.2535\tvalid_1's rmse: 0.000378847\tvalid_1's RMSPE: 0.2562\n",
      "[400]\ttraining's rmse: 0.000368475\ttraining's RMSPE: 0.2486\tvalid_1's rmse: 0.000372662\tvalid_1's RMSPE: 0.252\n",
      "[500]\ttraining's rmse: 0.000364382\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000369749\tvalid_1's RMSPE: 0.25\n",
      "[600]\ttraining's rmse: 0.00036145\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000367913\tvalid_1's RMSPE: 0.2488\n",
      "[700]\ttraining's rmse: 0.000359124\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000366608\tvalid_1's RMSPE: 0.2479\n",
      "[800]\ttraining's rmse: 0.000357099\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.00036567\tvalid_1's RMSPE: 0.2473\n",
      "[900]\ttraining's rmse: 0.000355414\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000364775\tvalid_1's RMSPE: 0.2467\n",
      "[1000]\ttraining's rmse: 0.000353934\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000364046\tvalid_1's RMSPE: 0.2462\n",
      "[1100]\ttraining's rmse: 0.000352619\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.000363452\tvalid_1's RMSPE: 0.2458\n",
      "[1200]\ttraining's rmse: 0.000351461\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000363042\tvalid_1's RMSPE: 0.2455\n",
      "[1300]\ttraining's rmse: 0.00035035\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.000362702\tvalid_1's RMSPE: 0.2453\n",
      "[1400]\ttraining's rmse: 0.000349337\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000362319\tvalid_1's RMSPE: 0.245\n",
      "[1500]\ttraining's rmse: 0.000348428\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000362077\tvalid_1's RMSPE: 0.2448\n",
      "[1600]\ttraining's rmse: 0.000347576\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000361819\tvalid_1's RMSPE: 0.2447\n",
      "[1700]\ttraining's rmse: 0.000346739\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000361579\tvalid_1's RMSPE: 0.2445\n",
      "[1800]\ttraining's rmse: 0.000345969\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000361384\tvalid_1's RMSPE: 0.2444\n",
      "[1900]\ttraining's rmse: 0.000345238\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000361181\tvalid_1's RMSPE: 0.2442\n",
      "Early stopping, best iteration is:\n",
      "[1898]\ttraining's rmse: 0.000345252\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000361192\tvalid_1's RMSPE: 0.2442\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2442\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 3\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001189\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048677\ttraining's RMSPE: 0.3318\tvalid_1's rmse: 0.000495366\tvalid_1's RMSPE: 0.3353\n",
      "[200]\ttraining's rmse: 0.000393663\ttraining's RMSPE: 0.2683\tvalid_1's rmse: 0.000408835\tvalid_1's RMSPE: 0.2767\n",
      "[300]\ttraining's rmse: 0.000372243\ttraining's RMSPE: 0.2537\tvalid_1's rmse: 0.000389722\tvalid_1's RMSPE: 0.2638\n",
      "[400]\ttraining's rmse: 0.000364986\ttraining's RMSPE: 0.2488\tvalid_1's rmse: 0.000383891\tvalid_1's RMSPE: 0.2598\n",
      "[500]\ttraining's rmse: 0.000360899\ttraining's RMSPE: 0.246\tvalid_1's rmse: 0.000381125\tvalid_1's RMSPE: 0.258\n",
      "[600]\ttraining's rmse: 0.000357982\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000379368\tvalid_1's RMSPE: 0.2568\n",
      "[700]\ttraining's rmse: 0.000355685\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000378035\tvalid_1's RMSPE: 0.2559\n",
      "[800]\ttraining's rmse: 0.000353727\ttraining's RMSPE: 0.2411\tvalid_1's rmse: 0.000376939\tvalid_1's RMSPE: 0.2551\n",
      "[900]\ttraining's rmse: 0.000352036\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000376106\tvalid_1's RMSPE: 0.2546\n",
      "[1000]\ttraining's rmse: 0.000350504\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000375427\tvalid_1's RMSPE: 0.2541\n",
      "[1100]\ttraining's rmse: 0.000349202\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.0003749\tvalid_1's RMSPE: 0.2537\n",
      "[1200]\ttraining's rmse: 0.000348001\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000374383\tvalid_1's RMSPE: 0.2534\n",
      "[1300]\ttraining's rmse: 0.000346914\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000373963\tvalid_1's RMSPE: 0.2531\n",
      "[1400]\ttraining's rmse: 0.00034589\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000373642\tvalid_1's RMSPE: 0.2529\n",
      "[1500]\ttraining's rmse: 0.000344949\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000373346\tvalid_1's RMSPE: 0.2527\n",
      "[1600]\ttraining's rmse: 0.000344072\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000373102\tvalid_1's RMSPE: 0.2525\n",
      "[1700]\ttraining's rmse: 0.000343239\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000372785\tvalid_1's RMSPE: 0.2523\n",
      "[1800]\ttraining's rmse: 0.000342479\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000372676\tvalid_1's RMSPE: 0.2522\n",
      "Early stopping, best iteration is:\n",
      "[1770]\ttraining's rmse: 0.000342707\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000372685\tvalid_1's RMSPE: 0.2522\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2522\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001210\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487054\ttraining's RMSPE: 0.3289\tvalid_1's rmse: 0.000512998\tvalid_1's RMSPE: 0.3601\n",
      "[200]\ttraining's rmse: 0.000396178\ttraining's RMSPE: 0.2675\tvalid_1's rmse: 0.000412949\tvalid_1's RMSPE: 0.2899\n",
      "[300]\ttraining's rmse: 0.000375174\ttraining's RMSPE: 0.2533\tvalid_1's rmse: 0.000388481\tvalid_1's RMSPE: 0.2727\n",
      "[400]\ttraining's rmse: 0.000367989\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.0003794\tvalid_1's RMSPE: 0.2663\n",
      "[500]\ttraining's rmse: 0.000363971\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.00037427\tvalid_1's RMSPE: 0.2627\n",
      "[600]\ttraining's rmse: 0.000361127\ttraining's RMSPE: 0.2438\tvalid_1's rmse: 0.000370721\tvalid_1's RMSPE: 0.2602\n",
      "[700]\ttraining's rmse: 0.00035884\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000368832\tvalid_1's RMSPE: 0.2589\n",
      "[800]\ttraining's rmse: 0.000356894\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000366829\tvalid_1's RMSPE: 0.2575\n",
      "[900]\ttraining's rmse: 0.000355214\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000366466\tvalid_1's RMSPE: 0.2573\n",
      "[1000]\ttraining's rmse: 0.000353745\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000365725\tvalid_1's RMSPE: 0.2567\n",
      "Early stopping, best iteration is:\n",
      "[994]\ttraining's rmse: 0.000353822\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000365728\tvalid_1's RMSPE: 0.2567\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2567\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486754\ttraining's RMSPE: 0.3321\tvalid_1's rmse: 0.000489243\tvalid_1's RMSPE: 0.3299\n",
      "[200]\ttraining's rmse: 0.000393701\ttraining's RMSPE: 0.2686\tvalid_1's rmse: 0.000400931\tvalid_1's RMSPE: 0.2704\n",
      "[300]\ttraining's rmse: 0.000371939\ttraining's RMSPE: 0.2537\tvalid_1's rmse: 0.000381469\tvalid_1's RMSPE: 0.2573\n",
      "[400]\ttraining's rmse: 0.00036455\ttraining's RMSPE: 0.2487\tvalid_1's rmse: 0.000375568\tvalid_1's RMSPE: 0.2533\n",
      "[500]\ttraining's rmse: 0.000360485\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000372845\tvalid_1's RMSPE: 0.2514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 0.000357596\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000371061\tvalid_1's RMSPE: 0.2502\n",
      "[700]\ttraining's rmse: 0.000355306\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.00036969\tvalid_1's RMSPE: 0.2493\n",
      "[800]\ttraining's rmse: 0.000353332\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000368609\tvalid_1's RMSPE: 0.2486\n",
      "[900]\ttraining's rmse: 0.000351663\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000367783\tvalid_1's RMSPE: 0.248\n",
      "[1000]\ttraining's rmse: 0.000350197\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000367075\tvalid_1's RMSPE: 0.2475\n",
      "[1100]\ttraining's rmse: 0.000348878\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000366459\tvalid_1's RMSPE: 0.2471\n",
      "[1200]\ttraining's rmse: 0.000347704\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000365887\tvalid_1's RMSPE: 0.2467\n",
      "[1300]\ttraining's rmse: 0.000346616\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000365384\tvalid_1's RMSPE: 0.2464\n",
      "[1400]\ttraining's rmse: 0.000345623\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000365049\tvalid_1's RMSPE: 0.2462\n",
      "[1500]\ttraining's rmse: 0.000344674\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000364683\tvalid_1's RMSPE: 0.2459\n",
      "[1600]\ttraining's rmse: 0.000343774\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000364397\tvalid_1's RMSPE: 0.2457\n",
      "[1700]\ttraining's rmse: 0.000342968\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.00036413\tvalid_1's RMSPE: 0.2456\n",
      "[1800]\ttraining's rmse: 0.000342205\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000363943\tvalid_1's RMSPE: 0.2454\n",
      "Early stopping, best iteration is:\n",
      "[1786]\ttraining's rmse: 0.00034232\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000363964\tvalid_1's RMSPE: 0.2454\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2454\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001184\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487382\ttraining's RMSPE: 0.3329\tvalid_1's rmse: 0.000487013\tvalid_1's RMSPE: 0.327\n",
      "[200]\ttraining's rmse: 0.000394668\ttraining's RMSPE: 0.2695\tvalid_1's rmse: 0.000401381\tvalid_1's RMSPE: 0.2695\n",
      "[300]\ttraining's rmse: 0.000372997\ttraining's RMSPE: 0.2547\tvalid_1's rmse: 0.000383893\tvalid_1's RMSPE: 0.2577\n",
      "[400]\ttraining's rmse: 0.000365594\ttraining's RMSPE: 0.2497\tvalid_1's rmse: 0.00037877\tvalid_1's RMSPE: 0.2543\n",
      "[500]\ttraining's rmse: 0.000361427\ttraining's RMSPE: 0.2468\tvalid_1's rmse: 0.000376332\tvalid_1's RMSPE: 0.2527\n",
      "[600]\ttraining's rmse: 0.000358509\ttraining's RMSPE: 0.2448\tvalid_1's rmse: 0.00037502\tvalid_1's RMSPE: 0.2518\n",
      "[700]\ttraining's rmse: 0.000356133\ttraining's RMSPE: 0.2432\tvalid_1's rmse: 0.000373737\tvalid_1's RMSPE: 0.2509\n",
      "[800]\ttraining's rmse: 0.000354151\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000372742\tvalid_1's RMSPE: 0.2503\n",
      "[900]\ttraining's rmse: 0.000352458\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000371857\tvalid_1's RMSPE: 0.2497\n",
      "[1000]\ttraining's rmse: 0.000350957\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000370825\tvalid_1's RMSPE: 0.249\n",
      "[1100]\ttraining's rmse: 0.000349681\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000370242\tvalid_1's RMSPE: 0.2486\n",
      "[1200]\ttraining's rmse: 0.000348474\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000369897\tvalid_1's RMSPE: 0.2483\n",
      "[1300]\ttraining's rmse: 0.000347372\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000369356\tvalid_1's RMSPE: 0.248\n",
      "[1400]\ttraining's rmse: 0.000346309\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000368938\tvalid_1's RMSPE: 0.2477\n",
      "[1500]\ttraining's rmse: 0.000345346\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000368692\tvalid_1's RMSPE: 0.2475\n",
      "[1600]\ttraining's rmse: 0.000344439\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000368459\tvalid_1's RMSPE: 0.2474\n",
      "[1700]\ttraining's rmse: 0.000343605\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000368141\tvalid_1's RMSPE: 0.2472\n",
      "[1800]\ttraining's rmse: 0.000342844\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000367556\tvalid_1's RMSPE: 0.2468\n",
      "[1900]\ttraining's rmse: 0.000342092\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000367468\tvalid_1's RMSPE: 0.2467\n",
      "Early stopping, best iteration is:\n",
      "[1808]\ttraining's rmse: 0.000342777\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000367499\tvalid_1's RMSPE: 0.2467\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2467\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001190\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486123\ttraining's RMSPE: 0.3312\tvalid_1's rmse: 0.000502003\tvalid_1's RMSPE: 0.3404\n",
      "[200]\ttraining's rmse: 0.000393288\ttraining's RMSPE: 0.2679\tvalid_1's rmse: 0.000418895\tvalid_1's RMSPE: 0.2841\n",
      "[300]\ttraining's rmse: 0.000371985\ttraining's RMSPE: 0.2534\tvalid_1's rmse: 0.000401533\tvalid_1's RMSPE: 0.2723\n",
      "[400]\ttraining's rmse: 0.000364821\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.0003956\tvalid_1's RMSPE: 0.2683\n",
      "[500]\ttraining's rmse: 0.000360813\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000392185\tvalid_1's RMSPE: 0.266\n",
      "[600]\ttraining's rmse: 0.000357943\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.00039034\tvalid_1's RMSPE: 0.2647\n",
      "[700]\ttraining's rmse: 0.000355653\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000388892\tvalid_1's RMSPE: 0.2637\n",
      "[800]\ttraining's rmse: 0.000353694\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000387679\tvalid_1's RMSPE: 0.2629\n",
      "[900]\ttraining's rmse: 0.000351973\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000387058\tvalid_1's RMSPE: 0.2625\n",
      "[1000]\ttraining's rmse: 0.000350481\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000386417\tvalid_1's RMSPE: 0.2621\n",
      "[1100]\ttraining's rmse: 0.000349224\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.00038587\tvalid_1's RMSPE: 0.2617\n",
      "[1200]\ttraining's rmse: 0.000348095\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000385468\tvalid_1's RMSPE: 0.2614\n",
      "[1300]\ttraining's rmse: 0.000347002\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.0003851\tvalid_1's RMSPE: 0.2612\n",
      "[1400]\ttraining's rmse: 0.000346014\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000384709\tvalid_1's RMSPE: 0.2609\n",
      "[1500]\ttraining's rmse: 0.000345093\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000383899\tvalid_1's RMSPE: 0.2603\n",
      "[1600]\ttraining's rmse: 0.000344234\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000384043\tvalid_1's RMSPE: 0.2604\n",
      "Early stopping, best iteration is:\n",
      "[1500]\ttraining's rmse: 0.000345093\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000383899\tvalid_1's RMSPE: 0.2603\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2603\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 4\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001189\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487317\ttraining's RMSPE: 0.3322\tvalid_1's rmse: 0.000496218\tvalid_1's RMSPE: 0.336\n",
      "[200]\ttraining's rmse: 0.00039403\ttraining's RMSPE: 0.2686\tvalid_1's rmse: 0.000411238\tvalid_1's RMSPE: 0.2785\n",
      "[300]\ttraining's rmse: 0.000372523\ttraining's RMSPE: 0.2539\tvalid_1's rmse: 0.000393017\tvalid_1's RMSPE: 0.2661\n",
      "[400]\ttraining's rmse: 0.000365331\ttraining's RMSPE: 0.249\tvalid_1's rmse: 0.00038745\tvalid_1's RMSPE: 0.2624\n",
      "[500]\ttraining's rmse: 0.000361336\ttraining's RMSPE: 0.2463\tvalid_1's rmse: 0.000384411\tvalid_1's RMSPE: 0.2603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 0.000358433\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000382387\tvalid_1's RMSPE: 0.2589\n",
      "[700]\ttraining's rmse: 0.000356111\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.00038084\tvalid_1's RMSPE: 0.2579\n",
      "[800]\ttraining's rmse: 0.000354199\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000379597\tvalid_1's RMSPE: 0.257\n",
      "[900]\ttraining's rmse: 0.000352492\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000378553\tvalid_1's RMSPE: 0.2563\n",
      "[1000]\ttraining's rmse: 0.000351016\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000377683\tvalid_1's RMSPE: 0.2557\n",
      "[1100]\ttraining's rmse: 0.000349729\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000376997\tvalid_1's RMSPE: 0.2553\n",
      "[1200]\ttraining's rmse: 0.000348552\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000376487\tvalid_1's RMSPE: 0.2549\n",
      "[1300]\ttraining's rmse: 0.000347435\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000375923\tvalid_1's RMSPE: 0.2546\n",
      "[1400]\ttraining's rmse: 0.000346433\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000375367\tvalid_1's RMSPE: 0.2542\n",
      "[1500]\ttraining's rmse: 0.000345496\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000374949\tvalid_1's RMSPE: 0.2539\n",
      "[1600]\ttraining's rmse: 0.00034461\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000374453\tvalid_1's RMSPE: 0.2536\n",
      "[1700]\ttraining's rmse: 0.000343769\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.00037403\tvalid_1's RMSPE: 0.2533\n",
      "[1800]\ttraining's rmse: 0.000343014\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000373686\tvalid_1's RMSPE: 0.253\n",
      "[1900]\ttraining's rmse: 0.000342286\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000373486\tvalid_1's RMSPE: 0.2529\n",
      "[2000]\ttraining's rmse: 0.000341583\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000373257\tvalid_1's RMSPE: 0.2527\n",
      "[2100]\ttraining's rmse: 0.00034093\ttraining's RMSPE: 0.2324\tvalid_1's rmse: 0.000372997\tvalid_1's RMSPE: 0.2526\n",
      "[2200]\ttraining's rmse: 0.000340264\ttraining's RMSPE: 0.2319\tvalid_1's rmse: 0.000372776\tvalid_1's RMSPE: 0.2524\n",
      "Early stopping, best iteration is:\n",
      "[2169]\ttraining's rmse: 0.00034047\ttraining's RMSPE: 0.2321\tvalid_1's rmse: 0.000372803\tvalid_1's RMSPE: 0.2524\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2524\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001189\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488531\ttraining's RMSPE: 0.333\tvalid_1's rmse: 0.000495106\tvalid_1's RMSPE: 0.3354\n",
      "[200]\ttraining's rmse: 0.000395239\ttraining's RMSPE: 0.2694\tvalid_1's rmse: 0.000410253\tvalid_1's RMSPE: 0.2779\n",
      "[300]\ttraining's rmse: 0.000373355\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000392085\tvalid_1's RMSPE: 0.2656\n",
      "[400]\ttraining's rmse: 0.000365826\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.000386173\tvalid_1's RMSPE: 0.2616\n",
      "[500]\ttraining's rmse: 0.000361618\ttraining's RMSPE: 0.2465\tvalid_1's rmse: 0.000383272\tvalid_1's RMSPE: 0.2596\n",
      "[600]\ttraining's rmse: 0.000358614\ttraining's RMSPE: 0.2444\tvalid_1's rmse: 0.000381726\tvalid_1's RMSPE: 0.2586\n",
      "[700]\ttraining's rmse: 0.000356228\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000380476\tvalid_1's RMSPE: 0.2577\n",
      "[800]\ttraining's rmse: 0.000354229\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000379338\tvalid_1's RMSPE: 0.257\n",
      "[900]\ttraining's rmse: 0.000352477\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000378361\tvalid_1's RMSPE: 0.2563\n",
      "[1000]\ttraining's rmse: 0.000350956\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000377745\tvalid_1's RMSPE: 0.2559\n",
      "[1100]\ttraining's rmse: 0.000349563\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000377123\tvalid_1's RMSPE: 0.2555\n",
      "[1200]\ttraining's rmse: 0.000348343\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.00037659\tvalid_1's RMSPE: 0.2551\n",
      "[1300]\ttraining's rmse: 0.000347247\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.00037614\tvalid_1's RMSPE: 0.2548\n",
      "[1400]\ttraining's rmse: 0.000346247\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000376047\tvalid_1's RMSPE: 0.2547\n",
      "Early stopping, best iteration is:\n",
      "[1316]\ttraining's rmse: 0.000347083\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000376056\tvalid_1's RMSPE: 0.2547\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2547\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001184\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487581\ttraining's RMSPE: 0.3331\tvalid_1's rmse: 0.000487734\tvalid_1's RMSPE: 0.3273\n",
      "[200]\ttraining's rmse: 0.000394382\ttraining's RMSPE: 0.2694\tvalid_1's rmse: 0.000400676\tvalid_1's RMSPE: 0.2689\n",
      "[300]\ttraining's rmse: 0.000372392\ttraining's RMSPE: 0.2544\tvalid_1's rmse: 0.000381732\tvalid_1's RMSPE: 0.2562\n",
      "[400]\ttraining's rmse: 0.000364763\ttraining's RMSPE: 0.2492\tvalid_1's rmse: 0.000375916\tvalid_1's RMSPE: 0.2523\n",
      "[500]\ttraining's rmse: 0.000360515\ttraining's RMSPE: 0.2463\tvalid_1's rmse: 0.000373011\tvalid_1's RMSPE: 0.2503\n",
      "[600]\ttraining's rmse: 0.000357527\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000371219\tvalid_1's RMSPE: 0.2491\n",
      "[700]\ttraining's rmse: 0.000355184\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000369916\tvalid_1's RMSPE: 0.2483\n",
      "[800]\ttraining's rmse: 0.000353184\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000368835\tvalid_1's RMSPE: 0.2475\n",
      "[900]\ttraining's rmse: 0.000351433\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000367968\tvalid_1's RMSPE: 0.247\n",
      "[1000]\ttraining's rmse: 0.000349911\ttraining's RMSPE: 0.239\tvalid_1's rmse: 0.000367251\tvalid_1's RMSPE: 0.2465\n",
      "[1100]\ttraining's rmse: 0.00034858\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000366661\tvalid_1's RMSPE: 0.2461\n",
      "[1200]\ttraining's rmse: 0.000347407\ttraining's RMSPE: 0.2373\tvalid_1's rmse: 0.00036614\tvalid_1's RMSPE: 0.2457\n",
      "[1300]\ttraining's rmse: 0.000346273\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.00036569\tvalid_1's RMSPE: 0.2454\n",
      "[1400]\ttraining's rmse: 0.000345262\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000365265\tvalid_1's RMSPE: 0.2451\n",
      "[1500]\ttraining's rmse: 0.000344296\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000364947\tvalid_1's RMSPE: 0.2449\n",
      "[1600]\ttraining's rmse: 0.000343452\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000364672\tvalid_1's RMSPE: 0.2447\n",
      "[1700]\ttraining's rmse: 0.000342613\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.00036444\tvalid_1's RMSPE: 0.2446\n",
      "[1800]\ttraining's rmse: 0.000341808\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.0003642\tvalid_1's RMSPE: 0.2444\n",
      "[1900]\ttraining's rmse: 0.000341045\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000364039\tvalid_1's RMSPE: 0.2443\n",
      "[2000]\ttraining's rmse: 0.000340311\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000363851\tvalid_1's RMSPE: 0.2442\n",
      "[2100]\ttraining's rmse: 0.000339664\ttraining's RMSPE: 0.232\tvalid_1's rmse: 0.000363647\tvalid_1's RMSPE: 0.2441\n",
      "[2200]\ttraining's rmse: 0.000339014\ttraining's RMSPE: 0.2316\tvalid_1's rmse: 0.000363492\tvalid_1's RMSPE: 0.244\n",
      "[2300]\ttraining's rmse: 0.000338387\ttraining's RMSPE: 0.2312\tvalid_1's rmse: 0.000363315\tvalid_1's RMSPE: 0.2438\n",
      "[2400]\ttraining's rmse: 0.000337758\ttraining's RMSPE: 0.2307\tvalid_1's rmse: 0.000363166\tvalid_1's RMSPE: 0.2437\n",
      "Early stopping, best iteration is:\n",
      "[2380]\ttraining's rmse: 0.000337883\ttraining's RMSPE: 0.2308\tvalid_1's rmse: 0.000363187\tvalid_1's RMSPE: 0.2437\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2437\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001191\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487023\ttraining's RMSPE: 0.3316\tvalid_1's rmse: 0.000496683\tvalid_1's RMSPE: 0.3378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.000394178\ttraining's RMSPE: 0.2684\tvalid_1's rmse: 0.000411414\tvalid_1's RMSPE: 0.2798\n",
      "[300]\ttraining's rmse: 0.000372668\ttraining's RMSPE: 0.2537\tvalid_1's rmse: 0.000392565\tvalid_1's RMSPE: 0.267\n",
      "[400]\ttraining's rmse: 0.000365436\ttraining's RMSPE: 0.2488\tvalid_1's rmse: 0.000386465\tvalid_1's RMSPE: 0.2628\n",
      "[500]\ttraining's rmse: 0.000361486\ttraining's RMSPE: 0.2461\tvalid_1's rmse: 0.000383616\tvalid_1's RMSPE: 0.2609\n",
      "[600]\ttraining's rmse: 0.00035867\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.00038171\tvalid_1's RMSPE: 0.2596\n",
      "[700]\ttraining's rmse: 0.000356256\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000380249\tvalid_1's RMSPE: 0.2586\n",
      "[800]\ttraining's rmse: 0.000354291\ttraining's RMSPE: 0.2412\tvalid_1's rmse: 0.000379188\tvalid_1's RMSPE: 0.2579\n",
      "[900]\ttraining's rmse: 0.000352574\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000378196\tvalid_1's RMSPE: 0.2572\n",
      "[1000]\ttraining's rmse: 0.000351066\ttraining's RMSPE: 0.239\tvalid_1's rmse: 0.000377454\tvalid_1's RMSPE: 0.2567\n",
      "[1100]\ttraining's rmse: 0.000349718\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000376907\tvalid_1's RMSPE: 0.2563\n",
      "[1200]\ttraining's rmse: 0.000348511\ttraining's RMSPE: 0.2373\tvalid_1's rmse: 0.000376394\tvalid_1's RMSPE: 0.256\n",
      "[1300]\ttraining's rmse: 0.000347394\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000376078\tvalid_1's RMSPE: 0.2558\n",
      "[1400]\ttraining's rmse: 0.000346369\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000375567\tvalid_1's RMSPE: 0.2554\n",
      "[1500]\ttraining's rmse: 0.000345384\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000375116\tvalid_1's RMSPE: 0.2551\n",
      "[1600]\ttraining's rmse: 0.000344483\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000374896\tvalid_1's RMSPE: 0.255\n",
      "[1700]\ttraining's rmse: 0.000343655\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000374779\tvalid_1's RMSPE: 0.2549\n",
      "Early stopping, best iteration is:\n",
      "[1618]\ttraining's rmse: 0.000344328\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000374868\tvalid_1's RMSPE: 0.2549\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2549\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001208\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048723\ttraining's RMSPE: 0.3296\tvalid_1's rmse: 0.000515062\tvalid_1's RMSPE: 0.3593\n",
      "[200]\ttraining's rmse: 0.000396425\ttraining's RMSPE: 0.2682\tvalid_1's rmse: 0.000416429\tvalid_1's RMSPE: 0.2905\n",
      "[300]\ttraining's rmse: 0.000375343\ttraining's RMSPE: 0.2539\tvalid_1's rmse: 0.000392182\tvalid_1's RMSPE: 0.2736\n",
      "[400]\ttraining's rmse: 0.000368126\ttraining's RMSPE: 0.249\tvalid_1's rmse: 0.000384344\tvalid_1's RMSPE: 0.2681\n",
      "[500]\ttraining's rmse: 0.000363958\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000379852\tvalid_1's RMSPE: 0.265\n",
      "[600]\ttraining's rmse: 0.000360987\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000376493\tvalid_1's RMSPE: 0.2626\n",
      "[700]\ttraining's rmse: 0.000358673\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000374843\tvalid_1's RMSPE: 0.2615\n",
      "[800]\ttraining's rmse: 0.000356666\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000374178\tvalid_1's RMSPE: 0.261\n",
      "[900]\ttraining's rmse: 0.000354917\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000373005\tvalid_1's RMSPE: 0.2602\n",
      "[1000]\ttraining's rmse: 0.000353385\ttraining's RMSPE: 0.239\tvalid_1's rmse: 0.000372375\tvalid_1's RMSPE: 0.2598\n",
      "[1100]\ttraining's rmse: 0.000351984\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000371963\tvalid_1's RMSPE: 0.2595\n",
      "[1200]\ttraining's rmse: 0.000350713\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.00037121\tvalid_1's RMSPE: 0.259\n",
      "[1300]\ttraining's rmse: 0.000349567\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000370714\tvalid_1's RMSPE: 0.2586\n",
      "[1400]\ttraining's rmse: 0.000348522\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000370131\tvalid_1's RMSPE: 0.2582\n",
      "[1500]\ttraining's rmse: 0.000347586\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000369687\tvalid_1's RMSPE: 0.2579\n",
      "[1600]\ttraining's rmse: 0.000346671\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000369779\tvalid_1's RMSPE: 0.258\n",
      "Early stopping, best iteration is:\n",
      "[1576]\ttraining's rmse: 0.000346882\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000369473\tvalid_1's RMSPE: 0.2577\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2577\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 5\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001190\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486551\ttraining's RMSPE: 0.3316\tvalid_1's rmse: 0.000494563\tvalid_1's RMSPE: 0.3355\n",
      "[200]\ttraining's rmse: 0.000393386\ttraining's RMSPE: 0.2681\tvalid_1's rmse: 0.000410175\tvalid_1's RMSPE: 0.2783\n",
      "[300]\ttraining's rmse: 0.000371502\ttraining's RMSPE: 0.2532\tvalid_1's rmse: 0.000392426\tvalid_1's RMSPE: 0.2662\n",
      "[400]\ttraining's rmse: 0.000364109\ttraining's RMSPE: 0.2482\tvalid_1's rmse: 0.000386722\tvalid_1's RMSPE: 0.2623\n",
      "[500]\ttraining's rmse: 0.000360126\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000383749\tvalid_1's RMSPE: 0.2603\n",
      "[600]\ttraining's rmse: 0.000357227\ttraining's RMSPE: 0.2435\tvalid_1's rmse: 0.000382292\tvalid_1's RMSPE: 0.2593\n",
      "[700]\ttraining's rmse: 0.000354959\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000381118\tvalid_1's RMSPE: 0.2585\n",
      "[800]\ttraining's rmse: 0.000353055\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000380128\tvalid_1's RMSPE: 0.2579\n",
      "[900]\ttraining's rmse: 0.000351383\ttraining's RMSPE: 0.2395\tvalid_1's rmse: 0.000379293\tvalid_1's RMSPE: 0.2573\n",
      "[1000]\ttraining's rmse: 0.000349874\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000378006\tvalid_1's RMSPE: 0.2564\n",
      "[1100]\ttraining's rmse: 0.000348559\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000377258\tvalid_1's RMSPE: 0.2559\n",
      "[1200]\ttraining's rmse: 0.000347382\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000376588\tvalid_1's RMSPE: 0.2555\n",
      "[1300]\ttraining's rmse: 0.000346274\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000375978\tvalid_1's RMSPE: 0.2551\n",
      "[1400]\ttraining's rmse: 0.000345263\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000375217\tvalid_1's RMSPE: 0.2545\n",
      "[1500]\ttraining's rmse: 0.000344304\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000374944\tvalid_1's RMSPE: 0.2544\n",
      "[1600]\ttraining's rmse: 0.000343462\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000374537\tvalid_1's RMSPE: 0.2541\n",
      "[1700]\ttraining's rmse: 0.000342661\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000374267\tvalid_1's RMSPE: 0.2539\n",
      "Early stopping, best iteration is:\n",
      "[1658]\ttraining's rmse: 0.000342985\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000374342\tvalid_1's RMSPE: 0.2539\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2539\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001211\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486158\ttraining's RMSPE: 0.3284\tvalid_1's rmse: 0.000508541\tvalid_1's RMSPE: 0.3572\n",
      "[200]\ttraining's rmse: 0.000395697\ttraining's RMSPE: 0.2673\tvalid_1's rmse: 0.000411657\tvalid_1's RMSPE: 0.2892\n",
      "[300]\ttraining's rmse: 0.000374893\ttraining's RMSPE: 0.2532\tvalid_1's rmse: 0.000387848\tvalid_1's RMSPE: 0.2725\n",
      "[400]\ttraining's rmse: 0.000367694\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000379504\tvalid_1's RMSPE: 0.2666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 0.000363726\ttraining's RMSPE: 0.2457\tvalid_1's rmse: 0.000374946\tvalid_1's RMSPE: 0.2634\n",
      "[600]\ttraining's rmse: 0.000360807\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000372133\tvalid_1's RMSPE: 0.2614\n",
      "[700]\ttraining's rmse: 0.00035853\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000370018\tvalid_1's RMSPE: 0.2599\n",
      "[800]\ttraining's rmse: 0.000356555\ttraining's RMSPE: 0.2408\tvalid_1's rmse: 0.00036843\tvalid_1's RMSPE: 0.2588\n",
      "[900]\ttraining's rmse: 0.000354884\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000367842\tvalid_1's RMSPE: 0.2584\n",
      "[1000]\ttraining's rmse: 0.000353369\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000367066\tvalid_1's RMSPE: 0.2579\n",
      "[1100]\ttraining's rmse: 0.000352066\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000366405\tvalid_1's RMSPE: 0.2574\n",
      "[1200]\ttraining's rmse: 0.000350844\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000365701\tvalid_1's RMSPE: 0.2569\n",
      "[1300]\ttraining's rmse: 0.000349726\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000365078\tvalid_1's RMSPE: 0.2565\n",
      "[1400]\ttraining's rmse: 0.000348699\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000364829\tvalid_1's RMSPE: 0.2563\n",
      "[1500]\ttraining's rmse: 0.000347805\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000364897\tvalid_1's RMSPE: 0.2563\n",
      "Early stopping, best iteration is:\n",
      "[1410]\ttraining's rmse: 0.0003486\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000364782\tvalid_1's RMSPE: 0.2562\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2562\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001185\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000485433\ttraining's RMSPE: 0.3318\tvalid_1's rmse: 0.000491756\tvalid_1's RMSPE: 0.3298\n",
      "[200]\ttraining's rmse: 0.000392233\ttraining's RMSPE: 0.2681\tvalid_1's rmse: 0.000405091\tvalid_1's RMSPE: 0.2717\n",
      "[300]\ttraining's rmse: 0.00037042\ttraining's RMSPE: 0.2532\tvalid_1's rmse: 0.000386162\tvalid_1's RMSPE: 0.259\n",
      "[400]\ttraining's rmse: 0.000363098\ttraining's RMSPE: 0.2482\tvalid_1's rmse: 0.000380493\tvalid_1's RMSPE: 0.2552\n",
      "[500]\ttraining's rmse: 0.000359066\ttraining's RMSPE: 0.2454\tvalid_1's rmse: 0.000377531\tvalid_1's RMSPE: 0.2532\n",
      "[600]\ttraining's rmse: 0.000356258\ttraining's RMSPE: 0.2435\tvalid_1's rmse: 0.000375534\tvalid_1's RMSPE: 0.2519\n",
      "[700]\ttraining's rmse: 0.000353939\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.00037413\tvalid_1's RMSPE: 0.2509\n",
      "[800]\ttraining's rmse: 0.000351973\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000372836\tvalid_1's RMSPE: 0.2501\n",
      "[900]\ttraining's rmse: 0.000350332\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000371946\tvalid_1's RMSPE: 0.2495\n",
      "[1000]\ttraining's rmse: 0.000348848\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000371078\tvalid_1's RMSPE: 0.2489\n",
      "[1100]\ttraining's rmse: 0.000347512\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.00037045\tvalid_1's RMSPE: 0.2485\n",
      "[1200]\ttraining's rmse: 0.000346324\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000369895\tvalid_1's RMSPE: 0.2481\n",
      "[1300]\ttraining's rmse: 0.00034524\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000369474\tvalid_1's RMSPE: 0.2478\n",
      "[1400]\ttraining's rmse: 0.000344246\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000369154\tvalid_1's RMSPE: 0.2476\n",
      "[1500]\ttraining's rmse: 0.000343322\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000368714\tvalid_1's RMSPE: 0.2473\n",
      "[1600]\ttraining's rmse: 0.000342504\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000368446\tvalid_1's RMSPE: 0.2471\n",
      "[1700]\ttraining's rmse: 0.000341685\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000368211\tvalid_1's RMSPE: 0.247\n",
      "[1800]\ttraining's rmse: 0.000340911\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000367962\tvalid_1's RMSPE: 0.2468\n",
      "[1900]\ttraining's rmse: 0.000340179\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000367792\tvalid_1's RMSPE: 0.2467\n",
      "[2000]\ttraining's rmse: 0.000339476\ttraining's RMSPE: 0.232\tvalid_1's rmse: 0.000367512\tvalid_1's RMSPE: 0.2465\n",
      "[2100]\ttraining's rmse: 0.000338841\ttraining's RMSPE: 0.2316\tvalid_1's rmse: 0.000367298\tvalid_1's RMSPE: 0.2464\n",
      "[2200]\ttraining's rmse: 0.000338197\ttraining's RMSPE: 0.2311\tvalid_1's rmse: 0.000367188\tvalid_1's RMSPE: 0.2463\n",
      "Early stopping, best iteration is:\n",
      "[2125]\ttraining's rmse: 0.000338686\ttraining's RMSPE: 0.2315\tvalid_1's rmse: 0.000367279\tvalid_1's RMSPE: 0.2463\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2463\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001190\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486692\ttraining's RMSPE: 0.3316\tvalid_1's rmse: 0.000494353\tvalid_1's RMSPE: 0.3358\n",
      "[200]\ttraining's rmse: 0.000393742\ttraining's RMSPE: 0.2683\tvalid_1's rmse: 0.000406651\tvalid_1's RMSPE: 0.2762\n",
      "[300]\ttraining's rmse: 0.00037205\ttraining's RMSPE: 0.2535\tvalid_1's rmse: 0.000386952\tvalid_1's RMSPE: 0.2629\n",
      "[400]\ttraining's rmse: 0.000364693\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.000381045\tvalid_1's RMSPE: 0.2588\n",
      "[500]\ttraining's rmse: 0.000360699\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000378283\tvalid_1's RMSPE: 0.257\n",
      "[600]\ttraining's rmse: 0.000357865\ttraining's RMSPE: 0.2438\tvalid_1's rmse: 0.000376495\tvalid_1's RMSPE: 0.2557\n",
      "[700]\ttraining's rmse: 0.00035553\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000375077\tvalid_1's RMSPE: 0.2548\n",
      "[800]\ttraining's rmse: 0.000353591\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000373995\tvalid_1's RMSPE: 0.254\n",
      "[900]\ttraining's rmse: 0.000351969\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000373143\tvalid_1's RMSPE: 0.2535\n",
      "[1000]\ttraining's rmse: 0.00035053\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000372631\tvalid_1's RMSPE: 0.2531\n",
      "[1100]\ttraining's rmse: 0.000349266\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.00037224\tvalid_1's RMSPE: 0.2529\n",
      "[1200]\ttraining's rmse: 0.000348104\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000371777\tvalid_1's RMSPE: 0.2525\n",
      "[1300]\ttraining's rmse: 0.000347061\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000371343\tvalid_1's RMSPE: 0.2522\n",
      "[1400]\ttraining's rmse: 0.000346054\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000370947\tvalid_1's RMSPE: 0.252\n",
      "[1500]\ttraining's rmse: 0.000345155\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000370533\tvalid_1's RMSPE: 0.2517\n",
      "[1600]\ttraining's rmse: 0.000344293\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000370276\tvalid_1's RMSPE: 0.2515\n",
      "[1700]\ttraining's rmse: 0.00034346\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000370073\tvalid_1's RMSPE: 0.2514\n",
      "[1800]\ttraining's rmse: 0.000342703\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000369809\tvalid_1's RMSPE: 0.2512\n",
      "Early stopping, best iteration is:\n",
      "[1750]\ttraining's rmse: 0.000343068\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000369872\tvalid_1's RMSPE: 0.2512\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2512\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001186\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048683\ttraining's RMSPE: 0.3324\tvalid_1's rmse: 0.000489909\tvalid_1's RMSPE: 0.3299\n",
      "[200]\ttraining's rmse: 0.00039373\ttraining's RMSPE: 0.2688\tvalid_1's rmse: 0.000401975\tvalid_1's RMSPE: 0.2707\n",
      "[300]\ttraining's rmse: 0.000371875\ttraining's RMSPE: 0.2539\tvalid_1's rmse: 0.000382564\tvalid_1's RMSPE: 0.2576\n",
      "[400]\ttraining's rmse: 0.000364504\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.000376458\tvalid_1's RMSPE: 0.2535\n",
      "[500]\ttraining's rmse: 0.000360454\ttraining's RMSPE: 0.2461\tvalid_1's rmse: 0.000373428\tvalid_1's RMSPE: 0.2514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 0.000357554\ttraining's RMSPE: 0.2441\tvalid_1's rmse: 0.000371397\tvalid_1's RMSPE: 0.2501\n",
      "[700]\ttraining's rmse: 0.000355244\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000369803\tvalid_1's RMSPE: 0.249\n",
      "[800]\ttraining's rmse: 0.000353315\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000368612\tvalid_1's RMSPE: 0.2482\n",
      "[900]\ttraining's rmse: 0.000351618\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000367651\tvalid_1's RMSPE: 0.2475\n",
      "[1000]\ttraining's rmse: 0.000350151\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000366735\tvalid_1's RMSPE: 0.2469\n",
      "[1100]\ttraining's rmse: 0.000348837\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000366119\tvalid_1's RMSPE: 0.2465\n",
      "[1200]\ttraining's rmse: 0.000347654\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000365585\tvalid_1's RMSPE: 0.2462\n",
      "[1300]\ttraining's rmse: 0.000346577\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000365212\tvalid_1's RMSPE: 0.2459\n",
      "[1400]\ttraining's rmse: 0.000345605\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000364815\tvalid_1's RMSPE: 0.2456\n",
      "[1500]\ttraining's rmse: 0.000344642\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000364446\tvalid_1's RMSPE: 0.2454\n",
      "[1600]\ttraining's rmse: 0.000343764\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000364076\tvalid_1's RMSPE: 0.2451\n",
      "[1700]\ttraining's rmse: 0.000342975\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000363804\tvalid_1's RMSPE: 0.245\n",
      "[1800]\ttraining's rmse: 0.000342207\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000363596\tvalid_1's RMSPE: 0.2448\n",
      "[1900]\ttraining's rmse: 0.000341501\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000363383\tvalid_1's RMSPE: 0.2447\n",
      "[2000]\ttraining's rmse: 0.000340816\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000363197\tvalid_1's RMSPE: 0.2446\n",
      "[2100]\ttraining's rmse: 0.00034011\ttraining's RMSPE: 0.2322\tvalid_1's rmse: 0.000363006\tvalid_1's RMSPE: 0.2444\n",
      "[2200]\ttraining's rmse: 0.00033947\ttraining's RMSPE: 0.2318\tvalid_1's rmse: 0.000362815\tvalid_1's RMSPE: 0.2443\n",
      "[2300]\ttraining's rmse: 0.000338827\ttraining's RMSPE: 0.2314\tvalid_1's rmse: 0.000362679\tvalid_1's RMSPE: 0.2442\n",
      "Early stopping, best iteration is:\n",
      "[2260]\ttraining's rmse: 0.000339085\ttraining's RMSPE: 0.2315\tvalid_1's rmse: 0.000362751\tvalid_1's RMSPE: 0.2442\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2442\n",
      "\t**********************************************************************\n",
      "Wall time: 23min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPSILON = 0\n",
    "simple_test = pd.DataFrame(\n",
    "    {'target_realized_volatility':[],'predicted_volatility_simple': [], 'time_id':[], 'stock_id':[]}\n",
    "\n",
    ")\n",
    "simple_models = []\n",
    "split_importance = []\n",
    "gain_importance = []\n",
    "train_scores = []\n",
    "inner_k = 5\n",
    "outer_k = 5\n",
    "\n",
    "params =  {\n",
    "    'boosting_type': 'goss',\n",
    "    'learning_rate': 0.01,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8, \n",
    "    'bagging_fraction': 0.8,\n",
    "    'lambda_l1': 1.2,\n",
    "    'lambda_l2': 1.2,\n",
    "    'n_jobs': -1,\n",
    "    'force_col_wise': True,\n",
    "    'extra_trees': True,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "outer_kfold = KFold(n_splits=outer_k, random_state=42, shuffle=True)\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_kfold.split(simpleX, simpleY)):\n",
    "    print('*'*120)\n",
    "    print(\"Outer Fold :\", outer_fold + 1)\n",
    "    print('*'*120)\n",
    "\n",
    "    X_outer_train = simpleX.iloc[outer_train_idx].reset_index(drop=True)  \n",
    "    X_outer_test = simpleX.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    y_outer_train = simpleY.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    y_outer_test = simpleY.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    \n",
    "    target = np.zeros(len(y_outer_test))\n",
    "    inner_scores = 0.0\n",
    "    models = []\n",
    "    \n",
    "    inner_kfold = KFold(n_splits= inner_k, random_state=42, shuffle=True)\n",
    "    for inner_fold, (inner_train_idx, inner_valid_idx) in enumerate(inner_kfold.split(X_outer_train, y_outer_train)):\n",
    "        print(\"\\n\\t\"+\"*\"*20)\n",
    "        print(f\"\\t*  Inner Fold : {inner_fold + 1}  *\")\n",
    "        print(\"\\t\"+\"*\"*20+\"\\n\")\n",
    "    \n",
    "        # inner train data and valid data\n",
    "        X_inner_train = X_outer_train.iloc[inner_train_idx].reset_index(drop=True)\n",
    "        X_inner_valid = X_outer_train.iloc[inner_valid_idx].reset_index(drop=True)\n",
    "\n",
    "        y_inner_train = y_outer_train.iloc[inner_train_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "        y_inner_valid = y_outer_train.iloc[inner_valid_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "            \n",
    "        lgbm_train = lgbm.Dataset(X_inner_train,y_inner_train,weight=1/(np.square(y_inner_train.values)+EPSILON))\n",
    "        lgbm_valid = lgbm.Dataset(\n",
    "            X_inner_valid,y_inner_valid,reference=lgbm_train,weight=1/(np.square(y_inner_valid.values)+EPSILON))\n",
    "        \n",
    "        # model training\n",
    "        model = lgbm.train(\n",
    "            params=params, #tuner.best_params,\n",
    "            train_set=lgbm_train,\n",
    "            valid_sets=[lgbm_train, lgbm_valid],\n",
    "            num_boost_round=10000,       \n",
    "            feval=feval_RMSPE,\n",
    "            callbacks=[lgbm.log_evaluation(period=100), lgbm.early_stopping(100)]\n",
    "        )\n",
    "        # validation \n",
    "        y_inner_pred = model.predict(X_inner_valid, num_iteration=model.best_iteration)\n",
    "        RMSPE = rmspe(\n",
    "            y_true=(y_inner_valid.values.flatten()), \n",
    "            y_pred=(y_inner_pred), n=4\n",
    "        )\n",
    "        \n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "        print(f'\\tInner Validation RMSPE: \\t{RMSPE}')\n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "\n",
    "        # keep training validation score\n",
    "        inner_scores += RMSPE / inner_k\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "        # record feature importances by gain and split\n",
    "        features = list(X_inner_train.columns.values)\n",
    "        \n",
    "        gain_importance.append(compute_importance(model, features, typ='gain'))\n",
    "        split_importance.append(compute_importance(model, features, typ='split'))\n",
    "        \n",
    "    # store all models for prediction in oof evaluation\n",
    "    simple_models.append(models)\n",
    "    train_scores.append(inner_scores)\n",
    "    \n",
    "    # out of fold test set\n",
    "    for model in simple_models[outer_fold]:\n",
    "        y_outer_pred = model.predict(X_outer_test,num_iteration=model.best_iteration)\n",
    "        target += y_outer_pred / len(simple_models[outer_fold])\n",
    "    \n",
    "    y_outer_test = y_outer_test.assign(predicted_volatility_simple = target)\n",
    " \n",
    "    simple_test = pd.concat([simple_test, y_outer_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2eced56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.251854"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(simple_test['target_realized_volatility'], simple_test['predicted_volatility_simple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70a5b13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>predicted_volatility_simple</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>32653.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  predicted_volatility_simple  time_id  \\\n",
       "0                         0.003397                     0.002806    103.0   \n",
       "1                         0.002993                     0.004295    146.0   \n",
       "2                         0.001094                     0.001244    250.0   \n",
       "3                         0.001711                     0.001679    297.0   \n",
       "4                         0.001197                     0.002181    319.0   \n",
       "...                            ...                          ...      ...   \n",
       "428786                    0.001746                     0.002272  32653.0   \n",
       "428787                    0.003511                     0.004173  32724.0   \n",
       "428788                    0.010431                     0.010131  32746.0   \n",
       "428789                    0.001827                     0.002191  32750.0   \n",
       "428790                    0.003454                     0.002073  32753.0   \n",
       "\n",
       "        stock_id  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "...          ...  \n",
       "428786     126.0  \n",
       "428787     126.0  \n",
       "428788     126.0  \n",
       "428789     126.0  \n",
       "428790     126.0  \n",
       "\n",
       "[428791 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e7bcd",
   "metadata": {},
   "source": [
    "### Train Extend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45446567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "Outer Fold : 1\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136509, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001202\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516669\ttraining's RMSPE: 0.3441\tvalid_1's rmse: 0.000524894\tvalid_1's RMSPE: 0.3445\n",
      "[200]\ttraining's rmse: 0.000418333\ttraining's RMSPE: 0.2786\tvalid_1's rmse: 0.000435921\tvalid_1's RMSPE: 0.2861\n",
      "[300]\ttraining's rmse: 0.000393334\ttraining's RMSPE: 0.2619\tvalid_1's rmse: 0.000415269\tvalid_1's RMSPE: 0.2726\n",
      "[400]\ttraining's rmse: 0.000383688\ttraining's RMSPE: 0.2555\tvalid_1's rmse: 0.000408104\tvalid_1's RMSPE: 0.2679\n",
      "[500]\ttraining's rmse: 0.000377941\ttraining's RMSPE: 0.2517\tvalid_1's rmse: 0.000404277\tvalid_1's RMSPE: 0.2654\n",
      "[600]\ttraining's rmse: 0.000373513\ttraining's RMSPE: 0.2487\tvalid_1's rmse: 0.000401797\tvalid_1's RMSPE: 0.2637\n",
      "[700]\ttraining's rmse: 0.000369813\ttraining's RMSPE: 0.2463\tvalid_1's rmse: 0.000400049\tvalid_1's RMSPE: 0.2626\n",
      "[800]\ttraining's rmse: 0.000366719\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000398919\tvalid_1's RMSPE: 0.2618\n",
      "[900]\ttraining's rmse: 0.000364086\ttraining's RMSPE: 0.2425\tvalid_1's rmse: 0.000398008\tvalid_1's RMSPE: 0.2613\n",
      "[1000]\ttraining's rmse: 0.00036168\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000397376\tvalid_1's RMSPE: 0.2608\n",
      "[1100]\ttraining's rmse: 0.000359461\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000396682\tvalid_1's RMSPE: 0.2604\n",
      "[1200]\ttraining's rmse: 0.000357432\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.0003963\tvalid_1's RMSPE: 0.2601\n",
      "[1300]\ttraining's rmse: 0.000355643\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000395744\tvalid_1's RMSPE: 0.2598\n",
      "[1400]\ttraining's rmse: 0.000354009\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.00039542\tvalid_1's RMSPE: 0.2596\n",
      "[1500]\ttraining's rmse: 0.000352409\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000395107\tvalid_1's RMSPE: 0.2593\n",
      "[1600]\ttraining's rmse: 0.000350908\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000394928\tvalid_1's RMSPE: 0.2592\n",
      "[1700]\ttraining's rmse: 0.000349499\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000394685\tvalid_1's RMSPE: 0.2591\n",
      "Early stopping, best iteration is:\n",
      "[1622]\ttraining's rmse: 0.000350569\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000394805\tvalid_1's RMSPE: 0.2591\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2591\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136509, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001199\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516418\ttraining's RMSPE: 0.3444\tvalid_1's rmse: 0.000523585\tvalid_1's RMSPE: 0.3415\n",
      "[200]\ttraining's rmse: 0.000417683\ttraining's RMSPE: 0.2786\tvalid_1's rmse: 0.000435381\tvalid_1's RMSPE: 0.284\n",
      "[300]\ttraining's rmse: 0.000392134\ttraining's RMSPE: 0.2615\tvalid_1's rmse: 0.000414318\tvalid_1's RMSPE: 0.2702\n",
      "[400]\ttraining's rmse: 0.000382267\ttraining's RMSPE: 0.255\tvalid_1's rmse: 0.000407056\tvalid_1's RMSPE: 0.2655\n",
      "[500]\ttraining's rmse: 0.000376396\ttraining's RMSPE: 0.251\tvalid_1's rmse: 0.000403058\tvalid_1's RMSPE: 0.2629\n",
      "[600]\ttraining's rmse: 0.000371929\ttraining's RMSPE: 0.2481\tvalid_1's rmse: 0.000400237\tvalid_1's RMSPE: 0.2611\n",
      "[700]\ttraining's rmse: 0.000368179\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.00039799\tvalid_1's RMSPE: 0.2596\n",
      "[800]\ttraining's rmse: 0.000365144\ttraining's RMSPE: 0.2435\tvalid_1's rmse: 0.000396156\tvalid_1's RMSPE: 0.2584\n",
      "[900]\ttraining's rmse: 0.000362517\ttraining's RMSPE: 0.2418\tvalid_1's rmse: 0.000394789\tvalid_1's RMSPE: 0.2575\n",
      "[1000]\ttraining's rmse: 0.00036018\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000393834\tvalid_1's RMSPE: 0.2569\n",
      "[1100]\ttraining's rmse: 0.000358024\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000392751\tvalid_1's RMSPE: 0.2562\n",
      "[1200]\ttraining's rmse: 0.000356092\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000391971\tvalid_1's RMSPE: 0.2557\n",
      "[1300]\ttraining's rmse: 0.000354274\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000391177\tvalid_1's RMSPE: 0.2551\n",
      "[1400]\ttraining's rmse: 0.000352605\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000390454\tvalid_1's RMSPE: 0.2547\n",
      "[1500]\ttraining's rmse: 0.00035108\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000389907\tvalid_1's RMSPE: 0.2543\n",
      "[1600]\ttraining's rmse: 0.000349647\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000389519\tvalid_1's RMSPE: 0.2541\n",
      "[1700]\ttraining's rmse: 0.00034832\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000389204\tvalid_1's RMSPE: 0.2539\n",
      "[1800]\ttraining's rmse: 0.0003471\ttraining's RMSPE: 0.2315\tvalid_1's rmse: 0.000388814\tvalid_1's RMSPE: 0.2536\n",
      "[1900]\ttraining's rmse: 0.000345899\ttraining's RMSPE: 0.2307\tvalid_1's rmse: 0.000388477\tvalid_1's RMSPE: 0.2534\n",
      "[2000]\ttraining's rmse: 0.000344801\ttraining's RMSPE: 0.23\tvalid_1's rmse: 0.000388293\tvalid_1's RMSPE: 0.2533\n",
      "[2100]\ttraining's rmse: 0.000343681\ttraining's RMSPE: 0.2292\tvalid_1's rmse: 0.000388037\tvalid_1's RMSPE: 0.2531\n",
      "[2200]\ttraining's rmse: 0.000342623\ttraining's RMSPE: 0.2285\tvalid_1's rmse: 0.000387783\tvalid_1's RMSPE: 0.2529\n",
      "Early stopping, best iteration is:\n",
      "[2161]\ttraining's rmse: 0.000343039\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000387812\tvalid_1's RMSPE: 0.2529\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2529\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001200\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516733\ttraining's RMSPE: 0.3446\tvalid_1's rmse: 0.000518679\tvalid_1's RMSPE: 0.3384\n",
      "[200]\ttraining's rmse: 0.000418998\ttraining's RMSPE: 0.2794\tvalid_1's rmse: 0.000428291\tvalid_1's RMSPE: 0.2794\n",
      "[300]\ttraining's rmse: 0.000393915\ttraining's RMSPE: 0.2627\tvalid_1's rmse: 0.000406496\tvalid_1's RMSPE: 0.2652\n",
      "[400]\ttraining's rmse: 0.000384214\ttraining's RMSPE: 0.2562\tvalid_1's rmse: 0.000398948\tvalid_1's RMSPE: 0.2603\n",
      "[500]\ttraining's rmse: 0.000378406\ttraining's RMSPE: 0.2524\tvalid_1's rmse: 0.000394576\tvalid_1's RMSPE: 0.2574\n",
      "[600]\ttraining's rmse: 0.000374063\ttraining's RMSPE: 0.2495\tvalid_1's rmse: 0.000391325\tvalid_1's RMSPE: 0.2553\n",
      "[700]\ttraining's rmse: 0.000370331\ttraining's RMSPE: 0.247\tvalid_1's rmse: 0.000388668\tvalid_1's RMSPE: 0.2536\n",
      "[800]\ttraining's rmse: 0.00036721\ttraining's RMSPE: 0.2449\tvalid_1's rmse: 0.000386364\tvalid_1's RMSPE: 0.2521\n",
      "[900]\ttraining's rmse: 0.000364382\ttraining's RMSPE: 0.243\tvalid_1's rmse: 0.000384732\tvalid_1's RMSPE: 0.251\n",
      "[1000]\ttraining's rmse: 0.000361919\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000383266\tvalid_1's RMSPE: 0.2501\n",
      "[1100]\ttraining's rmse: 0.000359732\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000382069\tvalid_1's RMSPE: 0.2493\n",
      "[1200]\ttraining's rmse: 0.000357661\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.00038084\tvalid_1's RMSPE: 0.2485\n",
      "[1300]\ttraining's rmse: 0.000355893\ttraining's RMSPE: 0.2373\tvalid_1's rmse: 0.000380124\tvalid_1's RMSPE: 0.248\n",
      "[1400]\ttraining's rmse: 0.000354187\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000379404\tvalid_1's RMSPE: 0.2475\n",
      "[1500]\ttraining's rmse: 0.000352654\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000378558\tvalid_1's RMSPE: 0.247\n",
      "[1600]\ttraining's rmse: 0.000351156\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.00037808\tvalid_1's RMSPE: 0.2467\n",
      "[1700]\ttraining's rmse: 0.000349811\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000377599\tvalid_1's RMSPE: 0.2464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1800]\ttraining's rmse: 0.000348488\ttraining's RMSPE: 0.2324\tvalid_1's rmse: 0.00037708\tvalid_1's RMSPE: 0.246\n",
      "[1900]\ttraining's rmse: 0.000347226\ttraining's RMSPE: 0.2316\tvalid_1's rmse: 0.000376758\tvalid_1's RMSPE: 0.2458\n",
      "[2000]\ttraining's rmse: 0.000346028\ttraining's RMSPE: 0.2308\tvalid_1's rmse: 0.000376554\tvalid_1's RMSPE: 0.2457\n",
      "[2100]\ttraining's rmse: 0.000344885\ttraining's RMSPE: 0.23\tvalid_1's rmse: 0.000376267\tvalid_1's RMSPE: 0.2455\n",
      "[2200]\ttraining's rmse: 0.000343802\ttraining's RMSPE: 0.2293\tvalid_1's rmse: 0.000375965\tvalid_1's RMSPE: 0.2453\n",
      "[2300]\ttraining's rmse: 0.000342764\ttraining's RMSPE: 0.2286\tvalid_1's rmse: 0.000375765\tvalid_1's RMSPE: 0.2452\n",
      "[2400]\ttraining's rmse: 0.000341772\ttraining's RMSPE: 0.2279\tvalid_1's rmse: 0.000375562\tvalid_1's RMSPE: 0.245\n",
      "[2500]\ttraining's rmse: 0.000340826\ttraining's RMSPE: 0.2273\tvalid_1's rmse: 0.000375316\tvalid_1's RMSPE: 0.2449\n",
      "[2600]\ttraining's rmse: 0.000339937\ttraining's RMSPE: 0.2267\tvalid_1's rmse: 0.000375171\tvalid_1's RMSPE: 0.2448\n",
      "Early stopping, best iteration is:\n",
      "[2515]\ttraining's rmse: 0.000340686\ttraining's RMSPE: 0.2272\tvalid_1's rmse: 0.00037527\tvalid_1's RMSPE: 0.2448\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2448\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001202\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516323\ttraining's RMSPE: 0.344\tvalid_1's rmse: 0.000525276\tvalid_1's RMSPE: 0.3442\n",
      "[200]\ttraining's rmse: 0.000417008\ttraining's RMSPE: 0.2778\tvalid_1's rmse: 0.000434342\tvalid_1's RMSPE: 0.2846\n",
      "[300]\ttraining's rmse: 0.00039209\ttraining's RMSPE: 0.2612\tvalid_1's rmse: 0.000412698\tvalid_1's RMSPE: 0.2704\n",
      "[400]\ttraining's rmse: 0.000382451\ttraining's RMSPE: 0.2548\tvalid_1's rmse: 0.000404896\tvalid_1's RMSPE: 0.2653\n",
      "[500]\ttraining's rmse: 0.000376794\ttraining's RMSPE: 0.251\tvalid_1's rmse: 0.000400767\tvalid_1's RMSPE: 0.2626\n",
      "[600]\ttraining's rmse: 0.000372409\ttraining's RMSPE: 0.2481\tvalid_1's rmse: 0.000397664\tvalid_1's RMSPE: 0.2606\n",
      "[700]\ttraining's rmse: 0.000368821\ttraining's RMSPE: 0.2457\tvalid_1's rmse: 0.00039531\tvalid_1's RMSPE: 0.259\n",
      "[800]\ttraining's rmse: 0.000365825\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000393604\tvalid_1's RMSPE: 0.2579\n",
      "[900]\ttraining's rmse: 0.000363148\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.00039206\tvalid_1's RMSPE: 0.2569\n",
      "[1000]\ttraining's rmse: 0.000360871\ttraining's RMSPE: 0.2404\tvalid_1's rmse: 0.000390936\tvalid_1's RMSPE: 0.2562\n",
      "[1100]\ttraining's rmse: 0.000358718\ttraining's RMSPE: 0.239\tvalid_1's rmse: 0.000389876\tvalid_1's RMSPE: 0.2555\n",
      "[1200]\ttraining's rmse: 0.000356803\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.00038901\tvalid_1's RMSPE: 0.2549\n",
      "[1300]\ttraining's rmse: 0.000354924\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000388136\tvalid_1's RMSPE: 0.2543\n",
      "[1400]\ttraining's rmse: 0.00035326\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000387479\tvalid_1's RMSPE: 0.2539\n",
      "[1500]\ttraining's rmse: 0.000351773\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000386809\tvalid_1's RMSPE: 0.2534\n",
      "[1600]\ttraining's rmse: 0.00035036\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000386261\tvalid_1's RMSPE: 0.2531\n",
      "[1700]\ttraining's rmse: 0.000349022\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000385876\tvalid_1's RMSPE: 0.2528\n",
      "[1800]\ttraining's rmse: 0.000347774\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000385424\tvalid_1's RMSPE: 0.2525\n",
      "[1900]\ttraining's rmse: 0.000346534\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000385033\tvalid_1's RMSPE: 0.2523\n",
      "[2000]\ttraining's rmse: 0.000345351\ttraining's RMSPE: 0.2301\tvalid_1's rmse: 0.000384578\tvalid_1's RMSPE: 0.252\n",
      "[2100]\ttraining's rmse: 0.000344244\ttraining's RMSPE: 0.2293\tvalid_1's rmse: 0.000384318\tvalid_1's RMSPE: 0.2518\n",
      "[2200]\ttraining's rmse: 0.000343201\ttraining's RMSPE: 0.2286\tvalid_1's rmse: 0.000384131\tvalid_1's RMSPE: 0.2517\n",
      "[2300]\ttraining's rmse: 0.000342204\ttraining's RMSPE: 0.228\tvalid_1's rmse: 0.0003839\tvalid_1's RMSPE: 0.2515\n",
      "[2400]\ttraining's rmse: 0.000341228\ttraining's RMSPE: 0.2273\tvalid_1's rmse: 0.000383685\tvalid_1's RMSPE: 0.2514\n",
      "[2500]\ttraining's rmse: 0.000340306\ttraining's RMSPE: 0.2267\tvalid_1's rmse: 0.000383551\tvalid_1's RMSPE: 0.2513\n",
      "[2600]\ttraining's rmse: 0.000339351\ttraining's RMSPE: 0.2261\tvalid_1's rmse: 0.000383268\tvalid_1's RMSPE: 0.2511\n",
      "[2700]\ttraining's rmse: 0.000338453\ttraining's RMSPE: 0.2255\tvalid_1's rmse: 0.000383069\tvalid_1's RMSPE: 0.251\n",
      "[2800]\ttraining's rmse: 0.000337631\ttraining's RMSPE: 0.2249\tvalid_1's rmse: 0.000382929\tvalid_1's RMSPE: 0.2509\n",
      "Early stopping, best iteration is:\n",
      "[2737]\ttraining's rmse: 0.00033812\ttraining's RMSPE: 0.2253\tvalid_1's rmse: 0.000382992\tvalid_1's RMSPE: 0.2509\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2509\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001247\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516124\ttraining's RMSPE: 0.3376\tvalid_1's rmse: 0.000587522\tvalid_1's RMSPE: 0.4127\n",
      "[200]\ttraining's rmse: 0.000422428\ttraining's RMSPE: 0.2763\tvalid_1's rmse: 0.00048471\tvalid_1's RMSPE: 0.3405\n",
      "[300]\ttraining's rmse: 0.0003985\ttraining's RMSPE: 0.2607\tvalid_1's rmse: 0.000451268\tvalid_1's RMSPE: 0.317\n",
      "[400]\ttraining's rmse: 0.000388972\ttraining's RMSPE: 0.2544\tvalid_1's rmse: 0.000435654\tvalid_1's RMSPE: 0.306\n",
      "[500]\ttraining's rmse: 0.00038322\ttraining's RMSPE: 0.2507\tvalid_1's rmse: 0.000427388\tvalid_1's RMSPE: 0.3002\n",
      "[600]\ttraining's rmse: 0.000378824\ttraining's RMSPE: 0.2478\tvalid_1's rmse: 0.000424679\tvalid_1's RMSPE: 0.2983\n",
      "[700]\ttraining's rmse: 0.000375151\ttraining's RMSPE: 0.2454\tvalid_1's rmse: 0.000420948\tvalid_1's RMSPE: 0.2957\n",
      "[800]\ttraining's rmse: 0.000372071\ttraining's RMSPE: 0.2434\tvalid_1's rmse: 0.000418322\tvalid_1's RMSPE: 0.2938\n",
      "[900]\ttraining's rmse: 0.000369422\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.00041683\tvalid_1's RMSPE: 0.2928\n",
      "[1000]\ttraining's rmse: 0.000367049\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000416549\tvalid_1's RMSPE: 0.2926\n",
      "Early stopping, best iteration is:\n",
      "[942]\ttraining's rmse: 0.000368372\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000415483\tvalid_1's RMSPE: 0.2918\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2918\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 2\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136509, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001198\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517342\ttraining's RMSPE: 0.3454\tvalid_1's rmse: 0.000519883\tvalid_1's RMSPE: 0.3381\n",
      "[200]\ttraining's rmse: 0.000418999\ttraining's RMSPE: 0.2798\tvalid_1's rmse: 0.000426981\tvalid_1's RMSPE: 0.2777\n",
      "[300]\ttraining's rmse: 0.000393674\ttraining's RMSPE: 0.2629\tvalid_1's rmse: 0.000403684\tvalid_1's RMSPE: 0.2625\n",
      "[400]\ttraining's rmse: 0.00038381\ttraining's RMSPE: 0.2563\tvalid_1's rmse: 0.000395338\tvalid_1's RMSPE: 0.2571\n",
      "[500]\ttraining's rmse: 0.000377908\ttraining's RMSPE: 0.2523\tvalid_1's rmse: 0.0003909\tvalid_1's RMSPE: 0.2542\n",
      "[600]\ttraining's rmse: 0.000373468\ttraining's RMSPE: 0.2494\tvalid_1's rmse: 0.000387722\tvalid_1's RMSPE: 0.2521\n",
      "[700]\ttraining's rmse: 0.000369683\ttraining's RMSPE: 0.2468\tvalid_1's rmse: 0.000385038\tvalid_1's RMSPE: 0.2504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttraining's rmse: 0.000366578\ttraining's RMSPE: 0.2448\tvalid_1's rmse: 0.000383082\tvalid_1's RMSPE: 0.2491\n",
      "[900]\ttraining's rmse: 0.000363851\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000381251\tvalid_1's RMSPE: 0.2479\n",
      "[1000]\ttraining's rmse: 0.000361386\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000379782\tvalid_1's RMSPE: 0.247\n",
      "[1100]\ttraining's rmse: 0.000359249\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000378573\tvalid_1's RMSPE: 0.2462\n",
      "[1200]\ttraining's rmse: 0.000357309\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000377492\tvalid_1's RMSPE: 0.2455\n",
      "[1300]\ttraining's rmse: 0.000355517\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000376617\tvalid_1's RMSPE: 0.2449\n",
      "[1400]\ttraining's rmse: 0.000353835\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000375678\tvalid_1's RMSPE: 0.2443\n",
      "[1500]\ttraining's rmse: 0.000352286\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000375021\tvalid_1's RMSPE: 0.2439\n",
      "[1600]\ttraining's rmse: 0.000350881\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000374643\tvalid_1's RMSPE: 0.2436\n",
      "[1700]\ttraining's rmse: 0.000349526\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000374222\tvalid_1's RMSPE: 0.2434\n",
      "[1800]\ttraining's rmse: 0.000348278\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000373693\tvalid_1's RMSPE: 0.243\n",
      "[1900]\ttraining's rmse: 0.000347032\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000373185\tvalid_1's RMSPE: 0.2427\n",
      "[2000]\ttraining's rmse: 0.000345865\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.00037278\tvalid_1's RMSPE: 0.2424\n",
      "[2100]\ttraining's rmse: 0.000344721\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.000372455\tvalid_1's RMSPE: 0.2422\n",
      "[2200]\ttraining's rmse: 0.000343692\ttraining's RMSPE: 0.2295\tvalid_1's rmse: 0.000372084\tvalid_1's RMSPE: 0.242\n",
      "[2300]\ttraining's rmse: 0.000342617\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000371823\tvalid_1's RMSPE: 0.2418\n",
      "[2400]\ttraining's rmse: 0.000341631\ttraining's RMSPE: 0.2281\tvalid_1's rmse: 0.000371594\tvalid_1's RMSPE: 0.2417\n",
      "Early stopping, best iteration is:\n",
      "[2388]\ttraining's rmse: 0.000341756\ttraining's RMSPE: 0.2282\tvalid_1's rmse: 0.000371583\tvalid_1's RMSPE: 0.2416\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2417\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136509, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001248\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516693\ttraining's RMSPE: 0.338\tvalid_1's rmse: 0.000629209\tvalid_1's RMSPE: 0.4425\n",
      "[200]\ttraining's rmse: 0.000422712\ttraining's RMSPE: 0.2765\tvalid_1's rmse: 0.000546761\tvalid_1's RMSPE: 0.3845\n",
      "[300]\ttraining's rmse: 0.000398806\ttraining's RMSPE: 0.2609\tvalid_1's rmse: 0.000513657\tvalid_1's RMSPE: 0.3612\n",
      "[400]\ttraining's rmse: 0.000389068\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000491074\tvalid_1's RMSPE: 0.3453\n",
      "[500]\ttraining's rmse: 0.00038298\ttraining's RMSPE: 0.2505\tvalid_1's rmse: 0.000476778\tvalid_1's RMSPE: 0.3353\n",
      "[600]\ttraining's rmse: 0.000378377\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000470415\tvalid_1's RMSPE: 0.3308\n",
      "[700]\ttraining's rmse: 0.000374655\ttraining's RMSPE: 0.2451\tvalid_1's rmse: 0.000466278\tvalid_1's RMSPE: 0.3279\n",
      "[800]\ttraining's rmse: 0.000371459\ttraining's RMSPE: 0.243\tvalid_1's rmse: 0.000465553\tvalid_1's RMSPE: 0.3274\n",
      "[900]\ttraining's rmse: 0.000368665\ttraining's RMSPE: 0.2412\tvalid_1's rmse: 0.000461179\tvalid_1's RMSPE: 0.3243\n",
      "[1000]\ttraining's rmse: 0.000366221\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000459255\tvalid_1's RMSPE: 0.323\n",
      "[1100]\ttraining's rmse: 0.000364082\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000459862\tvalid_1's RMSPE: 0.3234\n",
      "Early stopping, best iteration is:\n",
      "[1016]\ttraining's rmse: 0.000365848\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000458396\tvalid_1's RMSPE: 0.3223\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.3223\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001200\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516531\ttraining's RMSPE: 0.3444\tvalid_1's rmse: 0.000518776\tvalid_1's RMSPE: 0.3393\n",
      "[200]\ttraining's rmse: 0.000418392\ttraining's RMSPE: 0.279\tvalid_1's rmse: 0.000429728\tvalid_1's RMSPE: 0.2811\n",
      "[300]\ttraining's rmse: 0.000393089\ttraining's RMSPE: 0.2621\tvalid_1's rmse: 0.000408291\tvalid_1's RMSPE: 0.267\n",
      "[400]\ttraining's rmse: 0.000383255\ttraining's RMSPE: 0.2556\tvalid_1's rmse: 0.000400585\tvalid_1's RMSPE: 0.262\n",
      "[500]\ttraining's rmse: 0.000377403\ttraining's RMSPE: 0.2516\tvalid_1's rmse: 0.000396642\tvalid_1's RMSPE: 0.2594\n",
      "[600]\ttraining's rmse: 0.000372966\ttraining's RMSPE: 0.2487\tvalid_1's rmse: 0.000393814\tvalid_1's RMSPE: 0.2576\n",
      "[700]\ttraining's rmse: 0.000369236\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000391669\tvalid_1's RMSPE: 0.2562\n",
      "[800]\ttraining's rmse: 0.000366024\ttraining's RMSPE: 0.2441\tvalid_1's rmse: 0.00039\tvalid_1's RMSPE: 0.2551\n",
      "[900]\ttraining's rmse: 0.000363253\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000388314\tvalid_1's RMSPE: 0.254\n",
      "[1000]\ttraining's rmse: 0.000360866\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000387111\tvalid_1's RMSPE: 0.2532\n",
      "[1100]\ttraining's rmse: 0.000358621\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.0003862\tvalid_1's RMSPE: 0.2526\n",
      "[1200]\ttraining's rmse: 0.000356711\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.000385316\tvalid_1's RMSPE: 0.252\n",
      "[1300]\ttraining's rmse: 0.000354887\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000384537\tvalid_1's RMSPE: 0.2515\n",
      "[1400]\ttraining's rmse: 0.000353188\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.00038387\tvalid_1's RMSPE: 0.2511\n",
      "[1500]\ttraining's rmse: 0.000351622\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000383243\tvalid_1's RMSPE: 0.2507\n",
      "[1600]\ttraining's rmse: 0.000350227\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000382641\tvalid_1's RMSPE: 0.2503\n",
      "[1700]\ttraining's rmse: 0.000348915\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000382304\tvalid_1's RMSPE: 0.25\n",
      "[1800]\ttraining's rmse: 0.000347565\ttraining's RMSPE: 0.2318\tvalid_1's rmse: 0.000381936\tvalid_1's RMSPE: 0.2498\n",
      "[1900]\ttraining's rmse: 0.000346334\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000381602\tvalid_1's RMSPE: 0.2496\n",
      "[2000]\ttraining's rmse: 0.000345215\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.000381213\tvalid_1's RMSPE: 0.2493\n",
      "[2100]\ttraining's rmse: 0.000344128\ttraining's RMSPE: 0.2295\tvalid_1's rmse: 0.00038095\tvalid_1's RMSPE: 0.2492\n",
      "[2200]\ttraining's rmse: 0.000343087\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000380605\tvalid_1's RMSPE: 0.2489\n",
      "Early stopping, best iteration is:\n",
      "[2189]\ttraining's rmse: 0.000343206\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000380623\tvalid_1's RMSPE: 0.2489\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2489\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001200\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000514912\ttraining's RMSPE: 0.3433\tvalid_1's rmse: 0.000533216\tvalid_1's RMSPE: 0.3488\n",
      "[200]\ttraining's rmse: 0.00041662\ttraining's RMSPE: 0.2778\tvalid_1's rmse: 0.000455338\tvalid_1's RMSPE: 0.2978\n",
      "[300]\ttraining's rmse: 0.000391844\ttraining's RMSPE: 0.2613\tvalid_1's rmse: 0.000440137\tvalid_1's RMSPE: 0.2879\n",
      "[400]\ttraining's rmse: 0.000382396\ttraining's RMSPE: 0.255\tvalid_1's rmse: 0.00043556\tvalid_1's RMSPE: 0.2849\n",
      "[500]\ttraining's rmse: 0.000376709\ttraining's RMSPE: 0.2512\tvalid_1's rmse: 0.000432979\tvalid_1's RMSPE: 0.2832\n",
      "[600]\ttraining's rmse: 0.000372366\ttraining's RMSPE: 0.2483\tvalid_1's rmse: 0.000429969\tvalid_1's RMSPE: 0.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\ttraining's rmse: 0.000368766\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000428529\tvalid_1's RMSPE: 0.2803\n",
      "[800]\ttraining's rmse: 0.000365726\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000427647\tvalid_1's RMSPE: 0.2797\n",
      "[900]\ttraining's rmse: 0.000363052\ttraining's RMSPE: 0.2421\tvalid_1's rmse: 0.000427095\tvalid_1's RMSPE: 0.2793\n",
      "[1000]\ttraining's rmse: 0.000360683\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000426464\tvalid_1's RMSPE: 0.2789\n",
      "[1100]\ttraining's rmse: 0.000358508\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.0004258\tvalid_1's RMSPE: 0.2785\n",
      "Early stopping, best iteration is:\n",
      "[1090]\ttraining's rmse: 0.000358719\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000425851\tvalid_1's RMSPE: 0.2785\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2785\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001204\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000515962\ttraining's RMSPE: 0.3435\tvalid_1's rmse: 0.00052229\tvalid_1's RMSPE: 0.3438\n",
      "[200]\ttraining's rmse: 0.000417441\ttraining's RMSPE: 0.2779\tvalid_1's rmse: 0.000430532\tvalid_1's RMSPE: 0.2834\n",
      "[300]\ttraining's rmse: 0.000392148\ttraining's RMSPE: 0.2611\tvalid_1's rmse: 0.00040813\tvalid_1's RMSPE: 0.2686\n",
      "[400]\ttraining's rmse: 0.000382323\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000400247\tvalid_1's RMSPE: 0.2634\n",
      "[500]\ttraining's rmse: 0.000376446\ttraining's RMSPE: 0.2506\tvalid_1's rmse: 0.000396257\tvalid_1's RMSPE: 0.2608\n",
      "[600]\ttraining's rmse: 0.000372106\ttraining's RMSPE: 0.2477\tvalid_1's rmse: 0.000393403\tvalid_1's RMSPE: 0.2589\n",
      "[700]\ttraining's rmse: 0.000368316\ttraining's RMSPE: 0.2452\tvalid_1's rmse: 0.000391176\tvalid_1's RMSPE: 0.2575\n",
      "[800]\ttraining's rmse: 0.000365217\ttraining's RMSPE: 0.2432\tvalid_1's rmse: 0.000389513\tvalid_1's RMSPE: 0.2564\n",
      "[900]\ttraining's rmse: 0.00036246\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000388109\tvalid_1's RMSPE: 0.2555\n",
      "[1000]\ttraining's rmse: 0.000360062\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000387009\tvalid_1's RMSPE: 0.2547\n",
      "[1100]\ttraining's rmse: 0.000357892\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.00038603\tvalid_1's RMSPE: 0.2541\n",
      "[1200]\ttraining's rmse: 0.000355883\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000385267\tvalid_1's RMSPE: 0.2536\n",
      "[1300]\ttraining's rmse: 0.000354106\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000384539\tvalid_1's RMSPE: 0.2531\n",
      "[1400]\ttraining's rmse: 0.000352487\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.00038387\tvalid_1's RMSPE: 0.2527\n",
      "[1500]\ttraining's rmse: 0.000351018\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000383415\tvalid_1's RMSPE: 0.2524\n",
      "[1600]\ttraining's rmse: 0.000349557\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000383012\tvalid_1's RMSPE: 0.2521\n",
      "[1700]\ttraining's rmse: 0.000348209\ttraining's RMSPE: 0.2318\tvalid_1's rmse: 0.000382586\tvalid_1's RMSPE: 0.2518\n",
      "[1800]\ttraining's rmse: 0.000346964\ttraining's RMSPE: 0.231\tvalid_1's rmse: 0.000382268\tvalid_1's RMSPE: 0.2516\n",
      "[1900]\ttraining's rmse: 0.000345783\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.000382014\tvalid_1's RMSPE: 0.2514\n",
      "[2000]\ttraining's rmse: 0.000344641\ttraining's RMSPE: 0.2295\tvalid_1's rmse: 0.000381795\tvalid_1's RMSPE: 0.2513\n",
      "[2100]\ttraining's rmse: 0.00034355\ttraining's RMSPE: 0.2287\tvalid_1's rmse: 0.000381559\tvalid_1's RMSPE: 0.2511\n",
      "[2200]\ttraining's rmse: 0.000342492\ttraining's RMSPE: 0.228\tvalid_1's rmse: 0.000381359\tvalid_1's RMSPE: 0.251\n",
      "[2300]\ttraining's rmse: 0.000341506\ttraining's RMSPE: 0.2274\tvalid_1's rmse: 0.000381179\tvalid_1's RMSPE: 0.2509\n",
      "[2400]\ttraining's rmse: 0.00034054\ttraining's RMSPE: 0.2267\tvalid_1's rmse: 0.000381026\tvalid_1's RMSPE: 0.2508\n",
      "Early stopping, best iteration is:\n",
      "[2340]\ttraining's rmse: 0.000341101\ttraining's RMSPE: 0.2271\tvalid_1's rmse: 0.000381101\tvalid_1's RMSPE: 0.2508\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2508\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 3\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001204\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517982\ttraining's RMSPE: 0.3447\tvalid_1's rmse: 0.000519905\tvalid_1's RMSPE: 0.3411\n",
      "[200]\ttraining's rmse: 0.00041955\ttraining's RMSPE: 0.2792\tvalid_1's rmse: 0.000432842\tvalid_1's RMSPE: 0.284\n",
      "[300]\ttraining's rmse: 0.000394293\ttraining's RMSPE: 0.2624\tvalid_1's rmse: 0.000412666\tvalid_1's RMSPE: 0.2708\n",
      "[400]\ttraining's rmse: 0.00038438\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000405375\tvalid_1's RMSPE: 0.266\n",
      "[500]\ttraining's rmse: 0.000378453\ttraining's RMSPE: 0.2518\tvalid_1's rmse: 0.000401017\tvalid_1's RMSPE: 0.2631\n",
      "[600]\ttraining's rmse: 0.000373994\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.000398046\tvalid_1's RMSPE: 0.2612\n",
      "[700]\ttraining's rmse: 0.000370286\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.000395536\tvalid_1's RMSPE: 0.2595\n",
      "[800]\ttraining's rmse: 0.000367137\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000393703\tvalid_1's RMSPE: 0.2583\n",
      "[900]\ttraining's rmse: 0.000364323\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000392143\tvalid_1's RMSPE: 0.2573\n",
      "[1000]\ttraining's rmse: 0.000361819\ttraining's RMSPE: 0.2408\tvalid_1's rmse: 0.000390723\tvalid_1's RMSPE: 0.2564\n",
      "[1100]\ttraining's rmse: 0.000359703\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.0003896\tvalid_1's RMSPE: 0.2556\n",
      "[1200]\ttraining's rmse: 0.000357702\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000388846\tvalid_1's RMSPE: 0.2551\n",
      "[1300]\ttraining's rmse: 0.000355941\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000388065\tvalid_1's RMSPE: 0.2546\n",
      "[1400]\ttraining's rmse: 0.00035423\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000387431\tvalid_1's RMSPE: 0.2542\n",
      "[1500]\ttraining's rmse: 0.000352649\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000386854\tvalid_1's RMSPE: 0.2538\n",
      "[1600]\ttraining's rmse: 0.000351169\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.00038644\tvalid_1's RMSPE: 0.2536\n",
      "[1700]\ttraining's rmse: 0.000349803\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.00038589\tvalid_1's RMSPE: 0.2532\n",
      "[1800]\ttraining's rmse: 0.000348506\ttraining's RMSPE: 0.2319\tvalid_1's rmse: 0.00038553\tvalid_1's RMSPE: 0.253\n",
      "[1900]\ttraining's rmse: 0.000347279\ttraining's RMSPE: 0.2311\tvalid_1's rmse: 0.000385362\tvalid_1's RMSPE: 0.2529\n",
      "[2000]\ttraining's rmse: 0.000346112\ttraining's RMSPE: 0.2303\tvalid_1's rmse: 0.000385049\tvalid_1's RMSPE: 0.2527\n",
      "[2100]\ttraining's rmse: 0.00034502\ttraining's RMSPE: 0.2296\tvalid_1's rmse: 0.000384714\tvalid_1's RMSPE: 0.2524\n",
      "[2200]\ttraining's rmse: 0.000343932\ttraining's RMSPE: 0.2289\tvalid_1's rmse: 0.000384515\tvalid_1's RMSPE: 0.2523\n",
      "[2300]\ttraining's rmse: 0.000342901\ttraining's RMSPE: 0.2282\tvalid_1's rmse: 0.000384395\tvalid_1's RMSPE: 0.2522\n",
      "Early stopping, best iteration is:\n",
      "[2251]\ttraining's rmse: 0.000343388\ttraining's RMSPE: 0.2285\tvalid_1's rmse: 0.000384428\tvalid_1's RMSPE: 0.2522\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2522\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001201\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516203\ttraining's RMSPE: 0.3442\tvalid_1's rmse: 0.000530299\tvalid_1's RMSPE: 0.3452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.000418534\ttraining's RMSPE: 0.279\tvalid_1's rmse: 0.00044962\tvalid_1's RMSPE: 0.2927\n",
      "[300]\ttraining's rmse: 0.000393355\ttraining's RMSPE: 0.2623\tvalid_1's rmse: 0.000432343\tvalid_1's RMSPE: 0.2814\n",
      "[400]\ttraining's rmse: 0.000383716\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000426893\tvalid_1's RMSPE: 0.2779\n",
      "[500]\ttraining's rmse: 0.000377844\ttraining's RMSPE: 0.2519\tvalid_1's rmse: 0.000423716\tvalid_1's RMSPE: 0.2758\n",
      "[600]\ttraining's rmse: 0.000373382\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.00042139\tvalid_1's RMSPE: 0.2743\n",
      "[700]\ttraining's rmse: 0.000369688\ttraining's RMSPE: 0.2465\tvalid_1's rmse: 0.000419937\tvalid_1's RMSPE: 0.2733\n",
      "[800]\ttraining's rmse: 0.000366552\ttraining's RMSPE: 0.2444\tvalid_1's rmse: 0.000418556\tvalid_1's RMSPE: 0.2724\n",
      "[900]\ttraining's rmse: 0.000363751\ttraining's RMSPE: 0.2425\tvalid_1's rmse: 0.000417801\tvalid_1's RMSPE: 0.272\n",
      "[1000]\ttraining's rmse: 0.000361274\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000416844\tvalid_1's RMSPE: 0.2713\n",
      "[1100]\ttraining's rmse: 0.000359037\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000415966\tvalid_1's RMSPE: 0.2708\n",
      "[1200]\ttraining's rmse: 0.000356992\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000415732\tvalid_1's RMSPE: 0.2706\n",
      "[1300]\ttraining's rmse: 0.000355216\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000415641\tvalid_1's RMSPE: 0.2705\n",
      "Early stopping, best iteration is:\n",
      "[1249]\ttraining's rmse: 0.000356075\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000415332\tvalid_1's RMSPE: 0.2703\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2703\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001205\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00051741\ttraining's RMSPE: 0.3444\tvalid_1's rmse: 0.000526349\tvalid_1's RMSPE: 0.3448\n",
      "[200]\ttraining's rmse: 0.00042089\ttraining's RMSPE: 0.2802\tvalid_1's rmse: 0.000433278\tvalid_1's RMSPE: 0.2839\n",
      "[300]\ttraining's rmse: 0.000395688\ttraining's RMSPE: 0.2634\tvalid_1's rmse: 0.000409352\tvalid_1's RMSPE: 0.2682\n",
      "[400]\ttraining's rmse: 0.000385833\ttraining's RMSPE: 0.2568\tvalid_1's rmse: 0.000401082\tvalid_1's RMSPE: 0.2628\n",
      "[500]\ttraining's rmse: 0.000379866\ttraining's RMSPE: 0.2529\tvalid_1's rmse: 0.000396704\tvalid_1's RMSPE: 0.2599\n",
      "[600]\ttraining's rmse: 0.000375327\ttraining's RMSPE: 0.2498\tvalid_1's rmse: 0.000394038\tvalid_1's RMSPE: 0.2582\n",
      "[700]\ttraining's rmse: 0.000371576\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.00039149\tvalid_1's RMSPE: 0.2565\n",
      "[800]\ttraining's rmse: 0.000368368\ttraining's RMSPE: 0.2452\tvalid_1's rmse: 0.000389482\tvalid_1's RMSPE: 0.2552\n",
      "[900]\ttraining's rmse: 0.000365487\ttraining's RMSPE: 0.2433\tvalid_1's rmse: 0.000387835\tvalid_1's RMSPE: 0.2541\n",
      "[1000]\ttraining's rmse: 0.000363037\ttraining's RMSPE: 0.2417\tvalid_1's rmse: 0.000386769\tvalid_1's RMSPE: 0.2534\n",
      "[1100]\ttraining's rmse: 0.000360687\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000385866\tvalid_1's RMSPE: 0.2528\n",
      "[1200]\ttraining's rmse: 0.0003587\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000384955\tvalid_1's RMSPE: 0.2522\n",
      "[1300]\ttraining's rmse: 0.00035678\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000383853\tvalid_1's RMSPE: 0.2515\n",
      "[1400]\ttraining's rmse: 0.000355033\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000383162\tvalid_1's RMSPE: 0.251\n",
      "[1500]\ttraining's rmse: 0.000353434\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.00038263\tvalid_1's RMSPE: 0.2507\n",
      "[1600]\ttraining's rmse: 0.00035193\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000382136\tvalid_1's RMSPE: 0.2504\n",
      "[1700]\ttraining's rmse: 0.000350493\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000381463\tvalid_1's RMSPE: 0.2499\n",
      "[1800]\ttraining's rmse: 0.00034922\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000381084\tvalid_1's RMSPE: 0.2497\n",
      "[1900]\ttraining's rmse: 0.00034804\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000380812\tvalid_1's RMSPE: 0.2495\n",
      "[2000]\ttraining's rmse: 0.000346871\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000380636\tvalid_1's RMSPE: 0.2494\n",
      "[2100]\ttraining's rmse: 0.000345746\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.00038027\tvalid_1's RMSPE: 0.2491\n",
      "[2200]\ttraining's rmse: 0.000344613\ttraining's RMSPE: 0.2294\tvalid_1's rmse: 0.000380015\tvalid_1's RMSPE: 0.249\n",
      "Early stopping, best iteration is:\n",
      "[2125]\ttraining's rmse: 0.00034545\ttraining's RMSPE: 0.23\tvalid_1's rmse: 0.000380142\tvalid_1's RMSPE: 0.249\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.249\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001203\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517383\ttraining's RMSPE: 0.3445\tvalid_1's rmse: 0.000520387\tvalid_1's RMSPE: 0.3406\n",
      "[200]\ttraining's rmse: 0.000419467\ttraining's RMSPE: 0.2793\tvalid_1's rmse: 0.000431385\tvalid_1's RMSPE: 0.2823\n",
      "[300]\ttraining's rmse: 0.000393871\ttraining's RMSPE: 0.2623\tvalid_1's rmse: 0.000409935\tvalid_1's RMSPE: 0.2683\n",
      "[400]\ttraining's rmse: 0.000384003\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.000402657\tvalid_1's RMSPE: 0.2635\n",
      "[500]\ttraining's rmse: 0.000378052\ttraining's RMSPE: 0.2517\tvalid_1's rmse: 0.000398722\tvalid_1's RMSPE: 0.2609\n",
      "[600]\ttraining's rmse: 0.000373574\ttraining's RMSPE: 0.2487\tvalid_1's rmse: 0.00039587\tvalid_1's RMSPE: 0.2591\n",
      "[700]\ttraining's rmse: 0.000369795\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000393557\tvalid_1's RMSPE: 0.2576\n",
      "[800]\ttraining's rmse: 0.000366609\ttraining's RMSPE: 0.2441\tvalid_1's rmse: 0.000391726\tvalid_1's RMSPE: 0.2564\n",
      "[900]\ttraining's rmse: 0.000363882\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000390339\tvalid_1's RMSPE: 0.2555\n",
      "[1000]\ttraining's rmse: 0.000361395\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000389168\tvalid_1's RMSPE: 0.2547\n",
      "[1100]\ttraining's rmse: 0.000359204\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.0003881\tvalid_1's RMSPE: 0.254\n",
      "[1200]\ttraining's rmse: 0.000357203\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000387167\tvalid_1's RMSPE: 0.2534\n",
      "[1300]\ttraining's rmse: 0.0003553\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000386368\tvalid_1's RMSPE: 0.2529\n",
      "[1400]\ttraining's rmse: 0.000353594\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000385769\tvalid_1's RMSPE: 0.2525\n",
      "[1500]\ttraining's rmse: 0.000352037\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000385149\tvalid_1's RMSPE: 0.2521\n",
      "[1600]\ttraining's rmse: 0.000350584\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000384779\tvalid_1's RMSPE: 0.2518\n",
      "[1700]\ttraining's rmse: 0.000349261\ttraining's RMSPE: 0.2326\tvalid_1's rmse: 0.000384382\tvalid_1's RMSPE: 0.2516\n",
      "[1800]\ttraining's rmse: 0.000347972\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000383973\tvalid_1's RMSPE: 0.2513\n",
      "[1900]\ttraining's rmse: 0.000346712\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000383605\tvalid_1's RMSPE: 0.251\n",
      "[2000]\ttraining's rmse: 0.00034556\ttraining's RMSPE: 0.2301\tvalid_1's rmse: 0.000383331\tvalid_1's RMSPE: 0.2509\n",
      "[2100]\ttraining's rmse: 0.000344433\ttraining's RMSPE: 0.2293\tvalid_1's rmse: 0.000383041\tvalid_1's RMSPE: 0.2507\n",
      "[2200]\ttraining's rmse: 0.000343388\ttraining's RMSPE: 0.2286\tvalid_1's rmse: 0.000382722\tvalid_1's RMSPE: 0.2505\n",
      "[2300]\ttraining's rmse: 0.000342308\ttraining's RMSPE: 0.2279\tvalid_1's rmse: 0.000382413\tvalid_1's RMSPE: 0.2503\n",
      "[2400]\ttraining's rmse: 0.000341302\ttraining's RMSPE: 0.2273\tvalid_1's rmse: 0.000382186\tvalid_1's RMSPE: 0.2501\n",
      "[2500]\ttraining's rmse: 0.000340308\ttraining's RMSPE: 0.2266\tvalid_1's rmse: 0.000382038\tvalid_1's RMSPE: 0.25\n",
      "[2600]\ttraining's rmse: 0.00033936\ttraining's RMSPE: 0.226\tvalid_1's rmse: 0.000381827\tvalid_1's RMSPE: 0.2499\n",
      "[2700]\ttraining's rmse: 0.000338478\ttraining's RMSPE: 0.2254\tvalid_1's rmse: 0.000381636\tvalid_1's RMSPE: 0.2498\n",
      "[2800]\ttraining's rmse: 0.000337607\ttraining's RMSPE: 0.2248\tvalid_1's rmse: 0.000381557\tvalid_1's RMSPE: 0.2497\n",
      "Early stopping, best iteration is:\n",
      "[2709]\ttraining's rmse: 0.000338401\ttraining's RMSPE: 0.2253\tvalid_1's rmse: 0.000381625\tvalid_1's RMSPE: 0.2497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2497\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001248\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516316\ttraining's RMSPE: 0.3378\tvalid_1's rmse: 0.000588461\tvalid_1's RMSPE: 0.4119\n",
      "[200]\ttraining's rmse: 0.000423563\ttraining's RMSPE: 0.2771\tvalid_1's rmse: 0.000482102\tvalid_1's RMSPE: 0.3375\n",
      "[300]\ttraining's rmse: 0.000399921\ttraining's RMSPE: 0.2616\tvalid_1's rmse: 0.000444448\tvalid_1's RMSPE: 0.3111\n",
      "[400]\ttraining's rmse: 0.000390152\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000425825\tvalid_1's RMSPE: 0.2981\n",
      "[500]\ttraining's rmse: 0.000384215\ttraining's RMSPE: 0.2513\tvalid_1's rmse: 0.000416213\tvalid_1's RMSPE: 0.2913\n",
      "[600]\ttraining's rmse: 0.000379794\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000412721\tvalid_1's RMSPE: 0.2889\n",
      "[700]\ttraining's rmse: 0.00037604\ttraining's RMSPE: 0.246\tvalid_1's rmse: 0.000409205\tvalid_1's RMSPE: 0.2864\n",
      "[800]\ttraining's rmse: 0.00037284\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000407947\tvalid_1's RMSPE: 0.2856\n",
      "[900]\ttraining's rmse: 0.000369997\ttraining's RMSPE: 0.242\tvalid_1's rmse: 0.000404614\tvalid_1's RMSPE: 0.2832\n",
      "[1000]\ttraining's rmse: 0.000367493\ttraining's RMSPE: 0.2404\tvalid_1's rmse: 0.000403051\tvalid_1's RMSPE: 0.2821\n",
      "[1100]\ttraining's rmse: 0.000365256\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000400952\tvalid_1's RMSPE: 0.2807\n",
      "[1200]\ttraining's rmse: 0.000363149\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.00040019\tvalid_1's RMSPE: 0.2801\n",
      "Early stopping, best iteration is:\n",
      "[1192]\ttraining's rmse: 0.000363315\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.000400023\tvalid_1's RMSPE: 0.28\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.28\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 4\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001253\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517984\ttraining's RMSPE: 0.3376\tvalid_1's rmse: 0.000523519\tvalid_1's RMSPE: 0.3452\n",
      "[200]\ttraining's rmse: 0.000424626\ttraining's RMSPE: 0.2768\tvalid_1's rmse: 0.000438977\tvalid_1's RMSPE: 0.2894\n",
      "[300]\ttraining's rmse: 0.000400882\ttraining's RMSPE: 0.2613\tvalid_1's rmse: 0.000421306\tvalid_1's RMSPE: 0.2778\n",
      "[400]\ttraining's rmse: 0.000391325\ttraining's RMSPE: 0.2551\tvalid_1's rmse: 0.000415404\tvalid_1's RMSPE: 0.2739\n",
      "[500]\ttraining's rmse: 0.000385446\ttraining's RMSPE: 0.2512\tvalid_1's rmse: 0.000412091\tvalid_1's RMSPE: 0.2717\n",
      "[600]\ttraining's rmse: 0.000381076\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000409851\tvalid_1's RMSPE: 0.2702\n",
      "[700]\ttraining's rmse: 0.000377273\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000408068\tvalid_1's RMSPE: 0.269\n",
      "[800]\ttraining's rmse: 0.000374195\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000407454\tvalid_1's RMSPE: 0.2686\n",
      "[900]\ttraining's rmse: 0.000371438\ttraining's RMSPE: 0.2421\tvalid_1's rmse: 0.000406303\tvalid_1's RMSPE: 0.2679\n",
      "[1000]\ttraining's rmse: 0.000368934\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000405662\tvalid_1's RMSPE: 0.2675\n",
      "[1100]\ttraining's rmse: 0.000366815\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000404763\tvalid_1's RMSPE: 0.2669\n",
      "[1200]\ttraining's rmse: 0.000364792\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000404288\tvalid_1's RMSPE: 0.2665\n",
      "[1300]\ttraining's rmse: 0.00036291\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.00040441\tvalid_1's RMSPE: 0.2666\n",
      "Early stopping, best iteration is:\n",
      "[1220]\ttraining's rmse: 0.000364419\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000403991\tvalid_1's RMSPE: 0.2663\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2663\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001248\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000518001\ttraining's RMSPE: 0.3385\tvalid_1's rmse: 0.000521379\tvalid_1's RMSPE: 0.3403\n",
      "[200]\ttraining's rmse: 0.000424744\ttraining's RMSPE: 0.2775\tvalid_1's rmse: 0.000429901\tvalid_1's RMSPE: 0.2806\n",
      "[300]\ttraining's rmse: 0.000401155\ttraining's RMSPE: 0.2621\tvalid_1's rmse: 0.00040731\tvalid_1's RMSPE: 0.2659\n",
      "[400]\ttraining's rmse: 0.000391438\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000399114\tvalid_1's RMSPE: 0.2605\n",
      "[500]\ttraining's rmse: 0.000385413\ttraining's RMSPE: 0.2518\tvalid_1's rmse: 0.000394759\tvalid_1's RMSPE: 0.2577\n",
      "[600]\ttraining's rmse: 0.00038089\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.000391886\tvalid_1's RMSPE: 0.2558\n",
      "[700]\ttraining's rmse: 0.000377041\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.000389728\tvalid_1's RMSPE: 0.2544\n",
      "[800]\ttraining's rmse: 0.000373818\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000388116\tvalid_1's RMSPE: 0.2533\n",
      "[900]\ttraining's rmse: 0.000371055\ttraining's RMSPE: 0.2425\tvalid_1's rmse: 0.000386673\tvalid_1's RMSPE: 0.2524\n",
      "[1000]\ttraining's rmse: 0.00036846\ttraining's RMSPE: 0.2408\tvalid_1's rmse: 0.000385433\tvalid_1's RMSPE: 0.2516\n",
      "[1100]\ttraining's rmse: 0.000366214\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000384438\tvalid_1's RMSPE: 0.2509\n",
      "[1200]\ttraining's rmse: 0.000364227\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000383665\tvalid_1's RMSPE: 0.2504\n",
      "[1300]\ttraining's rmse: 0.00036234\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000382918\tvalid_1's RMSPE: 0.2499\n",
      "[1400]\ttraining's rmse: 0.000360597\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000382272\tvalid_1's RMSPE: 0.2495\n",
      "[1500]\ttraining's rmse: 0.000359032\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000381748\tvalid_1's RMSPE: 0.2492\n",
      "[1600]\ttraining's rmse: 0.000357537\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000381346\tvalid_1's RMSPE: 0.2489\n",
      "[1700]\ttraining's rmse: 0.00035612\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000380955\tvalid_1's RMSPE: 0.2487\n",
      "[1800]\ttraining's rmse: 0.000354776\ttraining's RMSPE: 0.2318\tvalid_1's rmse: 0.00038055\tvalid_1's RMSPE: 0.2484\n",
      "[1900]\ttraining's rmse: 0.000353516\ttraining's RMSPE: 0.231\tvalid_1's rmse: 0.00038024\tvalid_1's RMSPE: 0.2482\n",
      "[2000]\ttraining's rmse: 0.000352334\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.00038006\tvalid_1's RMSPE: 0.2481\n",
      "[2100]\ttraining's rmse: 0.000351137\ttraining's RMSPE: 0.2294\tvalid_1's rmse: 0.000379761\tvalid_1's RMSPE: 0.2479\n",
      "[2200]\ttraining's rmse: 0.000350018\ttraining's RMSPE: 0.2287\tvalid_1's rmse: 0.000379517\tvalid_1's RMSPE: 0.2477\n",
      "[2300]\ttraining's rmse: 0.000349033\ttraining's RMSPE: 0.2281\tvalid_1's rmse: 0.000379312\tvalid_1's RMSPE: 0.2476\n",
      "Early stopping, best iteration is:\n",
      "[2227]\ttraining's rmse: 0.000349738\ttraining's RMSPE: 0.2285\tvalid_1's rmse: 0.000379414\tvalid_1's RMSPE: 0.2476\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2476\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001251\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516129\ttraining's RMSPE: 0.3369\tvalid_1's rmse: 0.000527543\tvalid_1's RMSPE: 0.3459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.00042318\ttraining's RMSPE: 0.2762\tvalid_1's rmse: 0.000438633\tvalid_1's RMSPE: 0.2876\n",
      "[300]\ttraining's rmse: 0.000399554\ttraining's RMSPE: 0.2608\tvalid_1's rmse: 0.000417843\tvalid_1's RMSPE: 0.2739\n",
      "[400]\ttraining's rmse: 0.000390043\ttraining's RMSPE: 0.2546\tvalid_1's rmse: 0.000411057\tvalid_1's RMSPE: 0.2695\n",
      "[500]\ttraining's rmse: 0.000384136\ttraining's RMSPE: 0.2507\tvalid_1's rmse: 0.000407642\tvalid_1's RMSPE: 0.2672\n",
      "[600]\ttraining's rmse: 0.000379697\ttraining's RMSPE: 0.2478\tvalid_1's rmse: 0.000405415\tvalid_1's RMSPE: 0.2658\n",
      "[700]\ttraining's rmse: 0.000376055\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000403549\tvalid_1's RMSPE: 0.2646\n",
      "[800]\ttraining's rmse: 0.000372882\ttraining's RMSPE: 0.2434\tvalid_1's rmse: 0.000402359\tvalid_1's RMSPE: 0.2638\n",
      "[900]\ttraining's rmse: 0.000370095\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.000401387\tvalid_1's RMSPE: 0.2631\n",
      "[1000]\ttraining's rmse: 0.000367585\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000400569\tvalid_1's RMSPE: 0.2626\n",
      "[1100]\ttraining's rmse: 0.000365343\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000399934\tvalid_1's RMSPE: 0.2622\n",
      "[1200]\ttraining's rmse: 0.000363397\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000399395\tvalid_1's RMSPE: 0.2618\n",
      "[1300]\ttraining's rmse: 0.000361571\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000399018\tvalid_1's RMSPE: 0.2616\n",
      "[1400]\ttraining's rmse: 0.000359869\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000398539\tvalid_1's RMSPE: 0.2613\n",
      "[1500]\ttraining's rmse: 0.000358289\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000398162\tvalid_1's RMSPE: 0.261\n",
      "[1600]\ttraining's rmse: 0.000356824\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000397894\tvalid_1's RMSPE: 0.2609\n",
      "Early stopping, best iteration is:\n",
      "[1588]\ttraining's rmse: 0.000356986\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000397872\tvalid_1's RMSPE: 0.2608\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2608\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001247\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517243\ttraining's RMSPE: 0.338\tvalid_1's rmse: 0.000519244\tvalid_1's RMSPE: 0.3387\n",
      "[200]\ttraining's rmse: 0.000423442\ttraining's RMSPE: 0.2767\tvalid_1's rmse: 0.000432252\tvalid_1's RMSPE: 0.282\n",
      "[300]\ttraining's rmse: 0.000399428\ttraining's RMSPE: 0.261\tvalid_1's rmse: 0.000411969\tvalid_1's RMSPE: 0.2687\n",
      "[400]\ttraining's rmse: 0.000389781\ttraining's RMSPE: 0.2547\tvalid_1's rmse: 0.000404513\tvalid_1's RMSPE: 0.2639\n",
      "[500]\ttraining's rmse: 0.000383841\ttraining's RMSPE: 0.2509\tvalid_1's rmse: 0.000400256\tvalid_1's RMSPE: 0.2611\n",
      "[600]\ttraining's rmse: 0.000379323\ttraining's RMSPE: 0.2479\tvalid_1's rmse: 0.000397018\tvalid_1's RMSPE: 0.259\n",
      "[700]\ttraining's rmse: 0.00037563\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000394559\tvalid_1's RMSPE: 0.2574\n",
      "[800]\ttraining's rmse: 0.000372515\ttraining's RMSPE: 0.2435\tvalid_1's rmse: 0.000392656\tvalid_1's RMSPE: 0.2561\n",
      "[900]\ttraining's rmse: 0.000369778\ttraining's RMSPE: 0.2417\tvalid_1's rmse: 0.000391018\tvalid_1's RMSPE: 0.2551\n",
      "[1000]\ttraining's rmse: 0.000367358\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000389783\tvalid_1's RMSPE: 0.2543\n",
      "[1100]\ttraining's rmse: 0.000365224\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.00038868\tvalid_1's RMSPE: 0.2535\n",
      "[1200]\ttraining's rmse: 0.000363217\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000387712\tvalid_1's RMSPE: 0.2529\n",
      "[1300]\ttraining's rmse: 0.000361358\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000386986\tvalid_1's RMSPE: 0.2524\n",
      "[1400]\ttraining's rmse: 0.000359699\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000386247\tvalid_1's RMSPE: 0.252\n",
      "[1500]\ttraining's rmse: 0.000358085\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000385543\tvalid_1's RMSPE: 0.2515\n",
      "[1600]\ttraining's rmse: 0.000356637\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000385111\tvalid_1's RMSPE: 0.2512\n",
      "[1700]\ttraining's rmse: 0.0003552\ttraining's RMSPE: 0.2321\tvalid_1's rmse: 0.00038475\tvalid_1's RMSPE: 0.251\n",
      "[1800]\ttraining's rmse: 0.000353898\ttraining's RMSPE: 0.2313\tvalid_1's rmse: 0.000384226\tvalid_1's RMSPE: 0.2506\n",
      "[1900]\ttraining's rmse: 0.000352672\ttraining's RMSPE: 0.2305\tvalid_1's rmse: 0.000383886\tvalid_1's RMSPE: 0.2504\n",
      "[2000]\ttraining's rmse: 0.000351501\ttraining's RMSPE: 0.2297\tvalid_1's rmse: 0.00038364\tvalid_1's RMSPE: 0.2503\n",
      "[2100]\ttraining's rmse: 0.00035042\ttraining's RMSPE: 0.229\tvalid_1's rmse: 0.000383287\tvalid_1's RMSPE: 0.25\n",
      "[2200]\ttraining's rmse: 0.00034929\ttraining's RMSPE: 0.2283\tvalid_1's rmse: 0.000383018\tvalid_1's RMSPE: 0.2499\n",
      "[2300]\ttraining's rmse: 0.000348249\ttraining's RMSPE: 0.2276\tvalid_1's rmse: 0.0003829\tvalid_1's RMSPE: 0.2498\n",
      "Early stopping, best iteration is:\n",
      "[2202]\ttraining's rmse: 0.000349268\ttraining's RMSPE: 0.2283\tvalid_1's rmse: 0.000383007\tvalid_1's RMSPE: 0.2498\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2498\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001244\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516485\ttraining's RMSPE: 0.3383\tvalid_1's rmse: 0.000520649\tvalid_1's RMSPE: 0.3366\n",
      "[200]\ttraining's rmse: 0.000423453\ttraining's RMSPE: 0.2774\tvalid_1's rmse: 0.00043166\tvalid_1's RMSPE: 0.2791\n",
      "[300]\ttraining's rmse: 0.000399505\ttraining's RMSPE: 0.2617\tvalid_1's rmse: 0.000411118\tvalid_1's RMSPE: 0.2658\n",
      "[400]\ttraining's rmse: 0.000389886\ttraining's RMSPE: 0.2554\tvalid_1's rmse: 0.00040401\tvalid_1's RMSPE: 0.2612\n",
      "[500]\ttraining's rmse: 0.000383962\ttraining's RMSPE: 0.2515\tvalid_1's rmse: 0.000400298\tvalid_1's RMSPE: 0.2588\n",
      "[600]\ttraining's rmse: 0.000379485\ttraining's RMSPE: 0.2486\tvalid_1's rmse: 0.000397537\tvalid_1's RMSPE: 0.257\n",
      "[700]\ttraining's rmse: 0.000375649\ttraining's RMSPE: 0.246\tvalid_1's rmse: 0.00039533\tvalid_1's RMSPE: 0.2556\n",
      "[800]\ttraining's rmse: 0.000372484\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000393592\tvalid_1's RMSPE: 0.2545\n",
      "[900]\ttraining's rmse: 0.000369689\ttraining's RMSPE: 0.2421\tvalid_1's rmse: 0.000392043\tvalid_1's RMSPE: 0.2535\n",
      "[1000]\ttraining's rmse: 0.000367249\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000391054\tvalid_1's RMSPE: 0.2528\n",
      "[1100]\ttraining's rmse: 0.000365043\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000390123\tvalid_1's RMSPE: 0.2522\n",
      "[1200]\ttraining's rmse: 0.000363007\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.00038918\tvalid_1's RMSPE: 0.2516\n",
      "[1300]\ttraining's rmse: 0.000361185\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000388552\tvalid_1's RMSPE: 0.2512\n",
      "[1400]\ttraining's rmse: 0.000359435\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000387831\tvalid_1's RMSPE: 0.2507\n",
      "[1500]\ttraining's rmse: 0.000357811\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.00038735\tvalid_1's RMSPE: 0.2504\n",
      "[1600]\ttraining's rmse: 0.000356328\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000386989\tvalid_1's RMSPE: 0.2502\n",
      "[1700]\ttraining's rmse: 0.000354942\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000386504\tvalid_1's RMSPE: 0.2499\n",
      "[1800]\ttraining's rmse: 0.000353667\ttraining's RMSPE: 0.2316\tvalid_1's rmse: 0.000386313\tvalid_1's RMSPE: 0.2498\n",
      "[1900]\ttraining's rmse: 0.000352424\ttraining's RMSPE: 0.2308\tvalid_1's rmse: 0.000386001\tvalid_1's RMSPE: 0.2496\n",
      "[2000]\ttraining's rmse: 0.000351256\ttraining's RMSPE: 0.2301\tvalid_1's rmse: 0.000385803\tvalid_1's RMSPE: 0.2494\n",
      "[2100]\ttraining's rmse: 0.000350109\ttraining's RMSPE: 0.2293\tvalid_1's rmse: 0.000385516\tvalid_1's RMSPE: 0.2492\n",
      "[2200]\ttraining's rmse: 0.000349009\ttraining's RMSPE: 0.2286\tvalid_1's rmse: 0.000385378\tvalid_1's RMSPE: 0.2491\n",
      "[2300]\ttraining's rmse: 0.000347974\ttraining's RMSPE: 0.2279\tvalid_1's rmse: 0.000385128\tvalid_1's RMSPE: 0.249\n",
      "[2400]\ttraining's rmse: 0.000346927\ttraining's RMSPE: 0.2272\tvalid_1's rmse: 0.000384866\tvalid_1's RMSPE: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500]\ttraining's rmse: 0.000345958\ttraining's RMSPE: 0.2266\tvalid_1's rmse: 0.000384603\tvalid_1's RMSPE: 0.2486\n",
      "Early stopping, best iteration is:\n",
      "[2482]\ttraining's rmse: 0.000346122\ttraining's RMSPE: 0.2267\tvalid_1's rmse: 0.000384604\tvalid_1's RMSPE: 0.2486\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2486\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 5\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001208\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000518238\ttraining's RMSPE: 0.3442\tvalid_1's rmse: 0.000522555\tvalid_1's RMSPE: 0.3446\n",
      "[200]\ttraining's rmse: 0.000420477\ttraining's RMSPE: 0.2793\tvalid_1's rmse: 0.000432724\tvalid_1's RMSPE: 0.2854\n",
      "[300]\ttraining's rmse: 0.000395362\ttraining's RMSPE: 0.2626\tvalid_1's rmse: 0.000410049\tvalid_1's RMSPE: 0.2704\n",
      "[400]\ttraining's rmse: 0.000385548\ttraining's RMSPE: 0.2561\tvalid_1's rmse: 0.000402029\tvalid_1's RMSPE: 0.2651\n",
      "[500]\ttraining's rmse: 0.000379698\ttraining's RMSPE: 0.2522\tvalid_1's rmse: 0.000398123\tvalid_1's RMSPE: 0.2625\n",
      "[600]\ttraining's rmse: 0.000375334\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.000395092\tvalid_1's RMSPE: 0.2605\n",
      "[700]\ttraining's rmse: 0.000371664\ttraining's RMSPE: 0.2469\tvalid_1's rmse: 0.000392771\tvalid_1's RMSPE: 0.259\n",
      "[800]\ttraining's rmse: 0.00036858\ttraining's RMSPE: 0.2448\tvalid_1's rmse: 0.000390972\tvalid_1's RMSPE: 0.2578\n",
      "[900]\ttraining's rmse: 0.00036585\ttraining's RMSPE: 0.243\tvalid_1's rmse: 0.000389555\tvalid_1's RMSPE: 0.2569\n",
      "[1000]\ttraining's rmse: 0.000363361\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000388379\tvalid_1's RMSPE: 0.2561\n",
      "[1100]\ttraining's rmse: 0.000361191\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000387394\tvalid_1's RMSPE: 0.2555\n",
      "[1200]\ttraining's rmse: 0.00035918\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000386456\tvalid_1's RMSPE: 0.2548\n",
      "[1300]\ttraining's rmse: 0.000357383\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000385917\tvalid_1's RMSPE: 0.2545\n",
      "[1400]\ttraining's rmse: 0.000355755\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000385216\tvalid_1's RMSPE: 0.254\n",
      "[1500]\ttraining's rmse: 0.000354178\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000384709\tvalid_1's RMSPE: 0.2537\n",
      "[1600]\ttraining's rmse: 0.000352642\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000384019\tvalid_1's RMSPE: 0.2532\n",
      "[1700]\ttraining's rmse: 0.000351249\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000383544\tvalid_1's RMSPE: 0.2529\n",
      "[1800]\ttraining's rmse: 0.000349942\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000383016\tvalid_1's RMSPE: 0.2526\n",
      "[1900]\ttraining's rmse: 0.000348729\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000382643\tvalid_1's RMSPE: 0.2523\n",
      "[2000]\ttraining's rmse: 0.00034752\ttraining's RMSPE: 0.2308\tvalid_1's rmse: 0.000382283\tvalid_1's RMSPE: 0.2521\n",
      "[2100]\ttraining's rmse: 0.000346392\ttraining's RMSPE: 0.2301\tvalid_1's rmse: 0.000382018\tvalid_1's RMSPE: 0.2519\n",
      "Early stopping, best iteration is:\n",
      "[2070]\ttraining's rmse: 0.000346758\ttraining's RMSPE: 0.2303\tvalid_1's rmse: 0.000382066\tvalid_1's RMSPE: 0.2519\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2519\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001250\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517379\ttraining's RMSPE: 0.3378\tvalid_1's rmse: 0.000587189\tvalid_1's RMSPE: 0.4131\n",
      "[200]\ttraining's rmse: 0.000424346\ttraining's RMSPE: 0.277\tvalid_1's rmse: 0.000490485\tvalid_1's RMSPE: 0.345\n",
      "[300]\ttraining's rmse: 0.000400687\ttraining's RMSPE: 0.2616\tvalid_1's rmse: 0.000449677\tvalid_1's RMSPE: 0.3163\n",
      "[400]\ttraining's rmse: 0.000391031\ttraining's RMSPE: 0.2553\tvalid_1's rmse: 0.000432553\tvalid_1's RMSPE: 0.3043\n",
      "[500]\ttraining's rmse: 0.000385152\ttraining's RMSPE: 0.2515\tvalid_1's rmse: 0.00042386\tvalid_1's RMSPE: 0.2982\n",
      "[600]\ttraining's rmse: 0.000380605\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.00041941\tvalid_1's RMSPE: 0.295\n",
      "[700]\ttraining's rmse: 0.000377012\ttraining's RMSPE: 0.2461\tvalid_1's rmse: 0.00042021\tvalid_1's RMSPE: 0.2956\n",
      "Early stopping, best iteration is:\n",
      "[604]\ttraining's rmse: 0.000380447\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000418914\tvalid_1's RMSPE: 0.2947\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2947\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001200\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516758\ttraining's RMSPE: 0.3449\tvalid_1's rmse: 0.000527035\tvalid_1's RMSPE: 0.3406\n",
      "[200]\ttraining's rmse: 0.000418704\ttraining's RMSPE: 0.2795\tvalid_1's rmse: 0.000435853\tvalid_1's RMSPE: 0.2817\n",
      "[300]\ttraining's rmse: 0.000393591\ttraining's RMSPE: 0.2627\tvalid_1's rmse: 0.000413769\tvalid_1's RMSPE: 0.2674\n",
      "[400]\ttraining's rmse: 0.000383789\ttraining's RMSPE: 0.2562\tvalid_1's rmse: 0.000405813\tvalid_1's RMSPE: 0.2623\n",
      "[500]\ttraining's rmse: 0.000377938\ttraining's RMSPE: 0.2523\tvalid_1's rmse: 0.000401467\tvalid_1's RMSPE: 0.2595\n",
      "[600]\ttraining's rmse: 0.000373478\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.00039838\tvalid_1's RMSPE: 0.2575\n",
      "[700]\ttraining's rmse: 0.000369737\ttraining's RMSPE: 0.2468\tvalid_1's rmse: 0.00039595\tvalid_1's RMSPE: 0.2559\n",
      "[800]\ttraining's rmse: 0.000366626\ttraining's RMSPE: 0.2447\tvalid_1's rmse: 0.000394008\tvalid_1's RMSPE: 0.2547\n",
      "[900]\ttraining's rmse: 0.000363948\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000392403\tvalid_1's RMSPE: 0.2536\n",
      "[1000]\ttraining's rmse: 0.000361574\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000390784\tvalid_1's RMSPE: 0.2526\n",
      "[1100]\ttraining's rmse: 0.000359382\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000389844\tvalid_1's RMSPE: 0.252\n",
      "[1200]\ttraining's rmse: 0.000357422\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000388819\tvalid_1's RMSPE: 0.2513\n",
      "[1300]\ttraining's rmse: 0.000355671\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000387917\tvalid_1's RMSPE: 0.2507\n",
      "[1400]\ttraining's rmse: 0.000354067\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000387203\tvalid_1's RMSPE: 0.2503\n",
      "[1500]\ttraining's rmse: 0.000352533\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.00038655\tvalid_1's RMSPE: 0.2498\n",
      "[1600]\ttraining's rmse: 0.000351049\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000386029\tvalid_1's RMSPE: 0.2495\n",
      "[1700]\ttraining's rmse: 0.000349639\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000385535\tvalid_1's RMSPE: 0.2492\n",
      "[1800]\ttraining's rmse: 0.000348379\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000385144\tvalid_1's RMSPE: 0.2489\n",
      "[1900]\ttraining's rmse: 0.0003471\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000384644\tvalid_1's RMSPE: 0.2486\n",
      "[2000]\ttraining's rmse: 0.000345959\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000384164\tvalid_1's RMSPE: 0.2483\n",
      "[2100]\ttraining's rmse: 0.000344813\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.000383826\tvalid_1's RMSPE: 0.2481\n",
      "[2200]\ttraining's rmse: 0.000343745\ttraining's RMSPE: 0.2294\tvalid_1's rmse: 0.000383541\tvalid_1's RMSPE: 0.2479\n",
      "[2300]\ttraining's rmse: 0.000342735\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000383432\tvalid_1's RMSPE: 0.2478\n",
      "Early stopping, best iteration is:\n",
      "[2238]\ttraining's rmse: 0.000343364\ttraining's RMSPE: 0.2292\tvalid_1's rmse: 0.000383474\tvalid_1's RMSPE: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2478\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001205\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517708\ttraining's RMSPE: 0.3445\tvalid_1's rmse: 0.00052452\tvalid_1's RMSPE: 0.3434\n",
      "[200]\ttraining's rmse: 0.000419513\ttraining's RMSPE: 0.2792\tvalid_1's rmse: 0.000440433\tvalid_1's RMSPE: 0.2884\n",
      "[300]\ttraining's rmse: 0.000394738\ttraining's RMSPE: 0.2627\tvalid_1's rmse: 0.000422891\tvalid_1's RMSPE: 0.2769\n",
      "[400]\ttraining's rmse: 0.000384988\ttraining's RMSPE: 0.2562\tvalid_1's rmse: 0.000416798\tvalid_1's RMSPE: 0.2729\n",
      "[500]\ttraining's rmse: 0.000379113\ttraining's RMSPE: 0.2523\tvalid_1's rmse: 0.000413879\tvalid_1's RMSPE: 0.271\n",
      "[600]\ttraining's rmse: 0.000374647\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.000411636\tvalid_1's RMSPE: 0.2695\n",
      "[700]\ttraining's rmse: 0.000370961\ttraining's RMSPE: 0.2469\tvalid_1's rmse: 0.000409836\tvalid_1's RMSPE: 0.2683\n",
      "[800]\ttraining's rmse: 0.000367778\ttraining's RMSPE: 0.2447\tvalid_1's rmse: 0.000408247\tvalid_1's RMSPE: 0.2673\n",
      "[900]\ttraining's rmse: 0.000365082\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000406878\tvalid_1's RMSPE: 0.2664\n",
      "[1000]\ttraining's rmse: 0.000362711\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.0004062\tvalid_1's RMSPE: 0.2659\n",
      "[1100]\ttraining's rmse: 0.000360635\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000405998\tvalid_1's RMSPE: 0.2658\n",
      "[1200]\ttraining's rmse: 0.000358644\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000405328\tvalid_1's RMSPE: 0.2654\n",
      "[1300]\ttraining's rmse: 0.000356844\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000405041\tvalid_1's RMSPE: 0.2652\n",
      "[1400]\ttraining's rmse: 0.000355128\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000404302\tvalid_1's RMSPE: 0.2647\n",
      "[1500]\ttraining's rmse: 0.000353579\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000404009\tvalid_1's RMSPE: 0.2645\n",
      "[1600]\ttraining's rmse: 0.000352128\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000403718\tvalid_1's RMSPE: 0.2643\n",
      "Early stopping, best iteration is:\n",
      "[1521]\ttraining's rmse: 0.000353264\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000403756\tvalid_1's RMSPE: 0.2643\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2643\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001203\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516261\ttraining's RMSPE: 0.344\tvalid_1's rmse: 0.000528746\tvalid_1's RMSPE: 0.3442\n",
      "[200]\ttraining's rmse: 0.000418191\ttraining's RMSPE: 0.2787\tvalid_1's rmse: 0.000442334\tvalid_1's RMSPE: 0.288\n",
      "[300]\ttraining's rmse: 0.000393294\ttraining's RMSPE: 0.2621\tvalid_1's rmse: 0.000421665\tvalid_1's RMSPE: 0.2745\n",
      "[400]\ttraining's rmse: 0.000383781\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.000414605\tvalid_1's RMSPE: 0.2699\n",
      "[500]\ttraining's rmse: 0.000378055\ttraining's RMSPE: 0.2519\tvalid_1's rmse: 0.000410288\tvalid_1's RMSPE: 0.2671\n",
      "[600]\ttraining's rmse: 0.000373692\ttraining's RMSPE: 0.249\tvalid_1's rmse: 0.000407475\tvalid_1's RMSPE: 0.2653\n",
      "[700]\ttraining's rmse: 0.000370004\ttraining's RMSPE: 0.2466\tvalid_1's rmse: 0.000405394\tvalid_1's RMSPE: 0.2639\n",
      "[800]\ttraining's rmse: 0.000366871\ttraining's RMSPE: 0.2445\tvalid_1's rmse: 0.000403469\tvalid_1's RMSPE: 0.2626\n",
      "[900]\ttraining's rmse: 0.000364209\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.000402229\tvalid_1's RMSPE: 0.2618\n",
      "[1000]\ttraining's rmse: 0.000361766\ttraining's RMSPE: 0.2411\tvalid_1's rmse: 0.000401001\tvalid_1's RMSPE: 0.261\n",
      "[1100]\ttraining's rmse: 0.000359546\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000400039\tvalid_1's RMSPE: 0.2604\n",
      "[1200]\ttraining's rmse: 0.000357583\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000399782\tvalid_1's RMSPE: 0.2602\n",
      "[1300]\ttraining's rmse: 0.000355796\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000399483\tvalid_1's RMSPE: 0.2601\n",
      "[1400]\ttraining's rmse: 0.000354164\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000398941\tvalid_1's RMSPE: 0.2597\n",
      "[1500]\ttraining's rmse: 0.000352677\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000398562\tvalid_1's RMSPE: 0.2595\n",
      "[1600]\ttraining's rmse: 0.000351216\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000398216\tvalid_1's RMSPE: 0.2592\n",
      "[1700]\ttraining's rmse: 0.000349871\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000397906\tvalid_1's RMSPE: 0.259\n",
      "Early stopping, best iteration is:\n",
      "[1696]\ttraining's rmse: 0.000349912\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.00039793\tvalid_1's RMSPE: 0.259\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.259\n",
      "\t**********************************************************************\n",
      "Wall time: 16min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPSILON = 0\n",
    "extend_test = pd.DataFrame(\n",
    "    {'target_realized_volatility':[],'predicted_volatility_extend': [], 'time_id':[], 'stock_id':[]}\n",
    "\n",
    ")\n",
    "extend_models = []\n",
    "extend_split_importance = []\n",
    "extend_gain_importance = []\n",
    "train_scores = []\n",
    "inner_k = 5\n",
    "outer_k = 5\n",
    "\n",
    "params =  {\n",
    "    'boosting_type': 'goss',\n",
    "    'learning_rate': 0.01,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8, \n",
    "    'bagging_fraction': 0.8,\n",
    "    'lambda_l1': 1.2,\n",
    "    'lambda_l2': 1.2,\n",
    "    'n_jobs': -1,\n",
    "    'force_col_wise': True,\n",
    "    'extra_trees': True,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "outer_kfold = KFold(n_splits=outer_k, random_state=42, shuffle=True)\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_kfold.split(extendX, extendY)):\n",
    "    print('*'*120)\n",
    "    print(\"Outer Fold :\", outer_fold + 1)\n",
    "    print('*'*120)\n",
    "\n",
    "    X_outer_train = extendX.iloc[outer_train_idx].reset_index(drop=True)  \n",
    "    X_outer_test = extendX.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    y_outer_train = extendY.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    y_outer_test = extendY.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    \n",
    "    target = np.zeros(len(y_outer_test))\n",
    "    inner_scores = 0.0\n",
    "    models = []\n",
    "    \n",
    "    inner_kfold = KFold(n_splits= inner_k, random_state=42, shuffle=True)\n",
    "    for inner_fold, (inner_train_idx, inner_valid_idx) in enumerate(inner_kfold.split(X_outer_train, y_outer_train)):\n",
    "        print(\"\\n\\t\"+\"*\"*20)\n",
    "        print(f\"\\t*  Inner Fold : {inner_fold + 1}  *\")\n",
    "        print(\"\\t\"+\"*\"*20+\"\\n\")\n",
    "    \n",
    "        # inner train data and valid data\n",
    "        X_inner_train = X_outer_train.iloc[inner_train_idx].reset_index(drop=True)\n",
    "        X_inner_valid = X_outer_train.iloc[inner_valid_idx].reset_index(drop=True)\n",
    "\n",
    "        y_inner_train = y_outer_train.iloc[inner_train_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "        y_inner_valid = y_outer_train.iloc[inner_valid_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "            \n",
    "        lgbm_train = lgbm.Dataset(X_inner_train,y_inner_train,weight=1/(np.square(y_inner_train.values)+EPSILON))\n",
    "        lgbm_valid = lgbm.Dataset(\n",
    "            X_inner_valid,y_inner_valid,reference=lgbm_train,weight=1/(np.square(y_inner_valid.values)+EPSILON))\n",
    "        \n",
    "        # model training\n",
    "        model = lgbm.train(\n",
    "            params=params, #tuner.best_params,\n",
    "            train_set=lgbm_train,\n",
    "            valid_sets=[lgbm_train, lgbm_valid],\n",
    "            num_boost_round=10000,       \n",
    "            feval=feval_RMSPE,\n",
    "            callbacks=[lgbm.log_evaluation(period=100), lgbm.early_stopping(100)]\n",
    "        )\n",
    "        # validation \n",
    "        y_inner_pred = model.predict(X_inner_valid, num_iteration=model.best_iteration)\n",
    "        RMSPE = rmspe(\n",
    "            y_true=(y_inner_valid.values.flatten()), \n",
    "            y_pred=(y_inner_pred), n=4\n",
    "        )\n",
    "        \n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "        print(f'\\tInner Validation RMSPE: \\t{RMSPE}')\n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "\n",
    "        # keep training validation score\n",
    "        inner_scores += RMSPE / inner_k\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "        # record feature importances by gain and split\n",
    "        features = list(X_inner_train.columns.values)\n",
    "        \n",
    "        extend_gain_importance.append(compute_importance(model, features, typ='gain'))\n",
    "        extend_split_importance.append(compute_importance(model, features, typ='split'))\n",
    "        \n",
    "    # store all models for prediction in oof evaluation\n",
    "    extend_models.append(models)\n",
    "    train_scores.append(inner_scores)\n",
    "    \n",
    "    # out of fold test set\n",
    "    for model in extend_models[outer_fold]:\n",
    "        y_outer_pred = model.predict(X_outer_test,num_iteration=model.best_iteration)\n",
    "        target += y_outer_pred / len(extend_models[outer_fold])\n",
    "    \n",
    "    y_outer_test = y_outer_test.assign(predicted_volatility_extend = target)\n",
    " \n",
    "    extend_test = pd.concat([extend_test, y_outer_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afb04ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.260842"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(extend_test['target_realized_volatility'], extend_test['predicted_volatility_extend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37251268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>predicted_volatility_extend</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>454.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>627.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213292</th>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>32361.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213293</th>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>32614.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213294</th>\n",
       "      <td>0.012813</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>32649.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213295</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213296</th>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>32748.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213297 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  predicted_volatility_extend  time_id  \\\n",
       "0                         0.001498                     0.002114    169.0   \n",
       "1                         0.001420                     0.001232    211.0   \n",
       "2                         0.003371                     0.002278    266.0   \n",
       "3                         0.001155                     0.001133    454.0   \n",
       "4                         0.001119                     0.001276    627.0   \n",
       "...                            ...                          ...      ...   \n",
       "213292                    0.002960                     0.002583  32361.0   \n",
       "213293                    0.001725                     0.001673  32614.0   \n",
       "213294                    0.012813                     0.009662  32649.0   \n",
       "213295                    0.003511                     0.004358  32724.0   \n",
       "213296                    0.003057                     0.002234  32748.0   \n",
       "\n",
       "        stock_id  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "...          ...  \n",
       "213292     126.0  \n",
       "213293     126.0  \n",
       "213294     126.0  \n",
       "213295     126.0  \n",
       "213296     126.0  \n",
       "\n",
       "[213297 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extend_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf4edd",
   "metadata": {},
   "source": [
    "### Using Normalised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "380fc7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  5.3min finished\n"
     ]
    }
   ],
   "source": [
    "norm = process_stocks(beta.stock_id.unique(), extend=False, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce3a3afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>1.003701</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>1.003655</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>2.449679e-10</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>1.000025</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>5.966433e-13</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>1.000027</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>1.000102</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>4.260982e-11</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.999144</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.998773</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>3.424817e-11</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>1.021313e-11</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>1.182409e-10</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>1.001540</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>1.001618</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>3.711303e-11</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>1.000707</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>1.000623</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>8.922521e-12</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>1.001811</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>1.001805</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>1.357511e-10</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>1.000289</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>1.000429</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>8.055744e-12</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954      1.003701     0.000830   \n",
       "1            11                    0.000981      1.000025     0.000154   \n",
       "2            16                    0.001295      1.000027     0.000420   \n",
       "3            31                    0.001776      0.999144     0.000780   \n",
       "4            62                    0.001520      0.999752     0.000239   \n",
       "...         ...                         ...           ...          ...   \n",
       "428927    32751                    0.002899      0.999757     0.000538   \n",
       "428928    32753                    0.003454      1.001540     0.000556   \n",
       "428929    32758                    0.002792      1.000707     0.000298   \n",
       "428930    32763                    0.002379      1.001811     0.000441   \n",
       "428931    32767                    0.001414      1.000289     0.000227   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0            1.003655      0.000918                             0.003394   \n",
       "1            1.000016      0.000183                             0.000699   \n",
       "2            1.000102      0.000474                             0.001983   \n",
       "3            0.998773      0.000724                             0.001863   \n",
       "4            0.999638      0.000358                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428927       0.999857      0.000664                             0.002284   \n",
       "428928       1.001618      0.000632                             0.002217   \n",
       "428929       1.000623      0.000393                             0.001386   \n",
       "428930       1.001805      0.000497                             0.002783   \n",
       "428931       1.000429      0.000287                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000267                2.449679e-10   \n",
       "1                                  0.000076                5.966433e-13   \n",
       "2                                  0.000182                4.260982e-11   \n",
       "3                                  0.000229                3.424817e-11   \n",
       "4                                  0.000122                1.021313e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000177                1.182409e-10   \n",
       "428928                             0.000205                3.711303e-11   \n",
       "428929                             0.000148                8.922521e-12   \n",
       "428930                             0.000193                1.357511e-10   \n",
       "428931                             0.000141                8.055744e-12   \n",
       "\n",
       "        log_returns_mean_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0                   0.000013  ...            1              4   \n",
       "1                   0.000003  ...            1              4   \n",
       "2                   0.000004  ...            1              4   \n",
       "3                  -0.000025  ...            1              4   \n",
       "4                  -0.000008  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "428927             -0.000005  ...            1              2   \n",
       "428928              0.000009  ...            1              2   \n",
       "428929             -0.000008  ...            1              2   \n",
       "428930              0.000010  ...            1              2   \n",
       "428931             -0.000003  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428932 rows x 195 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fab25f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.to_feather(\"norm.fth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56f6ad1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm[norm['target_realized_volatility']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f7aa4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>1.003701</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>1.003655</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>2.449679e-10</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>1.000025</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>5.966433e-13</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>1.000027</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>1.000102</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>4.260982e-11</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.999144</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.998773</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>3.424817e-11</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>1.021313e-11</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>1.182409e-10</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>1.001540</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>1.001618</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>3.711303e-11</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>1.000707</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>1.000623</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>8.922521e-12</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>1.001811</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>1.001805</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>1.357511e-10</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>1.000289</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>1.000429</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>8.055744e-12</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954      1.003701     0.000830   \n",
       "1            11                    0.000981      1.000025     0.000154   \n",
       "2            16                    0.001295      1.000027     0.000420   \n",
       "3            31                    0.001776      0.999144     0.000780   \n",
       "4            62                    0.001520      0.999752     0.000239   \n",
       "...         ...                         ...           ...          ...   \n",
       "428927    32751                    0.002899      0.999757     0.000538   \n",
       "428928    32753                    0.003454      1.001540     0.000556   \n",
       "428929    32758                    0.002792      1.000707     0.000298   \n",
       "428930    32763                    0.002379      1.001811     0.000441   \n",
       "428931    32767                    0.001414      1.000289     0.000227   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0            1.003655      0.000918                             0.003394   \n",
       "1            1.000016      0.000183                             0.000699   \n",
       "2            1.000102      0.000474                             0.001983   \n",
       "3            0.998773      0.000724                             0.001863   \n",
       "4            0.999638      0.000358                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428927       0.999857      0.000664                             0.002284   \n",
       "428928       1.001618      0.000632                             0.002217   \n",
       "428929       1.000623      0.000393                             0.001386   \n",
       "428930       1.001805      0.000497                             0.002783   \n",
       "428931       1.000429      0.000287                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000267                2.449679e-10   \n",
       "1                                  0.000076                5.966433e-13   \n",
       "2                                  0.000182                4.260982e-11   \n",
       "3                                  0.000229                3.424817e-11   \n",
       "4                                  0.000122                1.021313e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000177                1.182409e-10   \n",
       "428928                             0.000205                3.711303e-11   \n",
       "428929                             0.000148                8.922521e-12   \n",
       "428930                             0.000193                1.357511e-10   \n",
       "428931                             0.000141                8.055744e-12   \n",
       "\n",
       "        log_returns_mean_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0                   0.000013  ...            1              4   \n",
       "1                   0.000003  ...            1              4   \n",
       "2                   0.000004  ...            1              4   \n",
       "3                  -0.000025  ...            1              4   \n",
       "4                  -0.000008  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "428927             -0.000005  ...            1              2   \n",
       "428928              0.000009  ...            1              2   \n",
       "428929             -0.000008  ...            1              2   \n",
       "428930              0.000010  ...            1              2   \n",
       "428931             -0.000003  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428931 rows x 195 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = norm[norm['target_realized_volatility'] != 0]#.sort_values(['stock_id','time_id']).reset_index(drop=True)\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f5cc103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm[norm.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c29ba972",
   "metadata": {},
   "outputs": [],
   "source": [
    "normX = norm.drop(['time_id','target_realized_volatility'], axis=1)  # leave stock id as feature\n",
    "normY = norm[['target_realized_volatility', 'stock_id', 'time_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c46ad8",
   "metadata": {},
   "source": [
    "### Can recover lost rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60133539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "Outer Fold : 1\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274515, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488689\ttraining's RMSPE: 0.335\tvalid_1's rmse: 0.000502472\tvalid_1's RMSPE: 0.3442\n",
      "[200]\ttraining's rmse: 0.000395031\ttraining's RMSPE: 0.2708\tvalid_1's rmse: 0.000420393\tvalid_1's RMSPE: 0.288\n",
      "[300]\ttraining's rmse: 0.000373085\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000402939\tvalid_1's RMSPE: 0.276\n",
      "[400]\ttraining's rmse: 0.000365596\ttraining's RMSPE: 0.2506\tvalid_1's rmse: 0.000396616\tvalid_1's RMSPE: 0.2717\n",
      "[500]\ttraining's rmse: 0.000361631\ttraining's RMSPE: 0.2479\tvalid_1's rmse: 0.000393612\tvalid_1's RMSPE: 0.2696\n",
      "[600]\ttraining's rmse: 0.000358684\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000391046\tvalid_1's RMSPE: 0.2679\n",
      "[700]\ttraining's rmse: 0.000356307\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000389678\tvalid_1's RMSPE: 0.2669\n",
      "[800]\ttraining's rmse: 0.000354326\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000388534\tvalid_1's RMSPE: 0.2662\n",
      "[900]\ttraining's rmse: 0.000352552\ttraining's RMSPE: 0.2417\tvalid_1's rmse: 0.000387318\tvalid_1's RMSPE: 0.2653\n",
      "[1000]\ttraining's rmse: 0.00035096\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000386096\tvalid_1's RMSPE: 0.2645\n",
      "[1100]\ttraining's rmse: 0.000349608\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000385677\tvalid_1's RMSPE: 0.2642\n",
      "[1200]\ttraining's rmse: 0.000348352\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000385671\tvalid_1's RMSPE: 0.2642\n",
      "Early stopping, best iteration is:\n",
      "[1110]\ttraining's rmse: 0.000349473\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000385601\tvalid_1's RMSPE: 0.2641\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2641\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274515, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001171\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489282\ttraining's RMSPE: 0.3363\tvalid_1's rmse: 0.000492284\tvalid_1's RMSPE: 0.3338\n",
      "[200]\ttraining's rmse: 0.000394561\ttraining's RMSPE: 0.2712\tvalid_1's rmse: 0.000403974\tvalid_1's RMSPE: 0.2739\n",
      "[300]\ttraining's rmse: 0.000372014\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.000383875\tvalid_1's RMSPE: 0.2603\n",
      "[400]\ttraining's rmse: 0.000364399\ttraining's RMSPE: 0.2504\tvalid_1's rmse: 0.000377839\tvalid_1's RMSPE: 0.2562\n",
      "[500]\ttraining's rmse: 0.000360317\ttraining's RMSPE: 0.2476\tvalid_1's rmse: 0.000375042\tvalid_1's RMSPE: 0.2543\n",
      "[600]\ttraining's rmse: 0.000357362\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.000373192\tvalid_1's RMSPE: 0.253\n",
      "[700]\ttraining's rmse: 0.000354951\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000371881\tvalid_1's RMSPE: 0.2521\n",
      "[800]\ttraining's rmse: 0.000352935\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000370869\tvalid_1's RMSPE: 0.2514\n",
      "[900]\ttraining's rmse: 0.00035115\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000370024\tvalid_1's RMSPE: 0.2509\n",
      "[1000]\ttraining's rmse: 0.000349613\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.00036924\tvalid_1's RMSPE: 0.2503\n",
      "[1100]\ttraining's rmse: 0.000348224\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000368673\tvalid_1's RMSPE: 0.2499\n",
      "[1200]\ttraining's rmse: 0.000346956\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000368191\tvalid_1's RMSPE: 0.2496\n",
      "[1300]\ttraining's rmse: 0.000345824\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.000367767\tvalid_1's RMSPE: 0.2493\n",
      "[1400]\ttraining's rmse: 0.000344761\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000367406\tvalid_1's RMSPE: 0.2491\n",
      "[1500]\ttraining's rmse: 0.00034377\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000367132\tvalid_1's RMSPE: 0.2489\n",
      "[1600]\ttraining's rmse: 0.000342859\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.00036679\tvalid_1's RMSPE: 0.2487\n",
      "[1700]\ttraining's rmse: 0.000342007\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000366478\tvalid_1's RMSPE: 0.2485\n",
      "[1800]\ttraining's rmse: 0.000341201\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000366238\tvalid_1's RMSPE: 0.2483\n",
      "[1900]\ttraining's rmse: 0.000340422\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000366039\tvalid_1's RMSPE: 0.2482\n",
      "[2000]\ttraining's rmse: 0.000339679\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000365871\tvalid_1's RMSPE: 0.248\n",
      "[2100]\ttraining's rmse: 0.000339012\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.00036571\tvalid_1's RMSPE: 0.2479\n",
      "[2200]\ttraining's rmse: 0.000338351\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000365517\tvalid_1's RMSPE: 0.2478\n",
      "[2300]\ttraining's rmse: 0.000337723\ttraining's RMSPE: 0.2321\tvalid_1's rmse: 0.0003654\tvalid_1's RMSPE: 0.2477\n",
      "Early stopping, best iteration is:\n",
      "[2247]\ttraining's rmse: 0.000338043\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000365431\tvalid_1's RMSPE: 0.2477\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2477\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274515, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001176\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489176\ttraining's RMSPE: 0.3357\tvalid_1's rmse: 0.000494745\tvalid_1's RMSPE: 0.3375\n",
      "[200]\ttraining's rmse: 0.000395146\ttraining's RMSPE: 0.2712\tvalid_1's rmse: 0.000403221\tvalid_1's RMSPE: 0.275\n",
      "[300]\ttraining's rmse: 0.000372725\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000382314\tvalid_1's RMSPE: 0.2608\n",
      "[400]\ttraining's rmse: 0.000364967\ttraining's RMSPE: 0.2505\tvalid_1's rmse: 0.000375911\tvalid_1's RMSPE: 0.2564\n",
      "[500]\ttraining's rmse: 0.000360793\ttraining's RMSPE: 0.2476\tvalid_1's rmse: 0.00037323\tvalid_1's RMSPE: 0.2546\n",
      "[600]\ttraining's rmse: 0.000357817\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.000371608\tvalid_1's RMSPE: 0.2535\n",
      "[700]\ttraining's rmse: 0.000355465\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000370524\tvalid_1's RMSPE: 0.2527\n",
      "[800]\ttraining's rmse: 0.000353412\ttraining's RMSPE: 0.2425\tvalid_1's rmse: 0.000369673\tvalid_1's RMSPE: 0.2522\n",
      "[900]\ttraining's rmse: 0.0003516\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000368895\tvalid_1's RMSPE: 0.2516\n",
      "[1000]\ttraining's rmse: 0.000350056\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000368213\tvalid_1's RMSPE: 0.2512\n",
      "[1100]\ttraining's rmse: 0.000348646\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000367605\tvalid_1's RMSPE: 0.2507\n",
      "[1200]\ttraining's rmse: 0.00034735\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000367242\tvalid_1's RMSPE: 0.2505\n",
      "[1300]\ttraining's rmse: 0.000346241\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000366914\tvalid_1's RMSPE: 0.2503\n",
      "[1400]\ttraining's rmse: 0.000345203\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000366574\tvalid_1's RMSPE: 0.25\n",
      "[1500]\ttraining's rmse: 0.000344282\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000366238\tvalid_1's RMSPE: 0.2498\n",
      "[1600]\ttraining's rmse: 0.000343333\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000365907\tvalid_1's RMSPE: 0.2496\n",
      "[1700]\ttraining's rmse: 0.00034244\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000365628\tvalid_1's RMSPE: 0.2494\n",
      "[1800]\ttraining's rmse: 0.000341649\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000365434\tvalid_1's RMSPE: 0.2493\n",
      "[1900]\ttraining's rmse: 0.000340852\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000365186\tvalid_1's RMSPE: 0.2491\n",
      "[2000]\ttraining's rmse: 0.000340113\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000364983\tvalid_1's RMSPE: 0.249\n",
      "[2100]\ttraining's rmse: 0.000339422\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000364857\tvalid_1's RMSPE: 0.2489\n",
      "Early stopping, best iteration is:\n",
      "[2005]\ttraining's rmse: 0.000340079\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000364972\tvalid_1's RMSPE: 0.2489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2489\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274515, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001174\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488206\ttraining's RMSPE: 0.3352\tvalid_1's rmse: 0.000501354\tvalid_1's RMSPE: 0.3414\n",
      "[200]\ttraining's rmse: 0.000394014\ttraining's RMSPE: 0.2705\tvalid_1's rmse: 0.000421962\tvalid_1's RMSPE: 0.2873\n",
      "[300]\ttraining's rmse: 0.000371991\ttraining's RMSPE: 0.2554\tvalid_1's rmse: 0.000407285\tvalid_1's RMSPE: 0.2773\n",
      "[400]\ttraining's rmse: 0.000364496\ttraining's RMSPE: 0.2502\tvalid_1's rmse: 0.000403626\tvalid_1's RMSPE: 0.2748\n",
      "[500]\ttraining's rmse: 0.000360469\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000401462\tvalid_1's RMSPE: 0.2734\n",
      "[600]\ttraining's rmse: 0.000357613\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000399824\tvalid_1's RMSPE: 0.2722\n",
      "[700]\ttraining's rmse: 0.000355303\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000398684\tvalid_1's RMSPE: 0.2715\n",
      "[800]\ttraining's rmse: 0.000353374\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000397534\tvalid_1's RMSPE: 0.2707\n",
      "[900]\ttraining's rmse: 0.00035161\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000396778\tvalid_1's RMSPE: 0.2702\n",
      "[1000]\ttraining's rmse: 0.000350066\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000395908\tvalid_1's RMSPE: 0.2696\n",
      "[1100]\ttraining's rmse: 0.000348695\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000395726\tvalid_1's RMSPE: 0.2695\n",
      "[1200]\ttraining's rmse: 0.000347471\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.00039507\tvalid_1's RMSPE: 0.269\n",
      "[1300]\ttraining's rmse: 0.000346306\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000394505\tvalid_1's RMSPE: 0.2686\n",
      "[1400]\ttraining's rmse: 0.000345248\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000393842\tvalid_1's RMSPE: 0.2682\n",
      "[1500]\ttraining's rmse: 0.000344265\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.00039352\tvalid_1's RMSPE: 0.268\n",
      "Early stopping, best iteration is:\n",
      "[1478]\ttraining's rmse: 0.000344481\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000393505\tvalid_1's RMSPE: 0.2679\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2679\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001192\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000490088\ttraining's RMSPE: 0.334\tvalid_1's rmse: 0.000515628\tvalid_1's RMSPE: 0.3614\n",
      "[200]\ttraining's rmse: 0.000397741\ttraining's RMSPE: 0.2711\tvalid_1's rmse: 0.000413677\tvalid_1's RMSPE: 0.2899\n",
      "[300]\ttraining's rmse: 0.000375767\ttraining's RMSPE: 0.2561\tvalid_1's rmse: 0.000388946\tvalid_1's RMSPE: 0.2726\n",
      "[400]\ttraining's rmse: 0.00036812\ttraining's RMSPE: 0.2509\tvalid_1's rmse: 0.000381335\tvalid_1's RMSPE: 0.2673\n",
      "[500]\ttraining's rmse: 0.000363917\ttraining's RMSPE: 0.248\tvalid_1's rmse: 0.000376264\tvalid_1's RMSPE: 0.2637\n",
      "[600]\ttraining's rmse: 0.000360888\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000374224\tvalid_1's RMSPE: 0.2623\n",
      "[700]\ttraining's rmse: 0.000358449\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000372707\tvalid_1's RMSPE: 0.2612\n",
      "[800]\ttraining's rmse: 0.000356292\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000370408\tvalid_1's RMSPE: 0.2596\n",
      "[900]\ttraining's rmse: 0.000354508\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.000369723\tvalid_1's RMSPE: 0.2591\n",
      "[1000]\ttraining's rmse: 0.00035289\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000370203\tvalid_1's RMSPE: 0.2595\n",
      "Early stopping, best iteration is:\n",
      "[988]\ttraining's rmse: 0.000353084\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000369011\tvalid_1's RMSPE: 0.2586\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2586\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 2\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001192\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488858\ttraining's RMSPE: 0.3331\tvalid_1's rmse: 0.000498136\tvalid_1's RMSPE: 0.3362\n",
      "[200]\ttraining's rmse: 0.000396087\ttraining's RMSPE: 0.2699\tvalid_1's rmse: 0.000413686\tvalid_1's RMSPE: 0.2792\n",
      "[300]\ttraining's rmse: 0.000374697\ttraining's RMSPE: 0.2553\tvalid_1's rmse: 0.000396451\tvalid_1's RMSPE: 0.2675\n",
      "[400]\ttraining's rmse: 0.000367383\ttraining's RMSPE: 0.2503\tvalid_1's rmse: 0.000391346\tvalid_1's RMSPE: 0.2641\n",
      "[500]\ttraining's rmse: 0.000363363\ttraining's RMSPE: 0.2476\tvalid_1's rmse: 0.000388756\tvalid_1's RMSPE: 0.2623\n",
      "[600]\ttraining's rmse: 0.000360492\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.000387101\tvalid_1's RMSPE: 0.2612\n",
      "[700]\ttraining's rmse: 0.000358083\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000385683\tvalid_1's RMSPE: 0.2603\n",
      "[800]\ttraining's rmse: 0.000356026\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000384348\tvalid_1's RMSPE: 0.2594\n",
      "[900]\ttraining's rmse: 0.000354227\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000383482\tvalid_1's RMSPE: 0.2588\n",
      "[1000]\ttraining's rmse: 0.000352698\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000382997\tvalid_1's RMSPE: 0.2585\n",
      "[1100]\ttraining's rmse: 0.000351332\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000382437\tvalid_1's RMSPE: 0.2581\n",
      "[1200]\ttraining's rmse: 0.000350117\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000382052\tvalid_1's RMSPE: 0.2578\n",
      "[1300]\ttraining's rmse: 0.000348979\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000381574\tvalid_1's RMSPE: 0.2575\n",
      "[1400]\ttraining's rmse: 0.000347927\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000381362\tvalid_1's RMSPE: 0.2574\n",
      "[1500]\ttraining's rmse: 0.000346918\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.000381004\tvalid_1's RMSPE: 0.2571\n",
      "[1600]\ttraining's rmse: 0.000346\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000380768\tvalid_1's RMSPE: 0.257\n",
      "[1700]\ttraining's rmse: 0.000345163\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000380699\tvalid_1's RMSPE: 0.2569\n",
      "Early stopping, best iteration is:\n",
      "[1669]\ttraining's rmse: 0.000345416\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000380616\tvalid_1's RMSPE: 0.2568\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2568\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001197\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000490546\ttraining's RMSPE: 0.3333\tvalid_1's rmse: 0.000491361\tvalid_1's RMSPE: 0.3353\n",
      "[200]\ttraining's rmse: 0.000398413\ttraining's RMSPE: 0.2707\tvalid_1's rmse: 0.000403271\tvalid_1's RMSPE: 0.2751\n",
      "[300]\ttraining's rmse: 0.000376661\ttraining's RMSPE: 0.2559\tvalid_1's rmse: 0.000384129\tvalid_1's RMSPE: 0.2621\n",
      "[400]\ttraining's rmse: 0.000369312\ttraining's RMSPE: 0.2509\tvalid_1's rmse: 0.000378329\tvalid_1's RMSPE: 0.2581\n",
      "[500]\ttraining's rmse: 0.000365239\ttraining's RMSPE: 0.2482\tvalid_1's rmse: 0.000375488\tvalid_1's RMSPE: 0.2562\n",
      "[600]\ttraining's rmse: 0.000362301\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000373838\tvalid_1's RMSPE: 0.2551\n",
      "[700]\ttraining's rmse: 0.000359867\ttraining's RMSPE: 0.2445\tvalid_1's rmse: 0.000372437\tvalid_1's RMSPE: 0.2541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttraining's rmse: 0.000357784\ttraining's RMSPE: 0.2431\tvalid_1's rmse: 0.000371314\tvalid_1's RMSPE: 0.2533\n",
      "[900]\ttraining's rmse: 0.000355976\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000370564\tvalid_1's RMSPE: 0.2528\n",
      "[1000]\ttraining's rmse: 0.000354355\ttraining's RMSPE: 0.2408\tvalid_1's rmse: 0.000369878\tvalid_1's RMSPE: 0.2524\n",
      "[1100]\ttraining's rmse: 0.000352931\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000369175\tvalid_1's RMSPE: 0.2519\n",
      "[1200]\ttraining's rmse: 0.000351631\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.00036864\tvalid_1's RMSPE: 0.2515\n",
      "[1300]\ttraining's rmse: 0.000350448\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000368004\tvalid_1's RMSPE: 0.2511\n",
      "[1400]\ttraining's rmse: 0.000349395\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000367582\tvalid_1's RMSPE: 0.2508\n",
      "[1500]\ttraining's rmse: 0.000348376\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000367184\tvalid_1's RMSPE: 0.2505\n",
      "[1600]\ttraining's rmse: 0.000347413\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000366728\tvalid_1's RMSPE: 0.2502\n",
      "[1700]\ttraining's rmse: 0.000346495\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000366369\tvalid_1's RMSPE: 0.25\n",
      "[1800]\ttraining's rmse: 0.000345678\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000366192\tvalid_1's RMSPE: 0.2498\n",
      "[1900]\ttraining's rmse: 0.00034493\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000365964\tvalid_1's RMSPE: 0.2497\n",
      "[2000]\ttraining's rmse: 0.000344188\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000365759\tvalid_1's RMSPE: 0.2496\n",
      "[2100]\ttraining's rmse: 0.000343473\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000365519\tvalid_1's RMSPE: 0.2494\n",
      "[2200]\ttraining's rmse: 0.000342788\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000365422\tvalid_1's RMSPE: 0.2493\n",
      "Early stopping, best iteration is:\n",
      "[2170]\ttraining's rmse: 0.000342989\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000365457\tvalid_1's RMSPE: 0.2493\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2493\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001195\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489493\ttraining's RMSPE: 0.333\tvalid_1's rmse: 0.00049439\tvalid_1's RMSPE: 0.3355\n",
      "[200]\ttraining's rmse: 0.000397002\ttraining's RMSPE: 0.2701\tvalid_1's rmse: 0.000405317\tvalid_1's RMSPE: 0.2751\n",
      "[300]\ttraining's rmse: 0.000375092\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000385294\tvalid_1's RMSPE: 0.2615\n",
      "[400]\ttraining's rmse: 0.00036762\ttraining's RMSPE: 0.2501\tvalid_1's rmse: 0.000379178\tvalid_1's RMSPE: 0.2573\n",
      "[500]\ttraining's rmse: 0.000363523\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.000376132\tvalid_1's RMSPE: 0.2553\n",
      "[600]\ttraining's rmse: 0.000360571\ttraining's RMSPE: 0.2453\tvalid_1's rmse: 0.000374282\tvalid_1's RMSPE: 0.254\n",
      "[700]\ttraining's rmse: 0.000358189\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000372802\tvalid_1's RMSPE: 0.253\n",
      "[800]\ttraining's rmse: 0.000356196\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000371731\tvalid_1's RMSPE: 0.2523\n",
      "[900]\ttraining's rmse: 0.000354374\ttraining's RMSPE: 0.2411\tvalid_1's rmse: 0.000370791\tvalid_1's RMSPE: 0.2516\n",
      "[1000]\ttraining's rmse: 0.000352779\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.00037009\tvalid_1's RMSPE: 0.2512\n",
      "[1100]\ttraining's rmse: 0.000351387\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.00036947\tvalid_1's RMSPE: 0.2507\n",
      "[1200]\ttraining's rmse: 0.000350152\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000368952\tvalid_1's RMSPE: 0.2504\n",
      "[1300]\ttraining's rmse: 0.000348964\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000368419\tvalid_1's RMSPE: 0.25\n",
      "[1400]\ttraining's rmse: 0.000347886\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000367983\tvalid_1's RMSPE: 0.2497\n",
      "[1500]\ttraining's rmse: 0.000346897\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.00036773\tvalid_1's RMSPE: 0.2496\n",
      "[1600]\ttraining's rmse: 0.000345992\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000367435\tvalid_1's RMSPE: 0.2494\n",
      "[1700]\ttraining's rmse: 0.000345123\ttraining's RMSPE: 0.2348\tvalid_1's rmse: 0.000367115\tvalid_1's RMSPE: 0.2491\n",
      "[1800]\ttraining's rmse: 0.000344306\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.00036684\tvalid_1's RMSPE: 0.2489\n",
      "[1900]\ttraining's rmse: 0.000343569\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000366508\tvalid_1's RMSPE: 0.2487\n",
      "[2000]\ttraining's rmse: 0.000342849\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000366336\tvalid_1's RMSPE: 0.2486\n",
      "[2100]\ttraining's rmse: 0.000342143\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.0003661\tvalid_1's RMSPE: 0.2484\n",
      "Early stopping, best iteration is:\n",
      "[2096]\ttraining's rmse: 0.000342168\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000366105\tvalid_1's RMSPE: 0.2484\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2484\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001203\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488777\ttraining's RMSPE: 0.3312\tvalid_1's rmse: 0.000509943\tvalid_1's RMSPE: 0.3517\n",
      "[200]\ttraining's rmse: 0.000396858\ttraining's RMSPE: 0.2689\tvalid_1's rmse: 0.000428545\tvalid_1's RMSPE: 0.2955\n",
      "[300]\ttraining's rmse: 0.000375634\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000411276\tvalid_1's RMSPE: 0.2836\n",
      "[400]\ttraining's rmse: 0.000368505\ttraining's RMSPE: 0.2497\tvalid_1's rmse: 0.000405186\tvalid_1's RMSPE: 0.2794\n",
      "[500]\ttraining's rmse: 0.000364599\ttraining's RMSPE: 0.2471\tvalid_1's rmse: 0.000402147\tvalid_1's RMSPE: 0.2773\n",
      "[600]\ttraining's rmse: 0.000361763\ttraining's RMSPE: 0.2451\tvalid_1's rmse: 0.000400344\tvalid_1's RMSPE: 0.2761\n",
      "[700]\ttraining's rmse: 0.000359466\ttraining's RMSPE: 0.2436\tvalid_1's rmse: 0.000398453\tvalid_1's RMSPE: 0.2748\n",
      "[800]\ttraining's rmse: 0.000357424\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000396841\tvalid_1's RMSPE: 0.2737\n",
      "[900]\ttraining's rmse: 0.000355703\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000396102\tvalid_1's RMSPE: 0.2732\n",
      "[1000]\ttraining's rmse: 0.000354176\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000395028\tvalid_1's RMSPE: 0.2724\n",
      "[1100]\ttraining's rmse: 0.00035285\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000394506\tvalid_1's RMSPE: 0.2721\n",
      "Early stopping, best iteration is:\n",
      "[1069]\ttraining's rmse: 0.00035324\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000394497\tvalid_1's RMSPE: 0.272\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.272\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001192\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048991\ttraining's RMSPE: 0.3338\tvalid_1's rmse: 0.00049071\tvalid_1's RMSPE: 0.3311\n",
      "[200]\ttraining's rmse: 0.000397104\ttraining's RMSPE: 0.2706\tvalid_1's rmse: 0.000399804\tvalid_1's RMSPE: 0.2698\n",
      "[300]\ttraining's rmse: 0.000375274\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.00037983\tvalid_1's RMSPE: 0.2563\n",
      "[400]\ttraining's rmse: 0.00036779\ttraining's RMSPE: 0.2506\tvalid_1's rmse: 0.000373974\tvalid_1's RMSPE: 0.2523\n",
      "[500]\ttraining's rmse: 0.000363629\ttraining's RMSPE: 0.2478\tvalid_1's rmse: 0.000371248\tvalid_1's RMSPE: 0.2505\n",
      "[600]\ttraining's rmse: 0.000360679\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000369502\tvalid_1's RMSPE: 0.2493\n",
      "[700]\ttraining's rmse: 0.000358337\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000368228\tvalid_1's RMSPE: 0.2485\n",
      "[800]\ttraining's rmse: 0.000356291\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000367138\tvalid_1's RMSPE: 0.2477\n",
      "[900]\ttraining's rmse: 0.00035454\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.000366201\tvalid_1's RMSPE: 0.2471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's rmse: 0.000352961\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000365365\tvalid_1's RMSPE: 0.2465\n",
      "[1100]\ttraining's rmse: 0.000351551\ttraining's RMSPE: 0.2395\tvalid_1's rmse: 0.000364657\tvalid_1's RMSPE: 0.246\n",
      "[1200]\ttraining's rmse: 0.000350291\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000364076\tvalid_1's RMSPE: 0.2457\n",
      "[1300]\ttraining's rmse: 0.000349142\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.00036359\tvalid_1's RMSPE: 0.2453\n",
      "[1400]\ttraining's rmse: 0.000348086\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.00036322\tvalid_1's RMSPE: 0.2451\n",
      "[1500]\ttraining's rmse: 0.000347114\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000362855\tvalid_1's RMSPE: 0.2448\n",
      "[1600]\ttraining's rmse: 0.000346161\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000362505\tvalid_1's RMSPE: 0.2446\n",
      "[1700]\ttraining's rmse: 0.000345312\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000362162\tvalid_1's RMSPE: 0.2444\n",
      "[1800]\ttraining's rmse: 0.000344482\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000361857\tvalid_1's RMSPE: 0.2442\n",
      "[1900]\ttraining's rmse: 0.000343703\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000361618\tvalid_1's RMSPE: 0.244\n",
      "[2000]\ttraining's rmse: 0.00034297\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000361429\tvalid_1's RMSPE: 0.2439\n",
      "[2100]\ttraining's rmse: 0.000342296\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.00036127\tvalid_1's RMSPE: 0.2438\n",
      "[2200]\ttraining's rmse: 0.000341607\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000361058\tvalid_1's RMSPE: 0.2436\n",
      "Early stopping, best iteration is:\n",
      "[2168]\ttraining's rmse: 0.000341811\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000361103\tvalid_1's RMSPE: 0.2436\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2437\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 3\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001175\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487849\ttraining's RMSPE: 0.3347\tvalid_1's rmse: 0.00049945\tvalid_1's RMSPE: 0.3406\n",
      "[200]\ttraining's rmse: 0.000393674\ttraining's RMSPE: 0.2701\tvalid_1's rmse: 0.000415243\tvalid_1's RMSPE: 0.2832\n",
      "[300]\ttraining's rmse: 0.000371926\ttraining's RMSPE: 0.2551\tvalid_1's rmse: 0.000397145\tvalid_1's RMSPE: 0.2708\n",
      "[400]\ttraining's rmse: 0.000364601\ttraining's RMSPE: 0.2501\tvalid_1's rmse: 0.000390684\tvalid_1's RMSPE: 0.2664\n",
      "[500]\ttraining's rmse: 0.00036075\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000387273\tvalid_1's RMSPE: 0.2641\n",
      "[600]\ttraining's rmse: 0.000357882\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000384654\tvalid_1's RMSPE: 0.2623\n",
      "[700]\ttraining's rmse: 0.000355567\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000382871\tvalid_1's RMSPE: 0.2611\n",
      "[800]\ttraining's rmse: 0.000353579\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000381693\tvalid_1's RMSPE: 0.2603\n",
      "[900]\ttraining's rmse: 0.000351839\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000380624\tvalid_1's RMSPE: 0.2596\n",
      "[1000]\ttraining's rmse: 0.000350335\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000379761\tvalid_1's RMSPE: 0.259\n",
      "[1100]\ttraining's rmse: 0.000348991\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000379259\tvalid_1's RMSPE: 0.2586\n",
      "[1200]\ttraining's rmse: 0.000347748\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000378872\tvalid_1's RMSPE: 0.2584\n",
      "[1300]\ttraining's rmse: 0.000346641\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000378471\tvalid_1's RMSPE: 0.2581\n",
      "[1400]\ttraining's rmse: 0.000345551\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000377704\tvalid_1's RMSPE: 0.2576\n",
      "[1500]\ttraining's rmse: 0.000344603\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.000377252\tvalid_1's RMSPE: 0.2573\n",
      "[1600]\ttraining's rmse: 0.000343691\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000376812\tvalid_1's RMSPE: 0.257\n",
      "Early stopping, best iteration is:\n",
      "[1534]\ttraining's rmse: 0.000344279\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000376878\tvalid_1's RMSPE: 0.257\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.257\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001173\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048806\ttraining's RMSPE: 0.335\tvalid_1's rmse: 0.000489328\tvalid_1's RMSPE: 0.3328\n",
      "[200]\ttraining's rmse: 0.000393726\ttraining's RMSPE: 0.2703\tvalid_1's rmse: 0.00040157\tvalid_1's RMSPE: 0.2731\n",
      "[300]\ttraining's rmse: 0.000371868\ttraining's RMSPE: 0.2553\tvalid_1's rmse: 0.000382083\tvalid_1's RMSPE: 0.2598\n",
      "[400]\ttraining's rmse: 0.000364704\ttraining's RMSPE: 0.2504\tvalid_1's rmse: 0.000376024\tvalid_1's RMSPE: 0.2557\n",
      "[500]\ttraining's rmse: 0.000360844\ttraining's RMSPE: 0.2477\tvalid_1's rmse: 0.000373105\tvalid_1's RMSPE: 0.2537\n",
      "[600]\ttraining's rmse: 0.000358076\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000371192\tvalid_1's RMSPE: 0.2524\n",
      "[700]\ttraining's rmse: 0.000355771\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000369636\tvalid_1's RMSPE: 0.2514\n",
      "[800]\ttraining's rmse: 0.000353908\ttraining's RMSPE: 0.243\tvalid_1's rmse: 0.000368533\tvalid_1's RMSPE: 0.2506\n",
      "[900]\ttraining's rmse: 0.000352166\ttraining's RMSPE: 0.2418\tvalid_1's rmse: 0.000367537\tvalid_1's RMSPE: 0.2499\n",
      "[1000]\ttraining's rmse: 0.000350638\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000366681\tvalid_1's RMSPE: 0.2494\n",
      "[1100]\ttraining's rmse: 0.000349309\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000366042\tvalid_1's RMSPE: 0.2489\n",
      "[1200]\ttraining's rmse: 0.000348039\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000365509\tvalid_1's RMSPE: 0.2486\n",
      "[1300]\ttraining's rmse: 0.000346945\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000364959\tvalid_1's RMSPE: 0.2482\n",
      "[1400]\ttraining's rmse: 0.000345885\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.00036461\tvalid_1's RMSPE: 0.2479\n",
      "[1500]\ttraining's rmse: 0.000344997\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000364261\tvalid_1's RMSPE: 0.2477\n",
      "[1600]\ttraining's rmse: 0.00034407\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000363953\tvalid_1's RMSPE: 0.2475\n",
      "[1700]\ttraining's rmse: 0.000343224\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000363527\tvalid_1's RMSPE: 0.2472\n",
      "[1800]\ttraining's rmse: 0.000342425\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000363283\tvalid_1's RMSPE: 0.247\n",
      "[1900]\ttraining's rmse: 0.000341656\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000363027\tvalid_1's RMSPE: 0.2469\n",
      "[2000]\ttraining's rmse: 0.000340968\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000362909\tvalid_1's RMSPE: 0.2468\n",
      "Early stopping, best iteration is:\n",
      "[1916]\ttraining's rmse: 0.000341539\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000363004\tvalid_1's RMSPE: 0.2468\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2468\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001190\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048789\ttraining's RMSPE: 0.3327\tvalid_1's rmse: 0.000518049\tvalid_1's RMSPE: 0.3617\n",
      "[200]\ttraining's rmse: 0.000395634\ttraining's RMSPE: 0.2698\tvalid_1's rmse: 0.000421501\tvalid_1's RMSPE: 0.2943\n",
      "[300]\ttraining's rmse: 0.000374157\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000397164\tvalid_1's RMSPE: 0.2773\n",
      "[400]\ttraining's rmse: 0.000366875\ttraining's RMSPE: 0.2502\tvalid_1's rmse: 0.000389116\tvalid_1's RMSPE: 0.2716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 0.000362983\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000386185\tvalid_1's RMSPE: 0.2696\n",
      "[600]\ttraining's rmse: 0.000360154\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.000383698\tvalid_1's RMSPE: 0.2679\n",
      "[700]\ttraining's rmse: 0.000357733\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000381859\tvalid_1's RMSPE: 0.2666\n",
      "[800]\ttraining's rmse: 0.000355733\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000380628\tvalid_1's RMSPE: 0.2657\n",
      "[900]\ttraining's rmse: 0.000353955\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000380443\tvalid_1's RMSPE: 0.2656\n",
      "[1000]\ttraining's rmse: 0.000352405\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000378957\tvalid_1's RMSPE: 0.2646\n",
      "[1100]\ttraining's rmse: 0.000351072\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000378405\tvalid_1's RMSPE: 0.2642\n",
      "[1200]\ttraining's rmse: 0.000349799\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000377779\tvalid_1's RMSPE: 0.2637\n",
      "[1300]\ttraining's rmse: 0.000348648\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000377313\tvalid_1's RMSPE: 0.2634\n",
      "[1400]\ttraining's rmse: 0.000347535\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000377597\tvalid_1's RMSPE: 0.2636\n",
      "Early stopping, best iteration is:\n",
      "[1305]\ttraining's rmse: 0.00034858\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.000377219\tvalid_1's RMSPE: 0.2633\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2633\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001179\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489075\ttraining's RMSPE: 0.3349\tvalid_1's rmse: 0.000495213\tvalid_1's RMSPE: 0.3402\n",
      "[200]\ttraining's rmse: 0.000395116\ttraining's RMSPE: 0.2706\tvalid_1's rmse: 0.000406502\tvalid_1's RMSPE: 0.2792\n",
      "[300]\ttraining's rmse: 0.000373083\ttraining's RMSPE: 0.2555\tvalid_1's rmse: 0.000387653\tvalid_1's RMSPE: 0.2663\n",
      "[400]\ttraining's rmse: 0.000365746\ttraining's RMSPE: 0.2505\tvalid_1's rmse: 0.000382182\tvalid_1's RMSPE: 0.2625\n",
      "[500]\ttraining's rmse: 0.000361727\ttraining's RMSPE: 0.2477\tvalid_1's rmse: 0.000379917\tvalid_1's RMSPE: 0.261\n",
      "[600]\ttraining's rmse: 0.000358874\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000378219\tvalid_1's RMSPE: 0.2598\n",
      "[700]\ttraining's rmse: 0.000356545\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000377096\tvalid_1's RMSPE: 0.259\n",
      "[800]\ttraining's rmse: 0.00035453\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000376346\tvalid_1's RMSPE: 0.2585\n",
      "[900]\ttraining's rmse: 0.000352754\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.00037545\tvalid_1's RMSPE: 0.2579\n",
      "[1000]\ttraining's rmse: 0.000351237\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000374973\tvalid_1's RMSPE: 0.2576\n",
      "[1100]\ttraining's rmse: 0.000349867\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000374461\tvalid_1's RMSPE: 0.2572\n",
      "[1200]\ttraining's rmse: 0.000348621\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000374072\tvalid_1's RMSPE: 0.2569\n",
      "[1300]\ttraining's rmse: 0.000347444\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.000373742\tvalid_1's RMSPE: 0.2567\n",
      "[1400]\ttraining's rmse: 0.00034639\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000373472\tvalid_1's RMSPE: 0.2565\n",
      "[1500]\ttraining's rmse: 0.000345431\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000373265\tvalid_1's RMSPE: 0.2564\n",
      "[1600]\ttraining's rmse: 0.000344488\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000373115\tvalid_1's RMSPE: 0.2563\n",
      "[1700]\ttraining's rmse: 0.000343603\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000372868\tvalid_1's RMSPE: 0.2561\n",
      "[1800]\ttraining's rmse: 0.00034278\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000372684\tvalid_1's RMSPE: 0.256\n",
      "[1900]\ttraining's rmse: 0.000342009\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000372383\tvalid_1's RMSPE: 0.2558\n",
      "Early stopping, best iteration is:\n",
      "[1840]\ttraining's rmse: 0.000342487\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000372474\tvalid_1's RMSPE: 0.2558\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2558\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001173\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489113\ttraining's RMSPE: 0.3359\tvalid_1's rmse: 0.00049175\tvalid_1's RMSPE: 0.3338\n",
      "[200]\ttraining's rmse: 0.000394766\ttraining's RMSPE: 0.2711\tvalid_1's rmse: 0.00040081\tvalid_1's RMSPE: 0.2721\n",
      "[300]\ttraining's rmse: 0.000372652\ttraining's RMSPE: 0.2559\tvalid_1's rmse: 0.00038036\tvalid_1's RMSPE: 0.2582\n",
      "[400]\ttraining's rmse: 0.000365287\ttraining's RMSPE: 0.2509\tvalid_1's rmse: 0.000374283\tvalid_1's RMSPE: 0.2541\n",
      "[500]\ttraining's rmse: 0.00036132\ttraining's RMSPE: 0.2482\tvalid_1's rmse: 0.000371615\tvalid_1's RMSPE: 0.2523\n",
      "[600]\ttraining's rmse: 0.000358424\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000369951\tvalid_1's RMSPE: 0.2511\n",
      "[700]\ttraining's rmse: 0.000356062\ttraining's RMSPE: 0.2445\tvalid_1's rmse: 0.000368742\tvalid_1's RMSPE: 0.2503\n",
      "[800]\ttraining's rmse: 0.000354056\ttraining's RMSPE: 0.2432\tvalid_1's rmse: 0.000367665\tvalid_1's RMSPE: 0.2496\n",
      "[900]\ttraining's rmse: 0.000352308\ttraining's RMSPE: 0.242\tvalid_1's rmse: 0.000366808\tvalid_1's RMSPE: 0.249\n",
      "[1000]\ttraining's rmse: 0.00035073\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000366103\tvalid_1's RMSPE: 0.2485\n",
      "[1100]\ttraining's rmse: 0.00034938\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000365508\tvalid_1's RMSPE: 0.2481\n",
      "[1200]\ttraining's rmse: 0.000348128\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000364866\tvalid_1's RMSPE: 0.2477\n",
      "[1300]\ttraining's rmse: 0.000346954\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000364407\tvalid_1's RMSPE: 0.2474\n",
      "[1400]\ttraining's rmse: 0.000345918\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000364045\tvalid_1's RMSPE: 0.2471\n",
      "[1500]\ttraining's rmse: 0.000344938\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000363799\tvalid_1's RMSPE: 0.247\n",
      "[1600]\ttraining's rmse: 0.00034406\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000363505\tvalid_1's RMSPE: 0.2468\n",
      "[1700]\ttraining's rmse: 0.000343195\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000363142\tvalid_1's RMSPE: 0.2465\n",
      "[1800]\ttraining's rmse: 0.000342426\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000362889\tvalid_1's RMSPE: 0.2463\n",
      "[1900]\ttraining's rmse: 0.000341655\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000362642\tvalid_1's RMSPE: 0.2462\n",
      "[2000]\ttraining's rmse: 0.000340919\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000362393\tvalid_1's RMSPE: 0.246\n",
      "[2100]\ttraining's rmse: 0.000340228\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000362234\tvalid_1's RMSPE: 0.2459\n",
      "[2200]\ttraining's rmse: 0.000339554\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000362052\tvalid_1's RMSPE: 0.2458\n",
      "[2300]\ttraining's rmse: 0.00033892\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000361878\tvalid_1's RMSPE: 0.2457\n",
      "[2400]\ttraining's rmse: 0.00033833\ttraining's RMSPE: 0.2324\tvalid_1's rmse: 0.000361786\tvalid_1's RMSPE: 0.2456\n",
      "Early stopping, best iteration is:\n",
      "[2301]\ttraining's rmse: 0.000338914\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000361873\tvalid_1's RMSPE: 0.2456\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2456\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 4\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001172\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 0.000489929\ttraining's RMSPE: 0.337\tvalid_1's rmse: 0.000490469\tvalid_1's RMSPE: 0.3309\n",
      "[200]\ttraining's rmse: 0.000395197\ttraining's RMSPE: 0.2718\tvalid_1's rmse: 0.000399202\tvalid_1's RMSPE: 0.2693\n",
      "[300]\ttraining's rmse: 0.000372858\ttraining's RMSPE: 0.2565\tvalid_1's rmse: 0.000378732\tvalid_1's RMSPE: 0.2555\n",
      "[400]\ttraining's rmse: 0.000365195\ttraining's RMSPE: 0.2512\tvalid_1's rmse: 0.000372263\tvalid_1's RMSPE: 0.2511\n",
      "[500]\ttraining's rmse: 0.000361105\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000369132\tvalid_1's RMSPE: 0.249\n",
      "[600]\ttraining's rmse: 0.000358136\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.00036705\tvalid_1's RMSPE: 0.2476\n",
      "[700]\ttraining's rmse: 0.000355741\ttraining's RMSPE: 0.2447\tvalid_1's rmse: 0.00036544\tvalid_1's RMSPE: 0.2465\n",
      "[800]\ttraining's rmse: 0.000353701\ttraining's RMSPE: 0.2433\tvalid_1's rmse: 0.000364164\tvalid_1's RMSPE: 0.2457\n",
      "[900]\ttraining's rmse: 0.000351884\ttraining's RMSPE: 0.2421\tvalid_1's rmse: 0.00036312\tvalid_1's RMSPE: 0.2449\n",
      "[1000]\ttraining's rmse: 0.00035034\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000362233\tvalid_1's RMSPE: 0.2443\n",
      "[1100]\ttraining's rmse: 0.000348962\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000361474\tvalid_1's RMSPE: 0.2438\n",
      "[1200]\ttraining's rmse: 0.000347707\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000360829\tvalid_1's RMSPE: 0.2434\n",
      "[1300]\ttraining's rmse: 0.000346528\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000360215\tvalid_1's RMSPE: 0.243\n",
      "[1400]\ttraining's rmse: 0.000345454\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000359716\tvalid_1's RMSPE: 0.2426\n",
      "[1500]\ttraining's rmse: 0.000344482\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000359297\tvalid_1's RMSPE: 0.2424\n",
      "[1600]\ttraining's rmse: 0.000343521\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000358828\tvalid_1's RMSPE: 0.2421\n",
      "[1700]\ttraining's rmse: 0.000342664\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.00035849\tvalid_1's RMSPE: 0.2418\n",
      "[1800]\ttraining's rmse: 0.00034186\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000358184\tvalid_1's RMSPE: 0.2416\n",
      "[1900]\ttraining's rmse: 0.000341065\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000357902\tvalid_1's RMSPE: 0.2414\n",
      "[2000]\ttraining's rmse: 0.000340334\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000357682\tvalid_1's RMSPE: 0.2413\n",
      "[2100]\ttraining's rmse: 0.000339608\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000357443\tvalid_1's RMSPE: 0.2411\n",
      "[2200]\ttraining's rmse: 0.000338941\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000357234\tvalid_1's RMSPE: 0.241\n",
      "[2300]\ttraining's rmse: 0.000338306\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000357055\tvalid_1's RMSPE: 0.2409\n",
      "[2400]\ttraining's rmse: 0.00033768\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000356891\tvalid_1's RMSPE: 0.2407\n",
      "Early stopping, best iteration is:\n",
      "[2396]\ttraining's rmse: 0.000337701\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000356894\tvalid_1's RMSPE: 0.2407\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2407\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048898\ttraining's RMSPE: 0.3352\tvalid_1's rmse: 0.000491898\tvalid_1's RMSPE: 0.3364\n",
      "[200]\ttraining's rmse: 0.000394043\ttraining's RMSPE: 0.2702\tvalid_1's rmse: 0.000403008\tvalid_1's RMSPE: 0.2756\n",
      "[300]\ttraining's rmse: 0.000371798\ttraining's RMSPE: 0.2549\tvalid_1's rmse: 0.00038348\tvalid_1's RMSPE: 0.2622\n",
      "[400]\ttraining's rmse: 0.000364453\ttraining's RMSPE: 0.2499\tvalid_1's rmse: 0.000377782\tvalid_1's RMSPE: 0.2583\n",
      "[500]\ttraining's rmse: 0.00036055\ttraining's RMSPE: 0.2472\tvalid_1's rmse: 0.000375166\tvalid_1's RMSPE: 0.2565\n",
      "[600]\ttraining's rmse: 0.000357641\ttraining's RMSPE: 0.2452\tvalid_1's rmse: 0.000373443\tvalid_1's RMSPE: 0.2554\n",
      "[700]\ttraining's rmse: 0.00035536\ttraining's RMSPE: 0.2436\tvalid_1's rmse: 0.000372141\tvalid_1's RMSPE: 0.2545\n",
      "[800]\ttraining's rmse: 0.000353333\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000370995\tvalid_1's RMSPE: 0.2537\n",
      "[900]\ttraining's rmse: 0.000351598\ttraining's RMSPE: 0.2411\tvalid_1's rmse: 0.000370167\tvalid_1's RMSPE: 0.2531\n",
      "[1000]\ttraining's rmse: 0.000350076\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000369361\tvalid_1's RMSPE: 0.2526\n",
      "[1100]\ttraining's rmse: 0.00034876\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000368745\tvalid_1's RMSPE: 0.2521\n",
      "[1200]\ttraining's rmse: 0.000347508\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000368213\tvalid_1's RMSPE: 0.2518\n",
      "[1300]\ttraining's rmse: 0.000346445\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000367721\tvalid_1's RMSPE: 0.2514\n",
      "[1400]\ttraining's rmse: 0.00034539\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000367415\tvalid_1's RMSPE: 0.2512\n",
      "[1500]\ttraining's rmse: 0.000344465\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.00036696\tvalid_1's RMSPE: 0.2509\n",
      "[1600]\ttraining's rmse: 0.00034358\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000366628\tvalid_1's RMSPE: 0.2507\n",
      "[1700]\ttraining's rmse: 0.00034272\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.00036623\tvalid_1's RMSPE: 0.2504\n",
      "[1800]\ttraining's rmse: 0.000341912\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000365994\tvalid_1's RMSPE: 0.2503\n",
      "[1900]\ttraining's rmse: 0.000341164\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000365719\tvalid_1's RMSPE: 0.2501\n",
      "[2000]\ttraining's rmse: 0.000340439\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000365475\tvalid_1's RMSPE: 0.2499\n",
      "[2100]\ttraining's rmse: 0.000339742\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000365245\tvalid_1's RMSPE: 0.2497\n",
      "[2200]\ttraining's rmse: 0.000339082\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000365103\tvalid_1's RMSPE: 0.2497\n",
      "Early stopping, best iteration is:\n",
      "[2100]\ttraining's rmse: 0.000339742\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000365245\tvalid_1's RMSPE: 0.2497\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2497\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001178\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488127\ttraining's RMSPE: 0.3347\tvalid_1's rmse: 0.000505563\tvalid_1's RMSPE: 0.3455\n",
      "[200]\ttraining's rmse: 0.000394222\ttraining's RMSPE: 0.2703\tvalid_1's rmse: 0.00042433\tvalid_1's RMSPE: 0.29\n",
      "[300]\ttraining's rmse: 0.000372103\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000407177\tvalid_1's RMSPE: 0.2783\n",
      "[400]\ttraining's rmse: 0.000364684\ttraining's RMSPE: 0.2501\tvalid_1's rmse: 0.000401541\tvalid_1's RMSPE: 0.2744\n",
      "[500]\ttraining's rmse: 0.000360632\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.000398113\tvalid_1's RMSPE: 0.2721\n",
      "[600]\ttraining's rmse: 0.000357779\ttraining's RMSPE: 0.2453\tvalid_1's rmse: 0.000396017\tvalid_1's RMSPE: 0.2706\n",
      "[700]\ttraining's rmse: 0.000355406\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000394765\tvalid_1's RMSPE: 0.2698\n",
      "[800]\ttraining's rmse: 0.000353485\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000393668\tvalid_1's RMSPE: 0.269\n",
      "[900]\ttraining's rmse: 0.0003517\ttraining's RMSPE: 0.2412\tvalid_1's rmse: 0.000392886\tvalid_1's RMSPE: 0.2685\n",
      "[1000]\ttraining's rmse: 0.000350196\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000391824\tvalid_1's RMSPE: 0.2678\n",
      "[1100]\ttraining's rmse: 0.000348848\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000391759\tvalid_1's RMSPE: 0.2677\n",
      "Early stopping, best iteration is:\n",
      "[1048]\ttraining's rmse: 0.00034951\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000391482\tvalid_1's RMSPE: 0.2675\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2675\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.001193\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489353\ttraining's RMSPE: 0.3333\tvalid_1's rmse: 0.000510631\tvalid_1's RMSPE: 0.3582\n",
      "[200]\ttraining's rmse: 0.000396578\ttraining's RMSPE: 0.2701\tvalid_1's rmse: 0.000415241\tvalid_1's RMSPE: 0.2913\n",
      "[300]\ttraining's rmse: 0.000374683\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000392286\tvalid_1's RMSPE: 0.2752\n",
      "[400]\ttraining's rmse: 0.000367198\ttraining's RMSPE: 0.2501\tvalid_1's rmse: 0.000385705\tvalid_1's RMSPE: 0.2706\n",
      "[500]\ttraining's rmse: 0.000363056\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.000381388\tvalid_1's RMSPE: 0.2675\n",
      "[600]\ttraining's rmse: 0.000360137\ttraining's RMSPE: 0.2453\tvalid_1's rmse: 0.000379369\tvalid_1's RMSPE: 0.2661\n",
      "[700]\ttraining's rmse: 0.000357713\ttraining's RMSPE: 0.2436\tvalid_1's rmse: 0.000377039\tvalid_1's RMSPE: 0.2645\n",
      "[800]\ttraining's rmse: 0.000355712\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000375073\tvalid_1's RMSPE: 0.2631\n",
      "[900]\ttraining's rmse: 0.000353879\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000374061\tvalid_1's RMSPE: 0.2624\n",
      "[1000]\ttraining's rmse: 0.000352357\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000373693\tvalid_1's RMSPE: 0.2621\n",
      "Early stopping, best iteration is:\n",
      "[986]\ttraining's rmse: 0.000352562\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000373428\tvalid_1's RMSPE: 0.2619\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2619\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488392\ttraining's RMSPE: 0.335\tvalid_1's rmse: 0.00049988\tvalid_1's RMSPE: 0.3413\n",
      "[200]\ttraining's rmse: 0.000394443\ttraining's RMSPE: 0.2705\tvalid_1's rmse: 0.000416053\tvalid_1's RMSPE: 0.284\n",
      "[300]\ttraining's rmse: 0.000372281\ttraining's RMSPE: 0.2553\tvalid_1's rmse: 0.000398376\tvalid_1's RMSPE: 0.272\n",
      "[400]\ttraining's rmse: 0.000364825\ttraining's RMSPE: 0.2502\tvalid_1's rmse: 0.00039327\tvalid_1's RMSPE: 0.2685\n",
      "[500]\ttraining's rmse: 0.000360791\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000391061\tvalid_1's RMSPE: 0.267\n",
      "[600]\ttraining's rmse: 0.000357978\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000389806\tvalid_1's RMSPE: 0.2661\n",
      "[700]\ttraining's rmse: 0.000355611\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000388524\tvalid_1's RMSPE: 0.2652\n",
      "[800]\ttraining's rmse: 0.000353655\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000387645\tvalid_1's RMSPE: 0.2646\n",
      "[900]\ttraining's rmse: 0.000351878\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000386885\tvalid_1's RMSPE: 0.2641\n",
      "[1000]\ttraining's rmse: 0.000350365\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000386294\tvalid_1's RMSPE: 0.2637\n",
      "[1100]\ttraining's rmse: 0.000349013\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000385998\tvalid_1's RMSPE: 0.2635\n",
      "[1200]\ttraining's rmse: 0.000347767\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000385348\tvalid_1's RMSPE: 0.2631\n",
      "[1300]\ttraining's rmse: 0.000346616\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.000384941\tvalid_1's RMSPE: 0.2628\n",
      "[1400]\ttraining's rmse: 0.000345603\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000384568\tvalid_1's RMSPE: 0.2625\n",
      "[1500]\ttraining's rmse: 0.000344638\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.000384355\tvalid_1's RMSPE: 0.2624\n",
      "[1600]\ttraining's rmse: 0.000343712\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000384137\tvalid_1's RMSPE: 0.2622\n",
      "Early stopping, best iteration is:\n",
      "[1580]\ttraining's rmse: 0.000343898\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000384131\tvalid_1's RMSPE: 0.2622\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2622\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 5\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001175\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488391\ttraining's RMSPE: 0.3354\tvalid_1's rmse: 0.000489065\tvalid_1's RMSPE: 0.331\n",
      "[200]\ttraining's rmse: 0.000394295\ttraining's RMSPE: 0.2708\tvalid_1's rmse: 0.000397446\tvalid_1's RMSPE: 0.269\n",
      "[300]\ttraining's rmse: 0.000372374\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.000377024\tvalid_1's RMSPE: 0.2551\n",
      "[400]\ttraining's rmse: 0.000364875\ttraining's RMSPE: 0.2506\tvalid_1's rmse: 0.000370842\tvalid_1's RMSPE: 0.251\n",
      "[500]\ttraining's rmse: 0.000360915\ttraining's RMSPE: 0.2479\tvalid_1's rmse: 0.000367882\tvalid_1's RMSPE: 0.249\n",
      "[600]\ttraining's rmse: 0.000358041\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000366002\tvalid_1's RMSPE: 0.2477\n",
      "[700]\ttraining's rmse: 0.000355702\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000364548\tvalid_1's RMSPE: 0.2467\n",
      "[800]\ttraining's rmse: 0.000353746\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000363404\tvalid_1's RMSPE: 0.2459\n",
      "[900]\ttraining's rmse: 0.000352005\ttraining's RMSPE: 0.2417\tvalid_1's rmse: 0.000362447\tvalid_1's RMSPE: 0.2453\n",
      "[1000]\ttraining's rmse: 0.000350446\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000361667\tvalid_1's RMSPE: 0.2448\n",
      "[1100]\ttraining's rmse: 0.000349098\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000361\tvalid_1's RMSPE: 0.2443\n",
      "[1200]\ttraining's rmse: 0.000347862\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000360372\tvalid_1's RMSPE: 0.2439\n",
      "[1300]\ttraining's rmse: 0.000346741\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000359884\tvalid_1's RMSPE: 0.2435\n",
      "[1400]\ttraining's rmse: 0.000345666\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.00035941\tvalid_1's RMSPE: 0.2432\n",
      "[1500]\ttraining's rmse: 0.000344723\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000359013\tvalid_1's RMSPE: 0.243\n",
      "[1600]\ttraining's rmse: 0.000343855\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000358689\tvalid_1's RMSPE: 0.2427\n",
      "[1700]\ttraining's rmse: 0.000342973\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000358318\tvalid_1's RMSPE: 0.2425\n",
      "[1800]\ttraining's rmse: 0.000342164\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000357985\tvalid_1's RMSPE: 0.2423\n",
      "[1900]\ttraining's rmse: 0.000341424\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000357786\tvalid_1's RMSPE: 0.2421\n",
      "[2000]\ttraining's rmse: 0.000340661\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000357605\tvalid_1's RMSPE: 0.242\n",
      "[2100]\ttraining's rmse: 0.000339949\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000357367\tvalid_1's RMSPE: 0.2418\n",
      "[2200]\ttraining's rmse: 0.000339267\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000357127\tvalid_1's RMSPE: 0.2417\n",
      "[2300]\ttraining's rmse: 0.000338623\ttraining's RMSPE: 0.2326\tvalid_1's rmse: 0.000356978\tvalid_1's RMSPE: 0.2416\n",
      "Early stopping, best iteration is:\n",
      "[2230]\ttraining's rmse: 0.000339067\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.00035708\tvalid_1's RMSPE: 0.2416\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2417\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001176\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486795\ttraining's RMSPE: 0.334\tvalid_1's rmse: 0.000495321\tvalid_1's RMSPE: 0.3364\n",
      "[200]\ttraining's rmse: 0.00039296\ttraining's RMSPE: 0.2696\tvalid_1's rmse: 0.000410606\tvalid_1's RMSPE: 0.2788\n",
      "[300]\ttraining's rmse: 0.000371332\ttraining's RMSPE: 0.2548\tvalid_1's rmse: 0.000392724\tvalid_1's RMSPE: 0.2667\n",
      "[400]\ttraining's rmse: 0.000364297\ttraining's RMSPE: 0.25\tvalid_1's rmse: 0.000387395\tvalid_1's RMSPE: 0.2631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 0.00036047\ttraining's RMSPE: 0.2474\tvalid_1's rmse: 0.000384955\tvalid_1's RMSPE: 0.2614\n",
      "[600]\ttraining's rmse: 0.000357756\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000383427\tvalid_1's RMSPE: 0.2604\n",
      "[700]\ttraining's rmse: 0.00035551\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000382096\tvalid_1's RMSPE: 0.2595\n",
      "[800]\ttraining's rmse: 0.000353631\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.000381028\tvalid_1's RMSPE: 0.2588\n",
      "[900]\ttraining's rmse: 0.000351978\ttraining's RMSPE: 0.2415\tvalid_1's rmse: 0.000380114\tvalid_1's RMSPE: 0.2581\n",
      "[1000]\ttraining's rmse: 0.000350534\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000379296\tvalid_1's RMSPE: 0.2576\n",
      "[1100]\ttraining's rmse: 0.000349224\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.00037868\tvalid_1's RMSPE: 0.2572\n",
      "[1200]\ttraining's rmse: 0.000348067\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.00037818\tvalid_1's RMSPE: 0.2568\n",
      "[1300]\ttraining's rmse: 0.000346972\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000377806\tvalid_1's RMSPE: 0.2566\n",
      "[1400]\ttraining's rmse: 0.000345958\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000377353\tvalid_1's RMSPE: 0.2563\n",
      "[1500]\ttraining's rmse: 0.000344971\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000376831\tvalid_1's RMSPE: 0.2559\n",
      "[1600]\ttraining's rmse: 0.00034408\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000376516\tvalid_1's RMSPE: 0.2557\n",
      "[1700]\ttraining's rmse: 0.000343212\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000376061\tvalid_1's RMSPE: 0.2554\n",
      "[1800]\ttraining's rmse: 0.000342406\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000375782\tvalid_1's RMSPE: 0.2552\n",
      "[1900]\ttraining's rmse: 0.000341653\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000375603\tvalid_1's RMSPE: 0.2551\n",
      "[2000]\ttraining's rmse: 0.000340944\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000375353\tvalid_1's RMSPE: 0.2549\n",
      "[2100]\ttraining's rmse: 0.000340202\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000375152\tvalid_1's RMSPE: 0.2548\n",
      "[2200]\ttraining's rmse: 0.000339541\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000375012\tvalid_1's RMSPE: 0.2547\n",
      "Early stopping, best iteration is:\n",
      "[2187]\ttraining's rmse: 0.000339635\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000374984\tvalid_1's RMSPE: 0.2546\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2546\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001181\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488168\ttraining's RMSPE: 0.3341\tvalid_1's rmse: 0.000493057\tvalid_1's RMSPE: 0.3383\n",
      "[200]\ttraining's rmse: 0.000394065\ttraining's RMSPE: 0.2697\tvalid_1's rmse: 0.000403031\tvalid_1's RMSPE: 0.2766\n",
      "[300]\ttraining's rmse: 0.00037187\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000382934\tvalid_1's RMSPE: 0.2628\n",
      "[400]\ttraining's rmse: 0.000364356\ttraining's RMSPE: 0.2494\tvalid_1's rmse: 0.000376641\tvalid_1's RMSPE: 0.2584\n",
      "[500]\ttraining's rmse: 0.000360372\ttraining's RMSPE: 0.2466\tvalid_1's rmse: 0.000373813\tvalid_1's RMSPE: 0.2565\n",
      "[600]\ttraining's rmse: 0.000357541\ttraining's RMSPE: 0.2447\tvalid_1's rmse: 0.000372137\tvalid_1's RMSPE: 0.2554\n",
      "[700]\ttraining's rmse: 0.000355268\ttraining's RMSPE: 0.2432\tvalid_1's rmse: 0.000370917\tvalid_1's RMSPE: 0.2545\n",
      "[800]\ttraining's rmse: 0.000353314\ttraining's RMSPE: 0.2418\tvalid_1's rmse: 0.000369822\tvalid_1's RMSPE: 0.2538\n",
      "[900]\ttraining's rmse: 0.000351625\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000368841\tvalid_1's RMSPE: 0.2531\n",
      "[1000]\ttraining's rmse: 0.000350155\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000368126\tvalid_1's RMSPE: 0.2526\n",
      "[1100]\ttraining's rmse: 0.000348874\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000367513\tvalid_1's RMSPE: 0.2522\n",
      "[1200]\ttraining's rmse: 0.000347689\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000366986\tvalid_1's RMSPE: 0.2518\n",
      "[1300]\ttraining's rmse: 0.000346601\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000366561\tvalid_1's RMSPE: 0.2515\n",
      "[1400]\ttraining's rmse: 0.000345546\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000366101\tvalid_1's RMSPE: 0.2512\n",
      "[1500]\ttraining's rmse: 0.000344578\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000365735\tvalid_1's RMSPE: 0.251\n",
      "[1600]\ttraining's rmse: 0.000343686\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000365435\tvalid_1's RMSPE: 0.2508\n",
      "[1700]\ttraining's rmse: 0.000342815\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000365155\tvalid_1's RMSPE: 0.2506\n",
      "[1800]\ttraining's rmse: 0.000342051\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000364873\tvalid_1's RMSPE: 0.2504\n",
      "[1900]\ttraining's rmse: 0.000341345\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000364575\tvalid_1's RMSPE: 0.2502\n",
      "[2000]\ttraining's rmse: 0.00034067\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000364371\tvalid_1's RMSPE: 0.25\n",
      "[2100]\ttraining's rmse: 0.000339995\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000364209\tvalid_1's RMSPE: 0.2499\n",
      "[2200]\ttraining's rmse: 0.000339353\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000364082\tvalid_1's RMSPE: 0.2498\n",
      "Early stopping, best iteration is:\n",
      "[2136]\ttraining's rmse: 0.000339755\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000364115\tvalid_1's RMSPE: 0.2498\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2498\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001195\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488776\ttraining's RMSPE: 0.3326\tvalid_1's rmse: 0.000515142\tvalid_1's RMSPE: 0.3616\n",
      "[200]\ttraining's rmse: 0.000396106\ttraining's RMSPE: 0.2695\tvalid_1's rmse: 0.000420428\tvalid_1's RMSPE: 0.2951\n",
      "[300]\ttraining's rmse: 0.000374564\ttraining's RMSPE: 0.2549\tvalid_1's rmse: 0.000395883\tvalid_1's RMSPE: 0.2779\n",
      "[400]\ttraining's rmse: 0.000367304\ttraining's RMSPE: 0.2499\tvalid_1's rmse: 0.000387372\tvalid_1's RMSPE: 0.2719\n",
      "[500]\ttraining's rmse: 0.000363354\ttraining's RMSPE: 0.2472\tvalid_1's rmse: 0.000384029\tvalid_1's RMSPE: 0.2696\n",
      "[600]\ttraining's rmse: 0.00036047\ttraining's RMSPE: 0.2453\tvalid_1's rmse: 0.0003819\tvalid_1's RMSPE: 0.2681\n",
      "[700]\ttraining's rmse: 0.000358159\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000380819\tvalid_1's RMSPE: 0.2673\n",
      "[800]\ttraining's rmse: 0.000356177\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000379499\tvalid_1's RMSPE: 0.2664\n",
      "[900]\ttraining's rmse: 0.000354427\ttraining's RMSPE: 0.2412\tvalid_1's rmse: 0.000378503\tvalid_1's RMSPE: 0.2657\n",
      "[1000]\ttraining's rmse: 0.00035293\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000377375\tvalid_1's RMSPE: 0.2649\n",
      "[1100]\ttraining's rmse: 0.000351595\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000376301\tvalid_1's RMSPE: 0.2641\n",
      "[1200]\ttraining's rmse: 0.000350382\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000377015\tvalid_1's RMSPE: 0.2646\n",
      "Early stopping, best iteration is:\n",
      "[1119]\ttraining's rmse: 0.000351348\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000376013\tvalid_1's RMSPE: 0.2639\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2639\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001176\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488225\ttraining's RMSPE: 0.335\tvalid_1's rmse: 0.000494503\tvalid_1's RMSPE: 0.3361\n",
      "[200]\ttraining's rmse: 0.000393746\ttraining's RMSPE: 0.2701\tvalid_1's rmse: 0.000406211\tvalid_1's RMSPE: 0.2761\n",
      "[300]\ttraining's rmse: 0.000371737\ttraining's RMSPE: 0.255\tvalid_1's rmse: 0.000386834\tvalid_1's RMSPE: 0.2629\n",
      "[400]\ttraining's rmse: 0.00036436\ttraining's RMSPE: 0.25\tvalid_1's rmse: 0.000381466\tvalid_1's RMSPE: 0.2593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 0.000360424\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.000378979\tvalid_1's RMSPE: 0.2576\n",
      "[600]\ttraining's rmse: 0.000357669\ttraining's RMSPE: 0.2454\tvalid_1's rmse: 0.000377159\tvalid_1's RMSPE: 0.2563\n",
      "[700]\ttraining's rmse: 0.000355325\ttraining's RMSPE: 0.2438\tvalid_1's rmse: 0.000375809\tvalid_1's RMSPE: 0.2554\n",
      "[800]\ttraining's rmse: 0.000353392\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000374738\tvalid_1's RMSPE: 0.2547\n",
      "[900]\ttraining's rmse: 0.000351699\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000373792\tvalid_1's RMSPE: 0.254\n",
      "[1000]\ttraining's rmse: 0.00035021\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000372945\tvalid_1's RMSPE: 0.2535\n",
      "[1100]\ttraining's rmse: 0.000348891\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000372415\tvalid_1's RMSPE: 0.2531\n",
      "[1200]\ttraining's rmse: 0.000347721\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000372019\tvalid_1's RMSPE: 0.2528\n",
      "[1300]\ttraining's rmse: 0.000346635\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000371649\tvalid_1's RMSPE: 0.2526\n",
      "[1400]\ttraining's rmse: 0.000345651\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000371175\tvalid_1's RMSPE: 0.2523\n",
      "[1500]\ttraining's rmse: 0.000344745\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000370812\tvalid_1's RMSPE: 0.252\n",
      "[1600]\ttraining's rmse: 0.000343885\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000370631\tvalid_1's RMSPE: 0.2519\n",
      "[1700]\ttraining's rmse: 0.000343067\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000370528\tvalid_1's RMSPE: 0.2518\n",
      "Early stopping, best iteration is:\n",
      "[1626]\ttraining's rmse: 0.000343657\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.00037056\tvalid_1's RMSPE: 0.2518\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2518\n",
      "\t**********************************************************************\n",
      "Wall time: 21min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPSILON = 0\n",
    "norm_test = pd.DataFrame(\n",
    "    {'target_realized_volatility':[],'predicted_volatility_norm': [], 'time_id':[], 'stock_id':[]}\n",
    "\n",
    ")\n",
    "norm_models = []\n",
    "norm_split_importance = []\n",
    "norm_gain_importance = []\n",
    "train_scores = []\n",
    "inner_k = 5\n",
    "outer_k = 5\n",
    "\n",
    "params =  {\n",
    "    'boosting_type': 'goss',\n",
    "    'learning_rate': 0.01,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8, \n",
    "    'bagging_fraction': 0.8,\n",
    "    'lambda_l1': 1.2,\n",
    "    'lambda_l2': 1.2,\n",
    "    'n_jobs': -1,\n",
    "    'force_col_wise': True,\n",
    "    'extra_trees': True,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "outer_kfold = KFold(n_splits=outer_k, random_state=42, shuffle=True)\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_kfold.split(normX, normY)):\n",
    "    print('*'*120)\n",
    "    print(\"Outer Fold :\", outer_fold + 1)\n",
    "    print('*'*120)\n",
    "\n",
    "    X_outer_train = normX.iloc[outer_train_idx].reset_index(drop=True)  \n",
    "    X_outer_test = normX.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    y_outer_train = normY.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    y_outer_test = normY.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    \n",
    "    target = np.zeros(len(y_outer_test))\n",
    "    inner_scores = 0.0\n",
    "    models = []\n",
    "    \n",
    "    inner_kfold = KFold(n_splits= inner_k, random_state=42, shuffle=True)\n",
    "    for inner_fold, (inner_train_idx, inner_valid_idx) in enumerate(inner_kfold.split(X_outer_train, y_outer_train)):\n",
    "        print(\"\\n\\t\"+\"*\"*20)\n",
    "        print(f\"\\t*  Inner Fold : {inner_fold + 1}  *\")\n",
    "        print(\"\\t\"+\"*\"*20+\"\\n\")\n",
    "    \n",
    "        # inner train data and valid data\n",
    "        X_inner_train = X_outer_train.iloc[inner_train_idx].reset_index(drop=True)\n",
    "        X_inner_valid = X_outer_train.iloc[inner_valid_idx].reset_index(drop=True)\n",
    "\n",
    "        y_inner_train = y_outer_train.iloc[inner_train_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "        y_inner_valid = y_outer_train.iloc[inner_valid_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "            \n",
    "        lgbm_train = lgbm.Dataset(X_inner_train,y_inner_train,weight=1/(np.square(y_inner_train.values)+EPSILON))\n",
    "        lgbm_valid = lgbm.Dataset(\n",
    "            X_inner_valid,y_inner_valid,reference=lgbm_train,weight=1/(np.square(y_inner_valid.values)+EPSILON))\n",
    "        \n",
    "        # model training\n",
    "        model = lgbm.train(\n",
    "            params=params, #tuner.best_params,\n",
    "            train_set=lgbm_train,\n",
    "            valid_sets=[lgbm_train, lgbm_valid],\n",
    "            num_boost_round=10000,       \n",
    "            feval=feval_RMSPE,\n",
    "            callbacks=[lgbm.log_evaluation(period=100), lgbm.early_stopping(100)]\n",
    "        )\n",
    "        # validation \n",
    "        y_inner_pred = model.predict(X_inner_valid, num_iteration=model.best_iteration)\n",
    "        RMSPE = rmspe(\n",
    "            y_true=(y_inner_valid.values.flatten()), \n",
    "            y_pred=(y_inner_pred), n=4\n",
    "        )\n",
    "        \n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "        print(f'\\tInner Validation RMSPE: \\t{RMSPE}')\n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "\n",
    "        # keep training validation score\n",
    "        inner_scores += RMSPE / inner_k\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "        # record feature importances by gain and split\n",
    "        features = list(X_inner_train.columns.values)\n",
    "        \n",
    "        norm_gain_importance.append(compute_importance(model, features, typ='gain'))\n",
    "        norm_split_importance.append(compute_importance(model, features, typ='split'))\n",
    "        \n",
    "    # store all models for prediction in oof evaluation\n",
    "    norm_models.append(models)\n",
    "    train_scores.append(inner_scores)\n",
    "    \n",
    "    # out of fold test set\n",
    "    for model in norm_models[outer_fold]:\n",
    "        y_outer_pred = model.predict(X_outer_test,num_iteration=model.best_iteration)\n",
    "        target += y_outer_pred / len(norm_models[outer_fold])\n",
    "    \n",
    "    y_outer_test = y_outer_test.assign(predicted_volatility_norm = target)\n",
    " \n",
    "    norm_test = pd.concat([norm_test, y_outer_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd33bfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.255564"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(norm_test['target_realized_volatility'], norm_test['predicted_volatility_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf4767ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>predicted_volatility_norm</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428926</th>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>32712.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  predicted_volatility_norm  time_id  \\\n",
       "0                         0.003397                   0.002773    103.0   \n",
       "1                         0.002993                   0.004439    146.0   \n",
       "2                         0.001094                   0.001236    250.0   \n",
       "3                         0.001711                   0.001649    297.0   \n",
       "4                         0.001197                   0.002248    319.0   \n",
       "...                            ...                        ...      ...   \n",
       "428926                    0.004120                   0.002958  32712.0   \n",
       "428927                    0.003511                   0.004066  32724.0   \n",
       "428928                    0.010431                   0.010297  32746.0   \n",
       "428929                    0.001827                   0.002151  32750.0   \n",
       "428930                    0.003454                   0.002089  32753.0   \n",
       "\n",
       "        stock_id  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "...          ...  \n",
       "428926     126.0  \n",
       "428927     126.0  \n",
       "428928     126.0  \n",
       "428929     126.0  \n",
       "428930     126.0  \n",
       "\n",
       "[428931 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290a346",
   "metadata": {},
   "source": [
    "## Poly Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4118d05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks = pd.read_feather(\"simple.fth\")\n",
    "stocks = remove_clusters(stocks)\n",
    "\n",
    "len(stocks[stocks['target_realized_volatility'] == 0].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0f59198a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>beta2-300</th>\n",
       "      <th>target_mean_enc</th>\n",
       "      <th>spread_mean_enc</th>\n",
       "      <th>dom_mean_enc</th>\n",
       "      <th>target_std_enc</th>\n",
       "      <th>spread_std_enc</th>\n",
       "      <th>dom_std_enc</th>\n",
       "      <th>encode_mean_beta</th>\n",
       "      <th>encode_mean_beta2</th>\n",
       "      <th>kmeans5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778640</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>391.300538</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>104.550467</td>\n",
       "      <td>0.635215</td>\n",
       "      <td>0.633070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>391.419207</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>104.729473</td>\n",
       "      <td>0.634261</td>\n",
       "      <td>0.632797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.519335</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>390.605604</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>103.699696</td>\n",
       "      <td>0.632499</td>\n",
       "      <td>0.630289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.378868</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>391.181654</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>104.727931</td>\n",
       "      <td>0.636051</td>\n",
       "      <td>0.633793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973303</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>391.062991</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>104.675748</td>\n",
       "      <td>0.633840</td>\n",
       "      <td>0.633266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049953</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.917628</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.568403</td>\n",
       "      <td>0.961481</td>\n",
       "      <td>0.958937</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160123</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.850800</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>99.572674</td>\n",
       "      <td>0.962304</td>\n",
       "      <td>0.960865</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046489</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>383.053325</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>99.715361</td>\n",
       "      <td>0.959879</td>\n",
       "      <td>0.957927</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.315896</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.605401</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.632305</td>\n",
       "      <td>0.961479</td>\n",
       "      <td>0.958523</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.130582</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>382.545423</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.228970</td>\n",
       "      <td>0.961903</td>\n",
       "      <td>0.960295</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954    194.495455     0.164908   \n",
       "1            11                    0.000981    199.598260     0.031047   \n",
       "2            16                    0.001295    209.021831     0.092861   \n",
       "3            31                    0.001776    216.281256     0.183025   \n",
       "4            62                    0.001520    214.542788     0.051133   \n",
       "...         ...                         ...           ...          ...   \n",
       "428786    32751                    0.002899    306.672174     0.163919   \n",
       "428787    32753                    0.003454    291.124934     0.147318   \n",
       "428788    32758                    0.002792    202.972820     0.064758   \n",
       "428789    32763                    0.002379    152.478929     0.068413   \n",
       "428790    32767                    0.001414    194.982143     0.040268   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0          194.479014      0.196558                             0.003394   \n",
       "1          199.597336      0.036259                             0.000699   \n",
       "2          209.053034      0.098250                             0.001983   \n",
       "3          216.198136      0.164985                             0.001863   \n",
       "4          214.524415      0.071793                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428786     306.730549      0.206509                             0.002284   \n",
       "428787     291.137380      0.164838                             0.002217   \n",
       "428788     202.958539      0.080424                             0.001386   \n",
       "428789     152.480008      0.075165                             0.002783   \n",
       "428790     195.012846      0.055999                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428786                             0.000132                2.117007e-10   \n",
       "428787                             0.000128                9.484441e-11   \n",
       "428788                             0.000080                3.031629e-11   \n",
       "428789                             0.000161                1.960849e-10   \n",
       "428790                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  ...  beta2-300  target_mean_enc  \\\n",
       "0               7.138250e-06  ...   0.778640         0.002964   \n",
       "1               8.823633e-07  ...   0.450700         0.002976   \n",
       "2               1.729093e-06  ...   1.519335         0.002974   \n",
       "3              -5.516464e-06  ...   2.378868         0.002975   \n",
       "4              -2.164288e-06  ...   0.973303         0.002974   \n",
       "...                      ...  ...        ...              ...   \n",
       "428786         -2.858852e-06  ...   1.049953         0.003899   \n",
       "428787          3.674218e-06  ...   2.160123         0.003897   \n",
       "428788         -2.437732e-06  ...  -0.046489         0.003903   \n",
       "428789          6.737309e-06  ...   1.315896         0.003896   \n",
       "428790         -1.059089e-06  ...  -2.130582         0.003894   \n",
       "\n",
       "        spread_mean_enc  dom_mean_enc  target_std_enc  spread_std_enc  \\\n",
       "0              0.001023    391.300538        0.002289        0.000925   \n",
       "1              0.001027    391.419207        0.002414        0.000949   \n",
       "2              0.001025    390.605604        0.002411        0.000946   \n",
       "3              0.001026    391.181654        0.002411        0.000947   \n",
       "4              0.001027    391.062991        0.002412        0.000948   \n",
       "...                 ...           ...             ...             ...   \n",
       "428786         0.001108    382.917628        0.002595        0.000784   \n",
       "428787         0.001108    382.850800        0.002590        0.000783   \n",
       "428788         0.001109    383.053325        0.002601        0.000786   \n",
       "428789         0.001108    382.605401        0.002593        0.000784   \n",
       "428790         0.001107    382.545423        0.002589        0.000784   \n",
       "\n",
       "        dom_std_enc  encode_mean_beta  encode_mean_beta2  kmeans5  \n",
       "0        104.550467          0.635215           0.633070        0  \n",
       "1        104.729473          0.634261           0.632797        0  \n",
       "2        103.699696          0.632499           0.630289        0  \n",
       "3        104.727931          0.636051           0.633793        0  \n",
       "4        104.675748          0.633840           0.633266        0  \n",
       "...             ...               ...                ...      ...  \n",
       "428786    99.568403          0.961481           0.958937        4  \n",
       "428787    99.572674          0.962304           0.960865        4  \n",
       "428788    99.715361          0.959879           0.957927        4  \n",
       "428789    99.632305          0.961479           0.958523        4  \n",
       "428790    99.228970          0.961903           0.960295        4  \n",
       "\n",
       "[428791 rows x 182 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks = stocks[stocks['target_realized_volatility'] != 0].reset_index(drop=True)\n",
    "stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e07464cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stocks[stocks.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "68d1c9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>log_returns2_realized_volatility_300</th>\n",
       "      <th>log_returns2_weighted_volatility_300</th>\n",
       "      <th>...</th>\n",
       "      <th>beta2-300</th>\n",
       "      <th>target_mean_enc</th>\n",
       "      <th>spread_mean_enc</th>\n",
       "      <th>dom_mean_enc</th>\n",
       "      <th>target_std_enc</th>\n",
       "      <th>spread_std_enc</th>\n",
       "      <th>dom_std_enc</th>\n",
       "      <th>encode_mean_beta</th>\n",
       "      <th>encode_mean_beta2</th>\n",
       "      <th>kmeans5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778640</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>391.300538</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>104.550467</td>\n",
       "      <td>0.635215</td>\n",
       "      <td>0.633070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>391.419207</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>104.729473</td>\n",
       "      <td>0.634261</td>\n",
       "      <td>0.632797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>...</td>\n",
       "      <td>1.519335</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>390.605604</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>103.699696</td>\n",
       "      <td>0.632499</td>\n",
       "      <td>0.630289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>2.378868</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>391.181654</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>104.727931</td>\n",
       "      <td>0.636051</td>\n",
       "      <td>0.633793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973303</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>391.062991</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>104.675748</td>\n",
       "      <td>0.633840</td>\n",
       "      <td>0.633266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049953</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.917628</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.568403</td>\n",
       "      <td>0.961481</td>\n",
       "      <td>0.958937</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160123</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.850800</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>99.572674</td>\n",
       "      <td>0.962304</td>\n",
       "      <td>0.960865</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046489</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>383.053325</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>99.715361</td>\n",
       "      <td>0.959879</td>\n",
       "      <td>0.957927</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.315896</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.605401</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.632305</td>\n",
       "      <td>0.961479</td>\n",
       "      <td>0.958523</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.130582</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>382.545423</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.228970</td>\n",
       "      <td>0.961903</td>\n",
       "      <td>0.960295</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wap_mean_300  wap_std_300  wap2_mean_300  wap2_std_300  \\\n",
       "0         194.495455     0.164908     194.479014      0.196558   \n",
       "1         199.598260     0.031047     199.597336      0.036259   \n",
       "2         209.021831     0.092861     209.053034      0.098250   \n",
       "3         216.281256     0.183025     216.198136      0.164985   \n",
       "4         214.542788     0.051133     214.524415      0.071793   \n",
       "...              ...          ...            ...           ...   \n",
       "428786    306.672174     0.163919     306.730549      0.206509   \n",
       "428787    291.124934     0.147318     291.137380      0.164838   \n",
       "428788    202.972820     0.064758     202.958539      0.080424   \n",
       "428789    152.478929     0.068413     152.480008      0.075165   \n",
       "428790    194.982143     0.040268     195.012846      0.055999   \n",
       "\n",
       "        log_returns_realized_volatility_300  \\\n",
       "0                                  0.003394   \n",
       "1                                  0.000699   \n",
       "2                                  0.001983   \n",
       "3                                  0.001863   \n",
       "4                                  0.001131   \n",
       "...                                     ...   \n",
       "428786                             0.002284   \n",
       "428787                             0.002217   \n",
       "428788                             0.001386   \n",
       "428789                             0.002783   \n",
       "428790                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428786                             0.000132                2.117007e-10   \n",
       "428787                             0.000128                9.484441e-11   \n",
       "428788                             0.000080                3.031629e-11   \n",
       "428789                             0.000161                1.960849e-10   \n",
       "428790                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  log_returns2_realized_volatility_300  \\\n",
       "0               7.138250e-06                              0.005032   \n",
       "1               8.823633e-07                              0.001448   \n",
       "2               1.729093e-06                              0.003583   \n",
       "3              -5.516464e-06                              0.002422   \n",
       "4              -2.164288e-06                              0.002412   \n",
       "...                      ...                                   ...   \n",
       "428786         -2.858852e-06                              0.004503   \n",
       "428787          3.674218e-06                              0.003652   \n",
       "428788         -2.437732e-06                              0.002686   \n",
       "428789          6.737309e-06                              0.004316   \n",
       "428790         -1.059089e-06                              0.001784   \n",
       "\n",
       "        log_returns2_weighted_volatility_300  ...  beta2-300  target_mean_enc  \\\n",
       "0                                   0.000291  ...   0.778640         0.002964   \n",
       "1                                   0.000084  ...   0.450700         0.002976   \n",
       "2                                   0.000207  ...   1.519335         0.002974   \n",
       "3                                   0.000140  ...   2.378868         0.002975   \n",
       "4                                   0.000140  ...   0.973303         0.002974   \n",
       "...                                      ...  ...        ...              ...   \n",
       "428786                              0.000260  ...   1.049953         0.003899   \n",
       "428787                              0.000211  ...   2.160123         0.003897   \n",
       "428788                              0.000155  ...  -0.046489         0.003903   \n",
       "428789                              0.000250  ...   1.315896         0.003896   \n",
       "428790                              0.000103  ...  -2.130582         0.003894   \n",
       "\n",
       "        spread_mean_enc  dom_mean_enc  target_std_enc  spread_std_enc  \\\n",
       "0              0.001023    391.300538        0.002289        0.000925   \n",
       "1              0.001027    391.419207        0.002414        0.000949   \n",
       "2              0.001025    390.605604        0.002411        0.000946   \n",
       "3              0.001026    391.181654        0.002411        0.000947   \n",
       "4              0.001027    391.062991        0.002412        0.000948   \n",
       "...                 ...           ...             ...             ...   \n",
       "428786         0.001108    382.917628        0.002595        0.000784   \n",
       "428787         0.001108    382.850800        0.002590        0.000783   \n",
       "428788         0.001109    383.053325        0.002601        0.000786   \n",
       "428789         0.001108    382.605401        0.002593        0.000784   \n",
       "428790         0.001107    382.545423        0.002589        0.000784   \n",
       "\n",
       "        dom_std_enc  encode_mean_beta  encode_mean_beta2  kmeans5  \n",
       "0        104.550467          0.635215           0.633070        0  \n",
       "1        104.729473          0.634261           0.632797        0  \n",
       "2        103.699696          0.632499           0.630289        0  \n",
       "3        104.727931          0.636051           0.633793        0  \n",
       "4        104.675748          0.633840           0.633266        0  \n",
       "...             ...               ...                ...      ...  \n",
       "428786    99.568403          0.961481           0.958937        4  \n",
       "428787    99.572674          0.962304           0.960865        4  \n",
       "428788    99.715361          0.959879           0.957927        4  \n",
       "428789    99.632305          0.961479           0.958523        4  \n",
       "428790    99.228970          0.961903           0.960295        4  \n",
       "\n",
       "[428791 rows x 180 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocksX = stocks.drop(['time_id','target_realized_volatility'], axis=1)\n",
    "stocksX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3ab5236b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>log_returns2_realized_volatility_300</th>\n",
       "      <th>log_returns2_weighted_volatility_300</th>\n",
       "      <th>...</th>\n",
       "      <th>beta2-300</th>\n",
       "      <th>target_mean_enc</th>\n",
       "      <th>spread_mean_enc</th>\n",
       "      <th>dom_mean_enc</th>\n",
       "      <th>target_std_enc</th>\n",
       "      <th>spread_std_enc</th>\n",
       "      <th>dom_std_enc</th>\n",
       "      <th>encode_mean_beta</th>\n",
       "      <th>encode_mean_beta2</th>\n",
       "      <th>kmeans5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.116622</td>\n",
       "      <td>-0.073991</td>\n",
       "      <td>-0.116666</td>\n",
       "      <td>-0.029679</td>\n",
       "      <td>0.115178</td>\n",
       "      <td>0.115178</td>\n",
       "      <td>-0.025665</td>\n",
       "      <td>0.777948</td>\n",
       "      <td>0.209614</td>\n",
       "      <td>0.209614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045470</td>\n",
       "      <td>0.114352</td>\n",
       "      <td>0.921646</td>\n",
       "      <td>-0.183144</td>\n",
       "      <td>0.361328</td>\n",
       "      <td>1.626625</td>\n",
       "      <td>-0.196633</td>\n",
       "      <td>-0.157124</td>\n",
       "      <td>-0.157451</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.102802</td>\n",
       "      <td>-0.393314</td>\n",
       "      <td>-0.102803</td>\n",
       "      <td>-0.390362</td>\n",
       "      <td>-0.861672</td>\n",
       "      <td>-0.861672</td>\n",
       "      <td>-0.026308</td>\n",
       "      <td>0.104284</td>\n",
       "      <td>-0.714063</td>\n",
       "      <td>-0.714063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124573</td>\n",
       "      <td>0.127112</td>\n",
       "      <td>0.931338</td>\n",
       "      <td>-0.183137</td>\n",
       "      <td>0.562387</td>\n",
       "      <td>1.709131</td>\n",
       "      <td>-0.196617</td>\n",
       "      <td>-0.157617</td>\n",
       "      <td>-0.157589</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.077279</td>\n",
       "      <td>-0.245858</td>\n",
       "      <td>-0.077194</td>\n",
       "      <td>-0.250878</td>\n",
       "      <td>-0.396223</td>\n",
       "      <td>-0.396223</td>\n",
       "      <td>-0.026158</td>\n",
       "      <td>0.195464</td>\n",
       "      <td>-0.163976</td>\n",
       "      <td>-0.163976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133196</td>\n",
       "      <td>0.125187</td>\n",
       "      <td>0.926293</td>\n",
       "      <td>-0.183186</td>\n",
       "      <td>0.557938</td>\n",
       "      <td>1.699614</td>\n",
       "      <td>-0.196709</td>\n",
       "      <td>-0.158527</td>\n",
       "      <td>-0.158860</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.057618</td>\n",
       "      <td>-0.030774</td>\n",
       "      <td>-0.057842</td>\n",
       "      <td>-0.100720</td>\n",
       "      <td>-0.439829</td>\n",
       "      <td>-0.439829</td>\n",
       "      <td>-0.026089</td>\n",
       "      <td>-0.584773</td>\n",
       "      <td>-0.463032</td>\n",
       "      <td>-0.463032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340528</td>\n",
       "      <td>0.126183</td>\n",
       "      <td>0.928879</td>\n",
       "      <td>-0.183151</td>\n",
       "      <td>0.558200</td>\n",
       "      <td>1.703427</td>\n",
       "      <td>-0.196617</td>\n",
       "      <td>-0.156693</td>\n",
       "      <td>-0.157084</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.062326</td>\n",
       "      <td>-0.345399</td>\n",
       "      <td>-0.062375</td>\n",
       "      <td>-0.310409</td>\n",
       "      <td>-0.705091</td>\n",
       "      <td>-0.705091</td>\n",
       "      <td>-0.026260</td>\n",
       "      <td>-0.223794</td>\n",
       "      <td>-0.465613</td>\n",
       "      <td>-0.465613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.125147</td>\n",
       "      <td>0.932751</td>\n",
       "      <td>-0.183158</td>\n",
       "      <td>0.558976</td>\n",
       "      <td>1.707109</td>\n",
       "      <td>-0.196621</td>\n",
       "      <td>-0.157834</td>\n",
       "      <td>-0.157351</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>0.187196</td>\n",
       "      <td>-0.076352</td>\n",
       "      <td>0.187356</td>\n",
       "      <td>-0.007289</td>\n",
       "      <td>-0.287284</td>\n",
       "      <td>-0.287284</td>\n",
       "      <td>-0.026008</td>\n",
       "      <td>-0.298588</td>\n",
       "      <td>0.073070</td>\n",
       "      <td>0.073070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019975</td>\n",
       "      <td>1.090489</td>\n",
       "      <td>1.137244</td>\n",
       "      <td>-0.183658</td>\n",
       "      <td>0.852915</td>\n",
       "      <td>1.137068</td>\n",
       "      <td>-0.197082</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>0.145088</td>\n",
       "      <td>-0.115953</td>\n",
       "      <td>0.145123</td>\n",
       "      <td>-0.101051</td>\n",
       "      <td>-0.311628</td>\n",
       "      <td>-0.311628</td>\n",
       "      <td>-0.026176</td>\n",
       "      <td>0.404924</td>\n",
       "      <td>-0.146166</td>\n",
       "      <td>-0.146166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287763</td>\n",
       "      <td>1.088662</td>\n",
       "      <td>1.136687</td>\n",
       "      <td>-0.183662</td>\n",
       "      <td>0.844337</td>\n",
       "      <td>1.133919</td>\n",
       "      <td>-0.197081</td>\n",
       "      <td>0.011807</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>-0.093662</td>\n",
       "      <td>-0.312896</td>\n",
       "      <td>-0.093700</td>\n",
       "      <td>-0.290989</td>\n",
       "      <td>-0.612571</td>\n",
       "      <td>-0.612571</td>\n",
       "      <td>-0.026268</td>\n",
       "      <td>-0.253240</td>\n",
       "      <td>-0.394997</td>\n",
       "      <td>-0.394997</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244502</td>\n",
       "      <td>1.094256</td>\n",
       "      <td>1.139253</td>\n",
       "      <td>-0.183650</td>\n",
       "      <td>0.862119</td>\n",
       "      <td>1.141944</td>\n",
       "      <td>-0.197069</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.007146</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>-0.230420</td>\n",
       "      <td>-0.304178</td>\n",
       "      <td>-0.230416</td>\n",
       "      <td>-0.302822</td>\n",
       "      <td>-0.106322</td>\n",
       "      <td>-0.106322</td>\n",
       "      <td>-0.026031</td>\n",
       "      <td>0.734773</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084124</td>\n",
       "      <td>1.086904</td>\n",
       "      <td>1.135989</td>\n",
       "      <td>-0.183677</td>\n",
       "      <td>0.849011</td>\n",
       "      <td>1.135628</td>\n",
       "      <td>-0.197076</td>\n",
       "      <td>0.011380</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>-0.115304</td>\n",
       "      <td>-0.371317</td>\n",
       "      <td>-0.115220</td>\n",
       "      <td>-0.345946</td>\n",
       "      <td>-0.556588</td>\n",
       "      <td>-0.556588</td>\n",
       "      <td>-0.026282</td>\n",
       "      <td>-0.104781</td>\n",
       "      <td>-0.627569</td>\n",
       "      <td>-0.627569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.747214</td>\n",
       "      <td>1.084767</td>\n",
       "      <td>1.134965</td>\n",
       "      <td>-0.183681</td>\n",
       "      <td>0.842662</td>\n",
       "      <td>1.134689</td>\n",
       "      <td>-0.197112</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wap_mean_300  wap_std_300  wap2_mean_300  wap2_std_300  \\\n",
       "0          -0.116622    -0.073991      -0.116666     -0.029679   \n",
       "1          -0.102802    -0.393314      -0.102803     -0.390362   \n",
       "2          -0.077279    -0.245858      -0.077194     -0.250878   \n",
       "3          -0.057618    -0.030774      -0.057842     -0.100720   \n",
       "4          -0.062326    -0.345399      -0.062375     -0.310409   \n",
       "...              ...          ...            ...           ...   \n",
       "428786      0.187196    -0.076352       0.187356     -0.007289   \n",
       "428787      0.145088    -0.115953       0.145123     -0.101051   \n",
       "428788     -0.093662    -0.312896      -0.093700     -0.290989   \n",
       "428789     -0.230420    -0.304178      -0.230416     -0.302822   \n",
       "428790     -0.115304    -0.371317      -0.115220     -0.345946   \n",
       "\n",
       "        log_returns_realized_volatility_300  \\\n",
       "0                                  0.115178   \n",
       "1                                 -0.861672   \n",
       "2                                 -0.396223   \n",
       "3                                 -0.439829   \n",
       "4                                 -0.705091   \n",
       "...                                     ...   \n",
       "428786                            -0.287284   \n",
       "428787                            -0.311628   \n",
       "428788                            -0.612571   \n",
       "428789                            -0.106322   \n",
       "428790                            -0.556588   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.115178                   -0.025665   \n",
       "1                                 -0.861672                   -0.026308   \n",
       "2                                 -0.396223                   -0.026158   \n",
       "3                                 -0.439829                   -0.026089   \n",
       "4                                 -0.705091                   -0.026260   \n",
       "...                                     ...                         ...   \n",
       "428786                            -0.287284                   -0.026008   \n",
       "428787                            -0.311628                   -0.026176   \n",
       "428788                            -0.612571                   -0.026268   \n",
       "428789                            -0.106322                   -0.026031   \n",
       "428790                            -0.556588                   -0.026282   \n",
       "\n",
       "        log_returns_mean_300  log_returns2_realized_volatility_300  \\\n",
       "0                   0.777948                              0.209614   \n",
       "1                   0.104284                             -0.714063   \n",
       "2                   0.195464                             -0.163976   \n",
       "3                  -0.584773                             -0.463032   \n",
       "4                  -0.223794                             -0.465613   \n",
       "...                      ...                                   ...   \n",
       "428786             -0.298588                              0.073070   \n",
       "428787              0.404924                             -0.146166   \n",
       "428788             -0.253240                             -0.394997   \n",
       "428789              0.734773                              0.025002   \n",
       "428790             -0.104781                             -0.627569   \n",
       "\n",
       "        log_returns2_weighted_volatility_300  ...  beta2-300  target_mean_enc  \\\n",
       "0                                   0.209614  ...  -0.045470         0.114352   \n",
       "1                                  -0.714063  ...  -0.124573         0.127112   \n",
       "2                                  -0.163976  ...   0.133196         0.125187   \n",
       "3                                  -0.463032  ...   0.340528         0.126183   \n",
       "4                                  -0.465613  ...   0.001486         0.125147   \n",
       "...                                      ...  ...        ...              ...   \n",
       "428786                              0.073070  ...   0.019975         1.090489   \n",
       "428787                             -0.146166  ...   0.287763         1.088662   \n",
       "428788                             -0.394997  ...  -0.244502         1.094256   \n",
       "428789                              0.025002  ...   0.084124         1.086904   \n",
       "428790                             -0.627569  ...  -0.747214         1.084767   \n",
       "\n",
       "        spread_mean_enc  dom_mean_enc  target_std_enc  spread_std_enc  \\\n",
       "0              0.921646     -0.183144        0.361328        1.626625   \n",
       "1              0.931338     -0.183137        0.562387        1.709131   \n",
       "2              0.926293     -0.183186        0.557938        1.699614   \n",
       "3              0.928879     -0.183151        0.558200        1.703427   \n",
       "4              0.932751     -0.183158        0.558976        1.707109   \n",
       "...                 ...           ...             ...             ...   \n",
       "428786         1.137244     -0.183658        0.852915        1.137068   \n",
       "428787         1.136687     -0.183662        0.844337        1.133919   \n",
       "428788         1.139253     -0.183650        0.862119        1.141944   \n",
       "428789         1.135989     -0.183677        0.849011        1.135628   \n",
       "428790         1.134965     -0.183681        0.842662        1.134689   \n",
       "\n",
       "        dom_std_enc  encode_mean_beta  encode_mean_beta2   kmeans5  \n",
       "0         -0.196633         -0.157124          -0.157451 -1.287891  \n",
       "1         -0.196617         -0.157617          -0.157589 -1.287891  \n",
       "2         -0.196709         -0.158527          -0.158860 -1.287891  \n",
       "3         -0.196617         -0.156693          -0.157084 -1.287891  \n",
       "4         -0.196621         -0.157834          -0.157351 -1.287891  \n",
       "...             ...               ...                ...       ...  \n",
       "428786    -0.197082          0.011382           0.007657  1.359310  \n",
       "428787    -0.197081          0.011807           0.008634  1.359310  \n",
       "428788    -0.197069          0.010554           0.007146  1.359310  \n",
       "428789    -0.197076          0.011380           0.007447  1.359310  \n",
       "428790    -0.197112          0.011600           0.008345  1.359310  \n",
       "\n",
       "[428791 rows x 180 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocksX[:] = StandardScaler().fit(stocksX).transform(stocksX)\n",
    "stocksX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ca516b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3830"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocksY = stocks[['target_realized_volatility', 'stock_id', 'time_id']]\n",
    "len(stocksY.time_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "201b258a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002954</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000981</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001295</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001776</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001520</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>0.002899</td>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>0.002792</td>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>0.002379</td>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>0.001414</td>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  stock_id  time_id\n",
       "0                         0.002954         0        5\n",
       "1                         0.000981         0       11\n",
       "2                         0.001295         0       16\n",
       "3                         0.001776         0       31\n",
       "4                         0.001520         0       62\n",
       "...                            ...       ...      ...\n",
       "428786                    0.002899       126    32751\n",
       "428787                    0.003454       126    32753\n",
       "428788                    0.002792       126    32758\n",
       "428789                    0.002379       126    32763\n",
       "428790                    0.001414       126    32767\n",
       "\n",
       "[428791 rows x 3 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocksY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ea99df2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_realized_volatility              1.000000\n",
       "log_returns_weighted_volatility_300     0.904700\n",
       "log_returns_realized_volatility_300     0.904700\n",
       "log_returns_weighted_volatility_150     0.896073\n",
       "log_returns_realized_volatility_150     0.896073\n",
       "log_returns2_weighted_volatility_300    0.889992\n",
       "log_returns2_realized_volatility_300    0.889992\n",
       "log_returns2_weighted_volatility_150    0.882288\n",
       "log_returns2_realized_volatility_150    0.882288\n",
       "log_returns_weighted_volatility_75      0.864377\n",
       "log_returns_realized_volatility_75      0.864377\n",
       "log_returns2_weighted_volatility_75     0.851997\n",
       "log_returns2_realized_volatility_75     0.851997\n",
       "price_diff_amax_150                     0.826341\n",
       "price_diff_amax_75                      0.822470\n",
       "price_diff2_amax_150                    0.812327\n",
       "price_diff_amax_300                     0.808808\n",
       "price_diff2_amax_75                     0.808346\n",
       "price_diff2_amax_300                    0.791596\n",
       "bid_ask_spread1_mean_300                0.786301\n",
       "Name: target_realized_volatility, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks.corr()['target_realized_volatility'].nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854343e",
   "metadata": {},
   "source": [
    "### PolyRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3c0ef4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.15178472e-01,  1.15178472e-01, ...,\n",
       "        -3.39332584e-04, -1.80268760e-04, -9.57668889e-05],\n",
       "       [ 1.00000000e+00, -8.61672011e-01, -8.61672011e-01, ...,\n",
       "        -8.73022721e-04, -2.91421509e-04, -9.72786780e-05],\n",
       "       [ 1.00000000e+00, -3.96223010e-01, -3.96223010e-01, ...,\n",
       "        -2.78156807e-04, -1.66880027e-04, -1.00119583e-04],\n",
       "       ...,\n",
       "       [ 1.00000000e+00, -6.12570671e-01, -6.12570671e-01, ...,\n",
       "         2.90446383e-07, -6.16735692e-09,  1.30958048e-10],\n",
       "       [ 1.00000000e+00, -1.06322387e-01, -1.06322387e-01, ...,\n",
       "         2.07317784e-07, -6.29089831e-09,  1.90892459e-10],\n",
       "       [ 1.00000000e+00, -5.56587551e-01, -5.56587551e-01, ...,\n",
       "         6.19762228e-08, -3.60767623e-09,  2.10005179e-10]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "EPSILON = 0\n",
    "feats = ['log_returns_realized_volatility_300','log_returns_weighted_volatility_300',\n",
    "         'beta-300','ofi_mean_300', 'dom_imbalance_mean_150', 'encode_mean_beta']\n",
    "\n",
    "x = stocksX[feats]\n",
    "deg = 5\n",
    "\n",
    "poly = PolynomialFeatures(degree=deg)\n",
    "poly_stocks_X = poly.fit_transform(x)\n",
    "poly_stocks_y = stocksY.reset_index(drop=True)['target_realized_volatility'].values.flatten()\n",
    "\n",
    "weights = 1/np.square(poly_stocks_y+EPSILON)\n",
    "poly_stocks_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7753ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "Outer Fold : 1\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2676\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2745\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.289\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2862\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2821\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 2\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2777\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2715\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2666\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2786\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2646\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 3\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2778\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2845\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2674\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2741\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2851\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 4\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.3358\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2745\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2717\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.281\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2941\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 5\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2766\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2845\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2743\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2717\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2657\n",
      "\t**********************************************************************\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "poly_test = pd.DataFrame(\n",
    "    {'target_realized_volatility':[],'predicted_volatility_poly': [],'time_id':[], 'stock_id':[]}\n",
    ")\n",
    "\n",
    "poly_models = []\n",
    "train_scores = []\n",
    "\n",
    "inner_k = 5\n",
    "outer_k = 5\n",
    "\n",
    "#params.update(search.best_trial.params)\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "outer_kfold = KFold(n_splits=outer_k, random_state=42, shuffle=True)\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_kfold.split(x, stocksY)):\n",
    "    print('*'*120)\n",
    "    print(\"Outer Fold :\", outer_fold + 1)\n",
    "    print('*'*120)\n",
    "\n",
    "    X_outer_train = x.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    X_outer_test = x.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    y_outer_train = stocksY.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    y_outer_test = stocksY.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    \n",
    "    target = np.zeros(len(y_outer_test))\n",
    "\n",
    "    inner_scores = 0.0\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    inner_kfold = KFold(n_splits= inner_k, random_state=42, shuffle=True)\n",
    "    for inner_fold, (inner_train_idx, inner_valid_idx) in enumerate(inner_kfold.split(X_outer_train, y_outer_train)):\n",
    "        print(\"\\n\\t\"+\"*\"*20)\n",
    "        print(f\"\\t*  Inner Fold : {inner_fold + 1}  *\")\n",
    "        print(\"\\t\"+\"*\"*20+\"\\n\")\n",
    "    \n",
    "        # inner train data and valid data\n",
    "        X_inner_train = X_outer_train.iloc[inner_train_idx].reset_index(drop=True)\n",
    "        X_inner_valid = X_outer_train.iloc[inner_valid_idx].reset_index(drop=True)\n",
    "\n",
    "        y_inner_train = y_outer_train.iloc[\n",
    "            inner_train_idx].reset_index(drop=True)['target_realized_volatility'].values.flatten()\n",
    "        y_inner_valid = y_outer_train.iloc[\n",
    "            inner_valid_idx].reset_index(drop=True)['target_realized_volatility'].values.flatten()\n",
    "            \n",
    "        weights_train = 1/np.square(y_inner_train+EPSILON)\n",
    "\n",
    "        poly = PolynomialFeatures(degree=deg)\n",
    "        poly_train_X = poly.fit_transform(X_inner_train)\n",
    "        poly_valid_X = poly.fit_transform(X_inner_valid)\n",
    "        \n",
    "        model.fit(poly_train_X, y_inner_train, sample_weight=weights_train)\n",
    "       \n",
    "        # validation \n",
    "        y_inner_pred = model.predict(poly_valid_X)\n",
    "        RMSPE = rmspe(\n",
    "            y_true=(y_inner_valid), \n",
    "            y_pred=(y_inner_pred), n=4\n",
    "        )\n",
    "        \n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "        print(f'\\tInner Validation RMSPE: \\t{RMSPE}')\n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "\n",
    "        # keep training validation score\n",
    "        inner_scores += RMSPE / inner_k\n",
    "        \n",
    "        models.append(model)\n",
    "    \n",
    "    # store all models for prediction in oof evaluation\n",
    "    poly_models.append(models)\n",
    "    train_scores.append(inner_scores)\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=deg)\n",
    "    poly_test_X = poly.fit_transform(X_outer_test)\n",
    "    \n",
    "    # out of fold test set\n",
    "    for model in poly_models[outer_fold]:\n",
    "        y_outer_pred = model.predict(poly_test_X)\n",
    "        target += y_outer_pred / len(poly_models[outer_fold])\n",
    "   \n",
    "    # out of fold test set 600\n",
    "    y_outer_test = y_outer_test.assign(predicted_volatility_poly = target)\n",
    "    \n",
    "    poly_test = pd.concat([poly_test, y_outer_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bafbb",
   "metadata": {},
   "source": [
    "### Training Validation Result with 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7aaa77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result RMSPE-poly: 0.279088\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Result RMSPE-poly: {np.mean(train_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e455162f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27988, 0.27180000000000004, 0.27778, 0.29142, 0.27455999999999997]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "687eff66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.278368"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(poly_test['target_realized_volatility'], poly_test['predicted_volatility_poly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3e476",
   "metadata": {},
   "source": [
    "# Merging Prediction Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3144d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving test tables\n",
    "norm_test.to_csv(\"norm_test.csv\")\n",
    "simple_test.to_csv(\"simple_test.csv\")\n",
    "extend_test.to_csv(\"extend_test.csv\")\n",
    "poly_test.to_csv(\"poly_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f1c6da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle dump to save model lists\n",
    "import pickle\n",
    "\n",
    "pickle.dump(norm_models, open(\"norm_models.pickle\",\"wb\"))\n",
    "pickle.dump(simple_models, open(\"simple_models.pickle\",\"wb\"))\n",
    "pickle.dump(extend_models, open(\"extend_models.pickle\",\"wb\"))\n",
    "pickle.dump(poly_models, open(\"poly_models.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c02acff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realized_volatility</th>\n",
       "      <th>norm_predict</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>simple_predict</th>\n",
       "      <th>extend_predict</th>\n",
       "      <th>poly_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.004512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428926</th>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>32712.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.003848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>0.010190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.002305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        realized_volatility  norm_predict  time_id  stock_id  simple_predict  \\\n",
       "0                  0.003397      0.002773    103.0       0.0        0.002806   \n",
       "1                  0.002993      0.004439    146.0       0.0        0.004295   \n",
       "2                  0.001094      0.001236    250.0       0.0        0.001244   \n",
       "3                  0.001711      0.001649    297.0       0.0        0.001679   \n",
       "4                  0.001197      0.002248    319.0       0.0        0.002181   \n",
       "...                     ...           ...      ...       ...             ...   \n",
       "428926             0.004120      0.002958  32712.0     126.0        0.002781   \n",
       "428927             0.003511      0.004066  32724.0     126.0        0.004173   \n",
       "428928             0.010431      0.010297  32746.0     126.0        0.010131   \n",
       "428929             0.001827      0.002151  32750.0     126.0        0.002191   \n",
       "428930             0.003454      0.002089  32753.0     126.0        0.002073   \n",
       "\n",
       "        extend_predict  poly_predict  \n",
       "0                  NaN      0.003289  \n",
       "1             0.004247      0.004512  \n",
       "2                  NaN      0.001410  \n",
       "3                  NaN      0.001702  \n",
       "4                  NaN      0.003186  \n",
       "...                ...           ...  \n",
       "428926             NaN      0.002591  \n",
       "428927        0.004358      0.003848  \n",
       "428928        0.009286      0.010190  \n",
       "428929        0.002251      0.002305  \n",
       "428930             NaN      0.001831  \n",
       "\n",
       "[428931 rows x 7 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make master table\n",
    "master = pd.merge(norm_test, simple_test, how=\"left\", on=[\"time_id\",\"stock_id\"])\n",
    "master = master.drop(\"target_realized_volatility_y\",axis=1)\n",
    "master = pd.merge(master, extend_test, how=\"left\", on=[\"time_id\",\"stock_id\"])\n",
    "master = master.drop([\"target_realized_volatility\"], axis=1)\n",
    "master = pd.merge(master, poly_test, how=\"left\", on=[\"time_id\",\"stock_id\"])\n",
    "master = master.drop(['target_realized_volatility'], axis=1)\n",
    "master = master.rename(columns={\n",
    "    \"target_realized_volatility_x\": \"realized_volatility\",\n",
    "    \"predicted_volatility_norm\": \"norm_predict\",\n",
    "    \"predicted_volatility_simple\": \"simple_predict\",\n",
    "    \"predicted_volatility_extend\": \"extend_predict\",\n",
    "    \"predicted_volatility_poly\": \"poly_predict\"}, errors=\"raise\")\n",
    "master.to_csv(\"master.csv\")\n",
    "master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17d675",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "960c16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.read_csv(\"master.csv\").drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "e131afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [\"norm_predict\", \"simple_predict\", \"extend_predict\", \"poly_predict\"]\n",
    "master['average'] = master[subset].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "7782ea52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.255601"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(master['realized_volatility'], master['average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0811c3",
   "metadata": {},
   "source": [
    "### Weighted Average\n",
    "\n",
    "* Merge and cover gaps with best performing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "d4706064",
   "metadata": {},
   "outputs": [],
   "source": [
    "master['extend_predict'] = np.where(master['extend_predict'].isna(), \n",
    "                                    master['simple_predict'], master['extend_predict'])\n",
    "master['extend_predict'] = np.where(master['extend_predict'].isna(), \n",
    "                                    master['norm_predict'], master['extend_predict'])\n",
    "master['simple_predict'] = np.where(master['simple_predict'].isna(), \n",
    "                                    master['norm_predict'], master['simple_predict'])\n",
    "master['poly_predict'] = np.where(master['poly_predict'].isna(), \n",
    "                                  master['norm_predict'], master['poly_predict'])\n",
    "\n",
    "#master[\"waverage\"] = norm * + simp * + ext * + poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "91fbca16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realized_volatility</th>\n",
       "      <th>norm_predict</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>simple_predict</th>\n",
       "      <th>extend_predict</th>\n",
       "      <th>poly_predict</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.004373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.001677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.002538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428926</th>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>32712.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.002777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.004111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.009976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.002224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.001998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        realized_volatility  norm_predict  time_id  stock_id  simple_predict  \\\n",
       "0                  0.003397      0.002773    103.0       0.0        0.002806   \n",
       "1                  0.002993      0.004439    146.0       0.0        0.004295   \n",
       "2                  0.001094      0.001236    250.0       0.0        0.001244   \n",
       "3                  0.001711      0.001649    297.0       0.0        0.001679   \n",
       "4                  0.001197      0.002248    319.0       0.0        0.002181   \n",
       "...                     ...           ...      ...       ...             ...   \n",
       "428926             0.004120      0.002958  32712.0     126.0        0.002781   \n",
       "428927             0.003511      0.004066  32724.0     126.0        0.004173   \n",
       "428928             0.010431      0.010297  32746.0     126.0        0.010131   \n",
       "428929             0.001827      0.002151  32750.0     126.0        0.002191   \n",
       "428930             0.003454      0.002089  32753.0     126.0        0.002073   \n",
       "\n",
       "        extend_predict  poly_predict   average  \n",
       "0             0.002806      0.003289  0.002956  \n",
       "1             0.004247      0.004512  0.004373  \n",
       "2             0.001244      0.001410  0.001297  \n",
       "3             0.001679      0.001702  0.001677  \n",
       "4             0.002181      0.003186  0.002538  \n",
       "...                ...           ...       ...  \n",
       "428926        0.002781      0.002591  0.002777  \n",
       "428927        0.004358      0.003848  0.004111  \n",
       "428928        0.009286      0.010190  0.009976  \n",
       "428929        0.002251      0.002305  0.002224  \n",
       "428930        0.002073      0.001831  0.001998  \n",
       "\n",
       "[428931 rows x 8 columns]"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "4f648538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_minimize(args):\n",
    "    norm = master['norm_predict']\n",
    "    simp = master['simple_predict']\n",
    "    ext = master['extend_predict']\n",
    "    poly = master['poly_predict']\n",
    "    #avg = master['average']\n",
    "    \n",
    "    w_norm, w_simp, w_ext, w_poly = args\n",
    "    \n",
    "    out = (w_norm * norm) + (w_simp * simp) + (w_ext * ext) + (w_poly * poly) #+ (w_avg * avg)\n",
    "    \n",
    "    return rmspe(master['realized_volatility'], out)\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "start_weights = [.25, .25, .25, .25]#, .2]\n",
    "\n",
    "res = minimize(rmspe_minimize, start_weights, method='Nelder-Mead', tol=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "8a76cca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0541321 ,  0.9553657 ,  0.00432063, -0.02161786])"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "3ade6904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.252502"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = master['norm_predict']\n",
    "simp = master['simple_predict']\n",
    "ext = master['extend_predict']\n",
    "poly = master['poly_predict']\n",
    "avg = master['average']\n",
    "\n",
    "master['wav'] = norm*res.x[0] + simp*res.x[1] + ext*res.x[2] + poly*res.x[3] #+ avg*res.x[4]\n",
    "\n",
    "rmspe(master['realized_volatility'], master['wav'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "465e0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master.rename(columns={'wav': 'ensemble-weighted', \"average\": \"ensemble-average\"})\n",
    "master.to_csv(\"final-ensemble.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a338400",
   "metadata": {},
   "source": [
    "## RMSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "d378f3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result RMSPE-ensemble: 0.252502\n"
     ]
    }
   ],
   "source": [
    "RMSPE = rmspe(y_true=master['realized_volatility'], y_pred=master['ensemble-weighted'])\n",
    "\n",
    "print(f'Test Result RMSPE-ensemble: {RMSPE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6406ea",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "fad9d2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result MAPE-ensemble: 0.18400463210667212\n"
     ]
    }
   ],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true+1e-10)))\n",
    "\n",
    "print(f\"Test Result MAPE-ensemble: {mape(master['realized_volatility'], master['ensemble-weighted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "1cf7dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_ens(test):\n",
    "    return rmspe(test['realized_volatility'], test['ensemble-weighted'])\n",
    "def rmspe_avg(test):\n",
    "    return rmspe(test['realized_volatility'], test['ensemble-average'])\n",
    "def rmspe_poly(test):\n",
    "    return rmspe(test['realized_volatility'], test['poly_predict'])\n",
    "def rmspe_ext(test):\n",
    "    return rmspe(test['realized_volatility'], test['extend_predict'])\n",
    "def rmspe_simp(test):\n",
    "    return rmspe(test['realized_volatility'], test['simple_predict'])\n",
    "def rmspe_norm(test):\n",
    "    return rmspe(test['realized_volatility'], test['norm_predict'])\n",
    "\n",
    "master['rmspe-weighted'] = master.apply(rmspe_ens, axis=1)\n",
    "master['rmspe-average'] = master.apply(rmspe_avg, axis=1)\n",
    "master['rmspe-poly'] = master.apply(rmspe_poly, axis=1)\n",
    "master['rmspe-ext'] = master.apply(rmspe_ext, axis=1)\n",
    "master['rmspe-simp'] = master.apply(rmspe_simp, axis=1)\n",
    "master['rmspe-norm'] = master.apply(rmspe_norm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "5fe69657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master = master.rename(columns={\n",
    "#     \"norm_predict\": \"lgbm-norm\",\n",
    "#     \"simple_predict\": \"lgbm-simp\",\n",
    "#     \"extend_predict\": \"lgbm-ext\",\n",
    "#     \"poly_predict\": \"poly-reg\"}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "ed3812c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realized_volatility</th>\n",
       "      <th>lgbm-norm</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>lgbm-simp</th>\n",
       "      <th>lgbm-ext</th>\n",
       "      <th>poly-reg</th>\n",
       "      <th>ensemble-average</th>\n",
       "      <th>ensemble-weighted</th>\n",
       "      <th>rmspe-weighted</th>\n",
       "      <th>rmspe-average</th>\n",
       "      <th>rmspe-poly</th>\n",
       "      <th>rmspe-ext</th>\n",
       "      <th>rmspe-simp</th>\n",
       "      <th>rmspe-norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.183877</td>\n",
       "      <td>0.129677</td>\n",
       "      <td>0.031670</td>\n",
       "      <td>0.173835</td>\n",
       "      <td>0.173835</td>\n",
       "      <td>0.183526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.424850</td>\n",
       "      <td>0.461346</td>\n",
       "      <td>0.507842</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.435070</td>\n",
       "      <td>0.483372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.124998</td>\n",
       "      <td>0.185491</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.137539</td>\n",
       "      <td>0.137539</td>\n",
       "      <td>0.130152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.027278</td>\n",
       "      <td>0.019846</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.018368</td>\n",
       "      <td>0.018368</td>\n",
       "      <td>0.036184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.792124</td>\n",
       "      <td>1.120070</td>\n",
       "      <td>1.661073</td>\n",
       "      <td>0.821435</td>\n",
       "      <td>0.821435</td>\n",
       "      <td>0.877701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428926</th>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>32712.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.326850</td>\n",
       "      <td>0.326010</td>\n",
       "      <td>0.371161</td>\n",
       "      <td>0.324909</td>\n",
       "      <td>0.324909</td>\n",
       "      <td>0.281960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.179623</td>\n",
       "      <td>0.170816</td>\n",
       "      <td>0.095997</td>\n",
       "      <td>0.241002</td>\n",
       "      <td>0.188311</td>\n",
       "      <td>0.157955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.009976</td>\n",
       "      <td>0.010056</td>\n",
       "      <td>0.035947</td>\n",
       "      <td>0.043606</td>\n",
       "      <td>0.023060</td>\n",
       "      <td>0.109770</td>\n",
       "      <td>0.028760</td>\n",
       "      <td>0.012832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.187058</td>\n",
       "      <td>0.217298</td>\n",
       "      <td>0.261440</td>\n",
       "      <td>0.232021</td>\n",
       "      <td>0.198802</td>\n",
       "      <td>0.176930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.402598</td>\n",
       "      <td>0.421597</td>\n",
       "      <td>0.469833</td>\n",
       "      <td>0.399670</td>\n",
       "      <td>0.399670</td>\n",
       "      <td>0.395287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        realized_volatility  lgbm-norm  time_id  stock_id  lgbm-simp  \\\n",
       "0                  0.003397   0.002773    103.0       0.0   0.002806   \n",
       "1                  0.002993   0.004439    146.0       0.0   0.004295   \n",
       "2                  0.001094   0.001236    250.0       0.0   0.001244   \n",
       "3                  0.001711   0.001649    297.0       0.0   0.001679   \n",
       "4                  0.001197   0.002248    319.0       0.0   0.002181   \n",
       "...                     ...        ...      ...       ...        ...   \n",
       "428926             0.004120   0.002958  32712.0     126.0   0.002781   \n",
       "428927             0.003511   0.004066  32724.0     126.0   0.004173   \n",
       "428928             0.010431   0.010297  32746.0     126.0   0.010131   \n",
       "428929             0.001827   0.002151  32750.0     126.0   0.002191   \n",
       "428930             0.003454   0.002089  32753.0     126.0   0.002073   \n",
       "\n",
       "        lgbm-ext  poly-reg  ensemble-average  ensemble-weighted  \\\n",
       "0       0.002806  0.003289          0.002956           0.002772   \n",
       "1       0.004247  0.004512          0.004373           0.004264   \n",
       "2       0.001244  0.001410          0.001297           0.001230   \n",
       "3       0.001679  0.001702          0.001677           0.001664   \n",
       "4       0.002181  0.003186          0.002538           0.002146   \n",
       "...          ...       ...               ...                ...   \n",
       "428926  0.002781  0.002591          0.002777           0.002773   \n",
       "428927  0.004358  0.003848          0.004111           0.004142   \n",
       "428928  0.009286  0.010190          0.009976           0.010056   \n",
       "428929  0.002251  0.002305          0.002224           0.002169   \n",
       "428930  0.002073  0.001831          0.001998           0.002063   \n",
       "\n",
       "        rmspe-weighted  rmspe-average  rmspe-poly  rmspe-ext  rmspe-simp  \\\n",
       "0             0.183877       0.129677    0.031670   0.173835    0.173835   \n",
       "1             0.424850       0.461346    0.507842   0.419100    0.435070   \n",
       "2             0.124998       0.185491    0.288782   0.137539    0.137539   \n",
       "3             0.027278       0.019846    0.004986   0.018368    0.018368   \n",
       "4             0.792124       1.120070    1.661073   0.821435    0.821435   \n",
       "...                ...            ...         ...        ...         ...   \n",
       "428926        0.326850       0.326010    0.371161   0.324909    0.324909   \n",
       "428927        0.179623       0.170816    0.095997   0.241002    0.188311   \n",
       "428928        0.035947       0.043606    0.023060   0.109770    0.028760   \n",
       "428929        0.187058       0.217298    0.261440   0.232021    0.198802   \n",
       "428930        0.402598       0.421597    0.469833   0.399670    0.399670   \n",
       "\n",
       "        rmspe-norm  \n",
       "0         0.183526  \n",
       "1         0.483372  \n",
       "2         0.130152  \n",
       "3         0.036184  \n",
       "4         0.877701  \n",
       "...            ...  \n",
       "428926    0.281960  \n",
       "428927    0.157955  \n",
       "428928    0.012832  \n",
       "428929    0.176930  \n",
       "428930    0.395287  \n",
       "\n",
       "[428931 rows x 15 columns]"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "67f5d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "master.to_csv(\"final-ensemble.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b5464",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "1a0d5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "597f757f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=master[[\"rmspe-weighted\",'rmspe-average','rmspe-poly',\n",
    "                            'rmspe-ext', 'rmspe-simp', 'rmspe-norm']], orient=\"h\", fliersize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "ee1ef875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.violinplot(data=master[[\"rmspe-weighted\",'rmspe-average','rmspe-poly',\n",
    "                            'rmspe-ext', 'rmspe-simp', 'rmspe-norm']], orient=\"h\", fliersize=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "f66a1f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=master[[\"rmspe-weighted\",'rmspe-average','rmspe-poly',\n",
    "                            'rmspe-ext', 'rmspe-simp', 'rmspe-norm']], orient=\"h\", showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "38449085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='rmspe-weighted', ylabel='Count'>"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(master.groupby(\"stock_id\")['rmspe-weighted'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "55c5c086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='rmspe-norm', ylabel='Count'>"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(master.groupby(\"stock_id\")['rmspe-norm'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "0f87469a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='rmspe-ext', ylabel='Count'>"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(master.groupby(\"stock_id\")['rmspe-ext'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "c710a90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='rmspe-simp', ylabel='Count'>"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(master.groupby(\"stock_id\")['rmspe-simp'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "1116829b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='rmspe-poly', ylabel='Count'>"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(master.groupby(\"stock_id\")['rmspe-poly'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "f0a61f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='rmspe-average', ylabel='Count'>"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(master.groupby(\"stock_id\")['rmspe-average'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "cde37f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realized_volatility</th>\n",
       "      <th>lgbm-norm</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>lgbm-simp</th>\n",
       "      <th>lgbm-ext</th>\n",
       "      <th>poly-reg</th>\n",
       "      <th>ensemble-average</th>\n",
       "      <th>ensemble-weighted</th>\n",
       "      <th>rmspe-weighted</th>\n",
       "      <th>rmspe-average</th>\n",
       "      <th>rmspe-poly</th>\n",
       "      <th>rmspe-ext</th>\n",
       "      <th>rmspe-simp</th>\n",
       "      <th>rmspe-norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "      <td>428931.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>16038.958725</td>\n",
       "      <td>62.438003</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.184005</td>\n",
       "      <td>0.184780</td>\n",
       "      <td>0.200012</td>\n",
       "      <td>0.185046</td>\n",
       "      <td>0.183049</td>\n",
       "      <td>0.184048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>9365.110137</td>\n",
       "      <td>37.126473</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.172916</td>\n",
       "      <td>0.176602</td>\n",
       "      <td>0.194627</td>\n",
       "      <td>0.178876</td>\n",
       "      <td>0.174138</td>\n",
       "      <td>0.177311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>-0.030429</td>\n",
       "      <td>-0.005941</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>7854.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.072032</td>\n",
       "      <td>0.072655</td>\n",
       "      <td>0.080287</td>\n",
       "      <td>0.071882</td>\n",
       "      <td>0.070891</td>\n",
       "      <td>0.071555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>15853.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.151556</td>\n",
       "      <td>0.151956</td>\n",
       "      <td>0.166148</td>\n",
       "      <td>0.151532</td>\n",
       "      <td>0.149697</td>\n",
       "      <td>0.150399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.003475</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>23994.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.003095</td>\n",
       "      <td>0.256110</td>\n",
       "      <td>0.256159</td>\n",
       "      <td>0.276297</td>\n",
       "      <td>0.256437</td>\n",
       "      <td>0.254186</td>\n",
       "      <td>0.255512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.065007</td>\n",
       "      <td>0.015133</td>\n",
       "      <td>32767.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.014701</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.314321</td>\n",
       "      <td>0.113946</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>19.262388</td>\n",
       "      <td>21.059015</td>\n",
       "      <td>20.723003</td>\n",
       "      <td>25.023422</td>\n",
       "      <td>19.025330</td>\n",
       "      <td>27.487513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       realized_volatility      lgbm-norm        time_id       stock_id  \\\n",
       "count        428931.000000  428931.000000  428931.000000  428931.000000   \n",
       "mean              0.002854       0.002537   16038.958725      62.438003   \n",
       "std               0.002358       0.001872    9365.110137      37.126473   \n",
       "min               0.000021       0.000150       5.000000       0.000000   \n",
       "25%               0.001403       0.001315    7854.000000      30.000000   \n",
       "50%               0.002160       0.001962   15853.000000      63.000000   \n",
       "75%               0.003475       0.003118   23994.000000      96.000000   \n",
       "max               0.065007       0.015133   32767.000000     126.000000   \n",
       "\n",
       "           lgbm-simp       lgbm-ext       poly-reg  ensemble-average  \\\n",
       "count  428931.000000  428931.000000  428931.000000     428931.000000   \n",
       "mean        0.002539       0.002542       0.002463          0.002517   \n",
       "std         0.001869       0.001870       0.001862          0.001844   \n",
       "min         0.000150       0.000150      -0.030429         -0.005941   \n",
       "25%         0.001316       0.001316       0.001301          0.001314   \n",
       "50%         0.001963       0.001965       0.001924          0.001953   \n",
       "75%         0.003117       0.003114       0.003008          0.003086   \n",
       "max         0.014701       0.014831       0.314321          0.113946   \n",
       "\n",
       "       ensemble-weighted  rmspe-weighted  rmspe-average     rmspe-poly  \\\n",
       "count      428931.000000   428931.000000  428931.000000  428931.000000   \n",
       "mean            0.002521        0.184005       0.184780       0.200012   \n",
       "std             0.001857        0.172916       0.176602       0.194627   \n",
       "min             0.000149        0.000000       0.000001       0.000000   \n",
       "25%             0.001306        0.072032       0.072655       0.080287   \n",
       "50%             0.001948        0.151556       0.151956       0.166148   \n",
       "75%             0.003095        0.256110       0.256159       0.276297   \n",
       "max             0.014593       19.262388      21.059015      20.723003   \n",
       "\n",
       "           rmspe-ext     rmspe-simp     rmspe-norm  \n",
       "count  428931.000000  428931.000000  428931.000000  \n",
       "mean        0.185046       0.183049       0.184048  \n",
       "std         0.178876       0.174138       0.177311  \n",
       "min         0.000001       0.000003       0.000000  \n",
       "25%         0.071882       0.070891       0.071555  \n",
       "50%         0.151532       0.149697       0.150399  \n",
       "75%         0.256437       0.254186       0.255512  \n",
       "max        25.023422      19.025330      27.487513  "
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "359c6c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmspe-weighted    11066\n",
       "rmspe-average     11565\n",
       "rmspe-poly        11240\n",
       "rmspe-ext         11720\n",
       "rmspe-simp        11567\n",
       "rmspe-norm        11667\n",
       "dtype: int64"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = master.drop(master.iloc[:,:9],axis=1)\n",
    "\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "\n",
    "IQR = Q3-Q1\n",
    "outlier = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()\n",
    "outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5f5a9",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "f2c7b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n",
    "df[mask] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "2b733dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x25015af7070>"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 471.75x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.relplot(data=df[['rmspe-norm','rmspe-ext', 'rmspe-simp', 'rmspe-average']])#, showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073849d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a423ee9",
   "metadata": {},
   "source": [
    "### Write Predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8968d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.astype({'time_id': int, 'stock_id': int})\n",
    "test.to_csv(\"extend3.csv\", index=False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe34fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"extend3.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d23168",
   "metadata": {},
   "source": [
    "## Model Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_importance(importance, title='', save_to_file=None, top=None):    \n",
    "    importance = importance.sort_values(\n",
    "        ['Importance'], ascending=False\n",
    "    )[:top]#.sort_values(['Importance'])\n",
    "    sns.set(font_scale=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    #importance.plot.barh(ax=ax)\n",
    "    sns.barplot(x=\"Importance\", y=\"Features\", data=importance.sort_values(by=\"Importance\",ascending=False))\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_to_file:\n",
    "        plt.savefig(save_to_file)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def compute_mean_importance(importance):\n",
    "    res = importance[0].copy()\n",
    "    res['Importance'] = np.mean(np.array(\n",
    "        [df['importance'].values for df in importance]\n",
    "    ), axis=0)\n",
    "    \n",
    "    res = res.drop(['importance'], axis=1)\n",
    "    \n",
    "    # reformat for plot\n",
    "    return pd.DataFrame(\n",
    "        {'Features':[f for f in res.index], \n",
    "         'Importance': res['Importance']}\n",
    "    ).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a3528",
   "metadata": {},
   "source": [
    "### Plotting Importance by Gain & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c47ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_gain = compute_mean_importance(gain_importance)\n",
    "\n",
    "plot_importance(mean_gain, title='Extend-3 Model: Top 40 Features by Gain', top=40,\n",
    "               save_to_file=\"importance_by_gain_extend3.png\")\n",
    "\n",
    "mean_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41abbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gain = compute_mean_importance(split_importance)\n",
    "\n",
    "plot_importance(mean_gain, title='Extend-3 Model: Top 40 Features by Split', top=40,\n",
    "               save_to_file=\"importance_by_split_extend3.png\")\n",
    "\n",
    "mean_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.plot_tree(models[0], tree_index = 0, figsize=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e31bf7",
   "metadata": {},
   "source": [
    "## Post-Hoc ANOVA tests\n",
    "https://statisticsbyjim.com/anova/post-hoc-tests-anova/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c655c89",
   "metadata": {},
   "source": [
    "Every time we t-test the same data there is chance for a Type I error (i.e. reject null hypothesis when it is true), which is often the set significance level of 0.05. If we conduct multiple t-tests, Type I errors are accumulative. For two t-tests the chance for Type I error ~15% as seen in table below. Thus, we use ANOVA and post-hoc tests to control Type I error to always be 0.05 even with multiple comparisons."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5b962994",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087f4b2",
   "metadata": {},
   "source": [
    "The experiment-wise error rate determines the probability of type I error (false positive) over a total family of comparisons. Our ANOVA example has 3 groups, which produces 3 comparisons and a family-wise error rate of 0.143. When performing statistical tests, we expect a false positive rate of 0.05 (our set significance level). As shown, when increasing groups from 2 to 3, the error rate is approximately tripled from 0.05 to 0.143. This introduces doubt as to whether we are observing a false positive as opposed to a significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3dec1a",
   "metadata": {},
   "source": [
    "If we use 2-sample t-tests to compare group means, and set a significance level of 0.05 for each test, then the number of comparisons will determine the experiment-wise error rate. In post hoc tests, we set an experiment-wise error rate for all comparisons, and the post hoc test calculates a significance level for individual comparisons to produce the specified familywise error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55754670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import tukey_hsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916a6c4",
   "metadata": {},
   "source": [
    "### ANOVA F-Test\n",
    "\n",
    "    Null: All group means are equal.\n",
    "    Alternative: Not all group means are equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "extend3 = test['rmspe-extend3']\n",
    "extend2 = test['rmspe-extend2']\n",
    "simple = test['rmspe-simple']\n",
    "\n",
    "f_oneway(extend3, extend2, simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759acf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.boxplot([extend3, extend2, simple], showfliers=False)\n",
    "ax.set_xticklabels([\"Extend-3\", \"Extend-2\", \"Simple\"]) \n",
    "ax.set_ylabel(\"mean\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4fe99",
   "metadata": {},
   "source": [
    "Since p-val < 0.05, we have sufficient evidence that the mean values across each group are not equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4d67f",
   "metadata": {},
   "source": [
    "### Tukey's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce119213",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tukey_hsd(extend3, extend2, simple)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea73bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey = pairwise_tukeyhsd(endog=df['score'],groups=df['group'],alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad60a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'rmspe':list(extend3)+list(extend2)+list(simple),\n",
    "                   'model':np.repeat(['extend3','extend2','simple'], repeats=len(extend3))})\n",
    "\n",
    "tukey = pairwise_tukeyhsd(endog=df['rmspe'],groups=df['model'],alpha=0.05)\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3627be7",
   "metadata": {},
   "source": [
    "* P-val for difference in means between extend2 and extend3: 0.9831\n",
    "* P-val for difference in means between extend2 and simple: ~0\n",
    "* P-val for difference in means between extend3 and simple: ~0\n",
    "\n",
    "Thus, we would conclude there is a statistically significant difference between means of extend3 and simple models and extend2 and simple models, but not a statistically significant difference between means of extend3 and extend2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a211fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_melt = pd.melt(test, value_vars=['rmspe-extend3', 'rmspe-extend2', 'rmspe-simple'])\n",
    "\n",
    "test_melt.columns = ['model', 'value']\n",
    "test_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "ax = sns.boxplot(x='model', y='value', data=test_melt, color='#99c2a2', showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822161cd",
   "metadata": {},
   "source": [
    "### Source: https://www.reneshbedre.com/blog/anova.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
