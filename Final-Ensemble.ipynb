{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39499a8",
   "metadata": {},
   "source": [
    "### Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c533be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "import matplotlib\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "pd.options.display.max_rows = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21b116",
   "metadata": {},
   "source": [
    "### Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ff35391",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = pd.read_feather(\"beta/betas_condensed.fth\")\n",
    "spread_dom = pd.read_csv(\"all_times.csv\").drop([\"beta\"],axis=1)\n",
    "DENORM = pd.read_csv(\"denormalize_multipliers.csv\")\n",
    "\n",
    "def mad(data):\n",
    "    return np.mean(np.absolute(data - np.mean(data)))\n",
    "\n",
    "def import_beta(df):\n",
    "    global beta\n",
    "    global spread_dom\n",
    "    \n",
    "    df = pd.merge(df, beta, how=\"left\")\n",
    "    df = pd.merge(df, spread_dom, how=\"left\")\n",
    "    return df\n",
    "    \n",
    "def compute_wap(df):\n",
    "    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \\\n",
    "          /(df['bid_size1'] + df['ask_size1'])\n",
    "\n",
    "def compute_wap2(df):\n",
    "    return (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \\\n",
    "           /(df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "def compute_dom(df):\n",
    "    dom = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2']) \\\n",
    "            + (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])\n",
    "    return dom\n",
    "\n",
    "def compute_dom_diff(df):\n",
    "    dom_diff = abs((df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2']) \\\n",
    "            - (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2']))\n",
    "    return dom_diff\n",
    "\n",
    "def fill_seconds(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    index_range = pd.Index(range(600), name='seconds_in_bucket')\n",
    "    df = df.set_index('seconds_in_bucket').reindex(index_range)\n",
    "    \n",
    "    # Forward fill & back fill seconds\n",
    "    df = df.ffill().reset_index()\n",
    "    return df.bfill().iloc[:600]\n",
    "\n",
    "def beta_encoding(df):\n",
    "    beta1 = np.repeat(np.nan, df.shape[0])\n",
    "    beta2 = np.repeat(np.nan, df.shape[0])\n",
    "    kf = KFold(n_splits = 112, shuffle=True,random_state = 0)\n",
    "    for idx_1, idx_2 in kf.split(df):\n",
    "        bmean1 = df.iloc[idx_1].groupby('stock_id')['beta'].mean()\n",
    "        bmean2 = df.iloc[idx_1].groupby('stock_id')['beta2'].mean()\n",
    "        beta1[idx_2] = df['stock_id'].iloc[idx_2].map(bmean1)\n",
    "        beta2[idx_2] = df['stock_id'].iloc[idx_2].map(bmean2)\n",
    "        \n",
    "    df['encode_mean_beta'] = beta1\n",
    "    df['encode_mean_beta2'] = beta2\n",
    "    \n",
    "    return df.drop(['beta','beta2'], axis=1)\n",
    "\n",
    "def import_clusters(df):\n",
    "    reduc_feats_label = pd.read_csv(\"pca10_clusts.csv\").iloc[:,1:]\n",
    "    bds_cluster_label = pd.read_csv(\"cluster_labels.csv\").iloc[:,1:]\n",
    "    som_cluster_label = pd.read_csv(\"SOM_clusters.csv\")\n",
    "    #feature_leakage = pd.read_csv(\"feats_to_cluster.csv\").iloc[:,1:]\n",
    "    \n",
    "    df = df.merge(reduc_feats_label, how=\"left\")\n",
    "    df = df.merge(bds_cluster_label, how=\"left\")\n",
    "    df = df.merge(som_cluster_label, how=\"left\")\n",
    "    #df = df.merge(feature_leakage, how=\"left\")\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def remove_clusters(df, n=13):\n",
    "    if \"bds_gmm6\" in df.columns:\n",
    "        return df.drop(df.columns[-n:], axis=1)\n",
    "    return df\n",
    "\n",
    "def target_encoding(df):\n",
    "    tenc = np.repeat(np.nan, df.shape[0])\n",
    "    senc = np.repeat(np.nan, df.shape[0])\n",
    "    denc = np.repeat(np.nan, df.shape[0])\n",
    "    tencs = np.repeat(np.nan, df.shape[0])\n",
    "    sencs = np.repeat(np.nan, df.shape[0])\n",
    "    dencs = np.repeat(np.nan, df.shape[0])\n",
    "    kf = KFold(n_splits = 112, shuffle=True,random_state = 0)\n",
    "    for idx_1, idx_2 in kf.split(df):\n",
    "        tmean = df.iloc[idx_1].groupby('stock_id')['target_realized_volatility'].mean()\n",
    "        smean = df.iloc[idx_1].groupby('stock_id')['spread'].mean()\n",
    "        dmean = df.iloc[idx_1].groupby('stock_id')['dom'].mean()\n",
    "        tmeans = df.iloc[idx_1].groupby('stock_id')['target_realized_volatility'].std()\n",
    "        smeans = df.iloc[idx_1].groupby('stock_id')['spread'].std()\n",
    "        dmeans = df.iloc[idx_1].groupby('stock_id')['dom'].std()\n",
    "        tenc[idx_2] = df['stock_id'].iloc[idx_2].map(tmean)\n",
    "        senc[idx_2] = df['stock_id'].iloc[idx_2].map(smean)\n",
    "        denc[idx_2] = df['stock_id'].iloc[idx_2].map(dmean)\n",
    "        tencs[idx_2] = df['stock_id'].iloc[idx_2].map(tmeans)\n",
    "        sencs[idx_2] = df['stock_id'].iloc[idx_2].map(smeans)\n",
    "        dencs[idx_2] = df['stock_id'].iloc[idx_2].map(dmeans)\n",
    "    df['target_mean_enc'] = tenc\n",
    "    df['spread_mean_enc'] = senc\n",
    "    df['dom_mean_enc'] = denc\n",
    "    df['target_std_enc'] = tencs\n",
    "    df['spread_std_enc'] = sencs\n",
    "    df['dom_std_enc'] = dencs\n",
    "    \n",
    "    return df.drop(['dom','spread'], axis=1)\n",
    "\n",
    "def generate_feature_seconds(features, df, feats, seconds=[300], overlap=1):\n",
    "    if overlap:\n",
    "        for second in seconds:\n",
    "            res = pd.DataFrame(\n",
    "                df.query(f'{seconds[0]-second} <= seconds_in_bucket < {seconds[0]}')\n",
    "                .groupby(['time_id']).agg(feats)\n",
    "            ).reset_index()\n",
    "            res.columns = ['_'.join(feat).rstrip('_') for feat in res.columns]\n",
    "            res = res.add_suffix('_' + str(second))\n",
    "            features = pd.merge(\n",
    "                features, res, \n",
    "                how='left', \n",
    "                left_on='time_id', \n",
    "                right_on=f'time_id_{second}'\n",
    "            )\n",
    "            features = features.drop([f'time_id_{second}'], axis=1)  \n",
    "    else:\n",
    "        for i, second in enumerate(seconds):\n",
    "            last_second = 0 if not i else seconds[i-1]\n",
    "            res = pd.DataFrame(\n",
    "                df.query(f'{last_second} <= seconds_in_bucket < {second}')\n",
    "                .groupby(['time_id']).agg(feats)\n",
    "            ).reset_index()\n",
    "            res.columns = ['_'.join(feat).rstrip('_') for feat in res.columns]\n",
    "            res = res.add_suffix('_' + str(second))\n",
    "            features = pd.merge(\n",
    "                features, res, \n",
    "                how='left', \n",
    "                left_on='time_id', \n",
    "                right_on=f'time_id_{second}'\n",
    "            )\n",
    "            features = features.drop([f'time_id_{second}'], axis=1)\n",
    "    return features\n",
    "\n",
    "def generate_target(df, target, second=300):\n",
    "    features = pd.DataFrame(\n",
    "        df.query(f'seconds_in_bucket >= {second}')\n",
    "        .groupby(['time_id']).agg(target)\n",
    "    ).reset_index()\n",
    "    \n",
    "    features.columns = ['_'.join(feat).rstrip('_') for feat in features.columns]\n",
    "    return features\n",
    "\n",
    "def calc_slope(df):\n",
    "    v0 = (df['bid_size1']+df['ask_size1'])/2\n",
    "    p0 = (df['bid_price1']+df['ask_price1'])/2\n",
    "    slope_bid = ((df['bid_size1']/v0)-1)/abs((df['bid_price1']/p0)-1)+(\n",
    "                (df['bid_size2']/df['bid_size1'])-1)/abs((df['bid_price2']/df['bid_price1'])-1)\n",
    "    slope_ask = ((df['ask_size1']/v0)-1)/abs((df['ask_price1']/p0)-1)+(\n",
    "                (df['ask_size2']/df['ask_size1'])-1)/abs((df['ask_price2']/df['ask_price1'])-1)\n",
    "    return (slope_bid+slope_ask)/2, abs(slope_bid-slope_ask)\n",
    "\n",
    "\n",
    "def calc_dispersion(df):\n",
    "    bspread = df['bid_diff']\n",
    "    aspread = df['ask_diff']\n",
    "    bmid = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price1']\n",
    "    bmid2 = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price2']\n",
    "    amid = df['ask_price1'] - (df['bid_price1'] + df['ask_price1'])/2\n",
    "    amid2 = df['ask_price2'] - (df['bid_price1'] + df['ask_price1'])/2\n",
    "    bdisp = (df['bid_size1']*bmid + df['bid_size2']*bspread)/(df['bid_size1']+df['bid_size2'])\n",
    "    bdisp2 = (df['bid_size1']*bmid + df['bid_size2']*bmid2)/(df['bid_size1']+df['bid_size2'])\n",
    "    adisp = (df['ask_size1']*amid + df['ask_size2']*aspread)/(df['ask_size1']+df['ask_size2'])      \n",
    "    adisp2 = (df['ask_size1']*amid + df['ask_size2']*amid2)/(df['ask_size1']+df['ask_size2'])\n",
    "    return (bdisp + adisp)/2, (bdisp2 + adisp2)/2\n",
    "\n",
    "def calc_price_impact(df):\n",
    "    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n",
    "    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n",
    "    return (df['ask_price1'] - ask)/df['ask_price1'], (df['bid_price1'] - bid)/df['bid_price1']\n",
    "\n",
    "\n",
    "def calc_ofi(df):\n",
    "    a = df['bid_size1']*np.where(df['bid_price1'].diff()>=0,1,0)\n",
    "    b = df['bid_size1'].shift()*np.where(df['bid_price1'].diff()<=0,1,0)\n",
    "    c = df['ask_size1']*np.where(df['ask_price1'].diff()<=0,1,0)\n",
    "    d = df['ask_size1'].shift()*np.where(df['ask_price1'].diff()>=0,1,0)\n",
    "    return a - b - c + d\n",
    "\n",
    "def calc_tt1(df):\n",
    "    p1 = df['ask_price1'] * df['ask_size1'] + df['bid_price1'] * df['bid_size1']\n",
    "    p2 = df['ask_price2'] * df['ask_size2'] + df['bid_price2'] * df['bid_size2']      \n",
    "    return p2 - p1 \n",
    "\n",
    "def log_returns(waps):\n",
    "    return np.log(waps).diff() \n",
    "\n",
    "def realized_volatility(log_returns):\n",
    "    return np.sqrt(np.sum(log_returns**2))\n",
    "\n",
    "def weighted_volatility(log_returns):\n",
    "    return np.sqrt(np.sum(log_returns**2)/log_returns.count())\n",
    "\n",
    "def quarticity(s):\n",
    "    return (s.count()/3) * np.sum(s**4)\n",
    "\n",
    "def denormalize(df):\n",
    "    global DENORM\n",
    "\n",
    "    df = df.merge(DENORM, how=\"left\")\n",
    "    df['ask_price1'] *= df['multiplier']\n",
    "    df['ask_price2'] *= df['multiplier']\n",
    "    df['bid_price1'] *= df['multiplier']\n",
    "    df['bid_price2'] *= df['multiplier']\n",
    "    \n",
    "    return df.drop(['multiplier'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b8b67",
   "metadata": {},
   "source": [
    "### Generate Features for First 5 Minutes of Each Stock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "359c0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(stock_id, df, extend, norm=False):\n",
    "    \n",
    "    if norm: df.groupby(\"time_id\").apply(fill_seconds)\n",
    "    \n",
    "    # compute waps for first and second ask/bids \n",
    "    df['wap'] = compute_wap(df)\n",
    "    df['wap2'] = compute_wap2(df)\n",
    "    \n",
    "    # compute log returns for realized volatility\n",
    "    df['log_returns'] = df.groupby('time_id')['wap'].apply(log_returns)\n",
    "    df['log_returns2'] = df.groupby('time_id')['wap2'].apply(log_returns)\n",
    "    \n",
    "    # compute difference in waps and price as feature\n",
    "    df['wap_diff'] = abs(df['wap'] - df['wap2'])\n",
    "    \n",
    "    # price difference regularized\n",
    "    df['price_diff'] = (df['ask_price1'] - df['bid_price1']) \\\n",
    "                        / ((df['ask_price1'] + df['bid_price1'])/2)\n",
    "    df['price_diff2'] = (df['ask_price2'] - df['bid_price2']) \\\n",
    "                        / ((df['ask_price2'] + df['bid_price2'])/2)\n",
    "    \n",
    "    # difference between first and second bid/ask price\n",
    "    df['bid_diff'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_diff'] = df['ask_price1'] - df['ask_price2']\n",
    "    \n",
    "    # sum of first and second ask sizes and bid sizes\n",
    "    df['bid_ask_volume'] = (df['ask_size1'] + df['ask_size2']) \\\n",
    "                        + (df['bid_size1'] + df['bid_size2'])\n",
    "    \n",
    "    # diff between ask sizes and bid sizes \n",
    "    df['bid_ask_volume_diff'] = abs((df['ask_size1'] + df['ask_size2']) \\\n",
    "                                - (df['bid_size1'] + df['bid_size2']))\n",
    "\n",
    "    # depth of market and difference\n",
    "    df['dom'] = compute_dom(df)\n",
    "    df['dom_diff'] = compute_dom_diff(df)\n",
    "    df['dom_imbalance'] = abs(df['dom'] - df['dom_diff'])\n",
    "    \n",
    "    # bid ask spread from lecture/lab\n",
    "    df[\"bid_ask_spread1\"] = df['ask_price1'] / df['bid_price1'] - 1\n",
    "    df[\"bid_ask_spread2\"] = df['ask_price2'] / df['bid_price2'] - 1\n",
    "    df['bid_ask_spread3'] = abs(df['bid_diff'] - df['ask_diff'])\n",
    "    \n",
    "    df[\"slope\"], _ = calc_slope(df)\n",
    "    df[\"dispersion\"], _ = calc_dispersion(df)\n",
    "    df[\"price_impact\"], _ = calc_price_impact(df)\n",
    "    df[\"ofi\"] = calc_ofi(df)\n",
    "    df[\"turn_over\"] = calc_tt1(df)   \n",
    "    \n",
    "    df['target'] =  df.groupby('time_id')['wap'].apply(log_returns)\n",
    "    \n",
    "    target_dict = {'target':[realized_volatility]}\n",
    "    feature_dict = {\n",
    "        'wap':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'wap2':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'log_returns':[realized_volatility, weighted_volatility, \n",
    "                       quarticity, np.mean],# np.std, np.max],\n",
    "        'log_returns2':[realized_volatility, weighted_volatility, \n",
    "                       quarticity, np.mean],# np.std, np.max],\n",
    "        'wap_diff':[np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'price_diff':[np.max, np.std], #np.max, np.sum],#, mad, np.max, np.sum],\n",
    "        'price_diff2':[np.max, np.std],#, np.max, np.sum],#, mad, np.max, np.sum],\n",
    "        'bid_diff':[np.max, np.std],#, np.max, np.sum],#, mad, np.max, np.sum],\n",
    "        'ask_diff':[np.max, np.std],#, np.max, np.sum],#, mad, np.max, np.sum],\n",
    "        'bid_ask_volume':[np.mean, np.std, np.max],#, np.sum],#, mad, np.max, np.sum],\n",
    "        'bid_ask_volume_diff':[np.mean, np.max],#, mad, np.max, np.sum],\n",
    "        'dom':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'dom_diff':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'dom_imbalance':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'bid_ask_spread1':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'bid_ask_spread2':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'bid_ask_spread3':[np.mean, np.std],#, mad, np.max, np.sum],\n",
    "        'slope':[np.mean, np.max, np.std],#, mad, np.max, np.sum],\n",
    "        'dispersion':[np.mean, np.max],#, mad, np.max, np.sum],\n",
    "        'price_impact':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'ofi':[np.mean, np.std, np.max],#, mad, np.max, np.sum],\n",
    "        'turn_over':[np.mean, np.std, np.max]#, mad, np.max, np.sum],\n",
    "    }\n",
    "    \n",
    "    # target realized volatility for next 300 second (5 min window in time_id)\n",
    "    feature = generate_target(\n",
    "        df, target=target_dict, \n",
    "        second=900 if extend else 300  # target volatility is for next 300 second (no overlap)\n",
    "    )\n",
    "    \n",
    "    # obtain features over entire 5 mins or last 2.5 mins..\n",
    "    \n",
    "    feature = generate_feature_seconds(\n",
    "        feature, df, \n",
    "        feats=feature_dict, \n",
    "        seconds=[900, 600, 300, 150, 75] if extend else [300, 150, 75],  # features generated for each second interval (different widths)\n",
    "        overlap=1,\n",
    "    )\n",
    "    #create stock_id\n",
    "    feature['stock_id'] = int(stock_id)\n",
    "    feature['time_id'] = feature['time_id'].apply(int)\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def concat_features(stock_id, df, extend=False, norm=False):\n",
    "    \n",
    "    if norm:\n",
    "        stock_df = pd.read_csv(f\"individual_book_train/stock_{stock_id}.csv\")\n",
    "        return pd.concat([df, generate_features(stock_id, stock_df, extend=extend, norm=norm)])\n",
    "    \n",
    "    stock_df = pd.read_feather(\n",
    "        f\"extend_two/stock_{stock_id}.fth\") if extend else pd.read_feather(\n",
    "                                    f\"extend_one/stock_{stock_id}.fth\")   \n",
    "    \n",
    "    return pd.concat([df, generate_features(stock_id, stock_df, extend=extend)])\n",
    "\n",
    "def process_stocks(stock_ids, extend=False, norm=False):\n",
    "    df = pd.DataFrame()\n",
    "    df = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(concat_features)(stock_id, df, extend=extend, norm=norm) for stock_id in stock_ids\n",
    "    )\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    \n",
    "    # lastly import pre-computed beta coefficients for all stock_ids\n",
    "    df = import_beta(df)\n",
    "    df = target_encoding(df)\n",
    "    df = beta_encoding(df)\n",
    "    df = import_clusters(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db988330",
   "metadata": {},
   "source": [
    "## Process Features for All Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca431fc",
   "metadata": {},
   "source": [
    "Loading Length1 TimeID (simple) stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf94f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 22s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954    194.495455     0.164908   \n",
       "1            11                    0.000981    199.598260     0.031047   \n",
       "2            16                    0.001295    209.021831     0.092861   \n",
       "3            31                    0.001776    216.281256     0.183025   \n",
       "4            62                    0.001520    214.542788     0.051133   \n",
       "...         ...                         ...           ...          ...   \n",
       "428927    32751                    0.002899    306.672174     0.163919   \n",
       "428928    32753                    0.003454    291.124934     0.147318   \n",
       "428929    32758                    0.002792    202.972820     0.064758   \n",
       "428930    32763                    0.002379    152.478929     0.068413   \n",
       "428931    32767                    0.001414    194.982143     0.040268   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0          194.479014      0.196558                             0.003394   \n",
       "1          199.597336      0.036259                             0.000699   \n",
       "2          209.053034      0.098250                             0.001983   \n",
       "3          216.198136      0.164985                             0.001863   \n",
       "4          214.524415      0.071793                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428927     306.730549      0.206509                             0.002284   \n",
       "428928     291.137380      0.164838                             0.002217   \n",
       "428929     202.958539      0.080424                             0.001386   \n",
       "428930     152.480008      0.075165                             0.002783   \n",
       "428931     195.012846      0.055999                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000132                2.117007e-10   \n",
       "428928                             0.000128                9.484441e-11   \n",
       "428929                             0.000080                3.031629e-11   \n",
       "428930                             0.000161                1.960849e-10   \n",
       "428931                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0               7.138250e-06  ...            1              4   \n",
       "1               8.823633e-07  ...            1              4   \n",
       "2               1.729093e-06  ...            1              4   \n",
       "3              -5.516464e-06  ...            1              4   \n",
       "4              -2.164288e-06  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "428927         -2.858852e-06  ...            1              2   \n",
       "428928          3.674218e-06  ...            1              2   \n",
       "428929         -2.437732e-06  ...            1              2   \n",
       "428930          6.737309e-06  ...            1              2   \n",
       "428931         -1.059089e-06  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428932 rows x 195 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "simple = process_stocks(beta.stock_id.unique(), extend=False)\n",
    "simple = simple.reset_index(drop=True)\n",
    "simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd58ae",
   "metadata": {},
   "source": [
    "Loading Length2 TimeID (extend2) stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df98c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 15s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_900</th>\n",
       "      <th>wap_std_900</th>\n",
       "      <th>wap2_mean_900</th>\n",
       "      <th>wap2_std_900</th>\n",
       "      <th>log_returns_realized_volatility_900</th>\n",
       "      <th>log_returns_weighted_volatility_900</th>\n",
       "      <th>log_returns_quarticity_900</th>\n",
       "      <th>log_returns_mean_900</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.523405</td>\n",
       "      <td>0.137002</td>\n",
       "      <td>194.494943</td>\n",
       "      <td>0.153934</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>3.455208e-09</td>\n",
       "      <td>-9.753985e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.519609</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>209.558521</td>\n",
       "      <td>0.383354</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.099042e-07</td>\n",
       "      <td>-2.355713e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>191.553810</td>\n",
       "      <td>0.364277</td>\n",
       "      <td>191.534358</td>\n",
       "      <td>0.383770</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>1.346483e-07</td>\n",
       "      <td>-3.936685e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>192.765232</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>192.735618</td>\n",
       "      <td>0.657042</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>1.127322e-06</td>\n",
       "      <td>-6.358684e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>192.219228</td>\n",
       "      <td>5.420681</td>\n",
       "      <td>192.208617</td>\n",
       "      <td>5.426199</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>3.980222e-03</td>\n",
       "      <td>6.443939e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213355</th>\n",
       "      <td>32748</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>191.609684</td>\n",
       "      <td>1.258293</td>\n",
       "      <td>191.604521</td>\n",
       "      <td>1.254466</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>6.904742e-06</td>\n",
       "      <td>2.088613e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213356</th>\n",
       "      <td>32750</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>244.526554</td>\n",
       "      <td>1.607117</td>\n",
       "      <td>244.538262</td>\n",
       "      <td>1.625978</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>5.504136e-06</td>\n",
       "      <td>-1.615979e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213357</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>308.076728</td>\n",
       "      <td>1.006311</td>\n",
       "      <td>308.107535</td>\n",
       "      <td>0.989751</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>5.021137e-07</td>\n",
       "      <td>-9.098024e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213358</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>199.448299</td>\n",
       "      <td>2.496420</td>\n",
       "      <td>199.399570</td>\n",
       "      <td>2.520482</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>1.302272e-04</td>\n",
       "      <td>2.968647e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213359</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>195.658217</td>\n",
       "      <td>0.521323</td>\n",
       "      <td>195.671518</td>\n",
       "      <td>0.508953</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>6.154658e-08</td>\n",
       "      <td>-4.053552e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213360 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_900  wap_std_900  \\\n",
       "0             5                    0.002954    194.523405     0.137002   \n",
       "1            16                    0.001295    209.519609     0.379075   \n",
       "2            72                    0.006357    191.553810     0.364277   \n",
       "3           146                    0.002993    192.765232     0.641515   \n",
       "4           169                    0.001498    192.219228     5.420681   \n",
       "...         ...                         ...           ...          ...   \n",
       "213355    32748                    0.003057    191.609684     1.258293   \n",
       "213356    32750                    0.001827    244.526554     1.607117   \n",
       "213357    32751                    0.002899    308.076728     1.006311   \n",
       "213358    32758                    0.002792    199.448299     2.496420   \n",
       "213359    32767                    0.001414    195.658217     0.521323   \n",
       "\n",
       "        wap2_mean_900  wap2_std_900  log_returns_realized_volatility_900  \\\n",
       "0          194.494943      0.153934                             0.004921   \n",
       "1          209.558521      0.383354                             0.005600   \n",
       "2          191.534358      0.383770                             0.007720   \n",
       "3          192.735618      0.657042                             0.011335   \n",
       "4          192.208617      5.426199                             0.060467   \n",
       "...               ...           ...                                  ...   \n",
       "213355     191.604521      1.254466                             0.013254   \n",
       "213356     244.538262      1.625978                             0.012686   \n",
       "213357     308.107535      0.989751                             0.007706   \n",
       "213358     199.399570      2.520482                             0.026106   \n",
       "213359     195.671518      0.508953                             0.006634   \n",
       "\n",
       "        log_returns_weighted_volatility_900  log_returns_quarticity_900  \\\n",
       "0                                  0.000164                3.455208e-09   \n",
       "1                                  0.000187                1.099042e-07   \n",
       "2                                  0.000257                1.346483e-07   \n",
       "3                                  0.000378                1.127322e-06   \n",
       "4                                  0.002017                3.980222e-03   \n",
       "...                                     ...                         ...   \n",
       "213355                             0.000442                6.904742e-06   \n",
       "213356                             0.000423                5.504136e-06   \n",
       "213357                             0.000257                5.021137e-07   \n",
       "213358                             0.000871                1.302272e-04   \n",
       "213359                             0.000221                6.154658e-08   \n",
       "\n",
       "        log_returns_mean_900  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0              -9.753985e-07  ...            1              4   \n",
       "1              -2.355713e-06  ...            1              4   \n",
       "2              -3.936685e-06  ...            1              4   \n",
       "3              -6.358684e-06  ...            1              4   \n",
       "4               6.443939e-05  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "213355          2.088613e-05  ...            1              2   \n",
       "213356         -1.615979e-05  ...            1              2   \n",
       "213357         -9.098024e-06  ...            1              2   \n",
       "213358          2.968647e-05  ...            1              2   \n",
       "213359         -4.053552e-06  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "213355                -1         2          1          1           3   \n",
       "213356                -1         2          1          1           3   \n",
       "213357                -1         2          1          1           3   \n",
       "213358                -1         2          1          1           3   \n",
       "213359                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "213355           12         1          1  \n",
       "213356           12         1          1  \n",
       "213357           12         1          1  \n",
       "213358           12         1          1  \n",
       "213359           12         1          1  \n",
       "\n",
       "[213360 rows x 307 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extend = process_stocks(beta.stock_id.unique(), extend=True)\n",
    "extend = extend.reset_index(drop=True)\n",
    "extend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb3ead",
   "metadata": {},
   "source": [
    "#### Comment out to save data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d27f8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#simple.to_feather(\"simple.fth\")\n",
    "#extend.to_feather(\"extend.fth\")\n",
    "#del simple\n",
    "#del extend "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf3df5",
   "metadata": {},
   "source": [
    "#### Comment out to read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e897f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple = pd.read_feather(\"simple.fth\")\n",
    "#extend = pd.read_feather(\"extend.fth\")\n",
    "\n",
    "#del beta\n",
    "#del spread_dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff423ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates = pd.merge(simple, extend, how='inner', \n",
    "#                       left_on=['stock_id','time_id'], \n",
    "#                       right_on=['stock_id','time_id'])\n",
    "\n",
    "# simple = simple.drop(duplicates.index).reset_index(drop=True)\n",
    "# simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30172175",
   "metadata": {},
   "source": [
    "### Check for NaN in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ee1295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple[simple.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd41f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extend[extend.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc93ad7",
   "metadata": {},
   "source": [
    "### Removing row with zero realized volatilities\n",
    "\n",
    "* Corrupts RMSPE and MAPE metric scores\n",
    "    * Denominator is y_true\n",
    "    * Added small epsilon to prevent zerodivision\n",
    "    * Results in very large error if y_true = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389747f",
   "metadata": {},
   "source": [
    "Note: Proportion is similar to that with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0873b28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple[simple['target_realized_volatility']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5643cba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extend[extend['target_realized_volatility']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e65c29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954    194.495455     0.164908   \n",
       "1            11                    0.000981    199.598260     0.031047   \n",
       "2            16                    0.001295    209.021831     0.092861   \n",
       "3            31                    0.001776    216.281256     0.183025   \n",
       "4            62                    0.001520    214.542788     0.051133   \n",
       "...         ...                         ...           ...          ...   \n",
       "428927    32751                    0.002899    306.672174     0.163919   \n",
       "428928    32753                    0.003454    291.124934     0.147318   \n",
       "428929    32758                    0.002792    202.972820     0.064758   \n",
       "428930    32763                    0.002379    152.478929     0.068413   \n",
       "428931    32767                    0.001414    194.982143     0.040268   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0          194.479014      0.196558                             0.003394   \n",
       "1          199.597336      0.036259                             0.000699   \n",
       "2          209.053034      0.098250                             0.001983   \n",
       "3          216.198136      0.164985                             0.001863   \n",
       "4          214.524415      0.071793                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428927     306.730549      0.206509                             0.002284   \n",
       "428928     291.137380      0.164838                             0.002217   \n",
       "428929     202.958539      0.080424                             0.001386   \n",
       "428930     152.480008      0.075165                             0.002783   \n",
       "428931     195.012846      0.055999                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000132                2.117007e-10   \n",
       "428928                             0.000128                9.484441e-11   \n",
       "428929                             0.000080                3.031629e-11   \n",
       "428930                             0.000161                1.960849e-10   \n",
       "428931                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0               7.138250e-06  ...            1              4   \n",
       "1               8.823633e-07  ...            1              4   \n",
       "2               1.729093e-06  ...            1              4   \n",
       "3              -5.516464e-06  ...            1              4   \n",
       "4              -2.164288e-06  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "428927         -2.858852e-06  ...            1              2   \n",
       "428928          3.674218e-06  ...            1              2   \n",
       "428929         -2.437732e-06  ...            1              2   \n",
       "428930          6.737309e-06  ...            1              2   \n",
       "428931         -1.059089e-06  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428791 rows x 195 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple = simple[simple['target_realized_volatility'] != 0]#.sort_values(['stock_id','time_id']).reset_index(drop=True)\n",
    "simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "661babcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_900</th>\n",
       "      <th>wap_std_900</th>\n",
       "      <th>wap2_mean_900</th>\n",
       "      <th>wap2_std_900</th>\n",
       "      <th>log_returns_realized_volatility_900</th>\n",
       "      <th>log_returns_weighted_volatility_900</th>\n",
       "      <th>log_returns_quarticity_900</th>\n",
       "      <th>log_returns_mean_900</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.523405</td>\n",
       "      <td>0.137002</td>\n",
       "      <td>194.494943</td>\n",
       "      <td>0.153934</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>3.455208e-09</td>\n",
       "      <td>-9.753985e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.519609</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>209.558521</td>\n",
       "      <td>0.383354</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.099042e-07</td>\n",
       "      <td>-2.355713e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>191.553810</td>\n",
       "      <td>0.364277</td>\n",
       "      <td>191.534358</td>\n",
       "      <td>0.383770</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>1.346483e-07</td>\n",
       "      <td>-3.936685e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>192.765232</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>192.735618</td>\n",
       "      <td>0.657042</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>1.127322e-06</td>\n",
       "      <td>-6.358684e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>192.219228</td>\n",
       "      <td>5.420681</td>\n",
       "      <td>192.208617</td>\n",
       "      <td>5.426199</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>3.980222e-03</td>\n",
       "      <td>6.443939e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213355</th>\n",
       "      <td>32748</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>191.609684</td>\n",
       "      <td>1.258293</td>\n",
       "      <td>191.604521</td>\n",
       "      <td>1.254466</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>6.904742e-06</td>\n",
       "      <td>2.088613e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213356</th>\n",
       "      <td>32750</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>244.526554</td>\n",
       "      <td>1.607117</td>\n",
       "      <td>244.538262</td>\n",
       "      <td>1.625978</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>5.504136e-06</td>\n",
       "      <td>-1.615979e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213357</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>308.076728</td>\n",
       "      <td>1.006311</td>\n",
       "      <td>308.107535</td>\n",
       "      <td>0.989751</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>5.021137e-07</td>\n",
       "      <td>-9.098024e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213358</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>199.448299</td>\n",
       "      <td>2.496420</td>\n",
       "      <td>199.399570</td>\n",
       "      <td>2.520482</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>1.302272e-04</td>\n",
       "      <td>2.968647e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213359</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>195.658217</td>\n",
       "      <td>0.521323</td>\n",
       "      <td>195.671518</td>\n",
       "      <td>0.508953</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>6.154658e-08</td>\n",
       "      <td>-4.053552e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213297 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_900  wap_std_900  \\\n",
       "0             5                    0.002954    194.523405     0.137002   \n",
       "1            16                    0.001295    209.519609     0.379075   \n",
       "2            72                    0.006357    191.553810     0.364277   \n",
       "3           146                    0.002993    192.765232     0.641515   \n",
       "4           169                    0.001498    192.219228     5.420681   \n",
       "...         ...                         ...           ...          ...   \n",
       "213355    32748                    0.003057    191.609684     1.258293   \n",
       "213356    32750                    0.001827    244.526554     1.607117   \n",
       "213357    32751                    0.002899    308.076728     1.006311   \n",
       "213358    32758                    0.002792    199.448299     2.496420   \n",
       "213359    32767                    0.001414    195.658217     0.521323   \n",
       "\n",
       "        wap2_mean_900  wap2_std_900  log_returns_realized_volatility_900  \\\n",
       "0          194.494943      0.153934                             0.004921   \n",
       "1          209.558521      0.383354                             0.005600   \n",
       "2          191.534358      0.383770                             0.007720   \n",
       "3          192.735618      0.657042                             0.011335   \n",
       "4          192.208617      5.426199                             0.060467   \n",
       "...               ...           ...                                  ...   \n",
       "213355     191.604521      1.254466                             0.013254   \n",
       "213356     244.538262      1.625978                             0.012686   \n",
       "213357     308.107535      0.989751                             0.007706   \n",
       "213358     199.399570      2.520482                             0.026106   \n",
       "213359     195.671518      0.508953                             0.006634   \n",
       "\n",
       "        log_returns_weighted_volatility_900  log_returns_quarticity_900  \\\n",
       "0                                  0.000164                3.455208e-09   \n",
       "1                                  0.000187                1.099042e-07   \n",
       "2                                  0.000257                1.346483e-07   \n",
       "3                                  0.000378                1.127322e-06   \n",
       "4                                  0.002017                3.980222e-03   \n",
       "...                                     ...                         ...   \n",
       "213355                             0.000442                6.904742e-06   \n",
       "213356                             0.000423                5.504136e-06   \n",
       "213357                             0.000257                5.021137e-07   \n",
       "213358                             0.000871                1.302272e-04   \n",
       "213359                             0.000221                6.154658e-08   \n",
       "\n",
       "        log_returns_mean_900  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0              -9.753985e-07  ...            1              4   \n",
       "1              -2.355713e-06  ...            1              4   \n",
       "2              -3.936685e-06  ...            1              4   \n",
       "3              -6.358684e-06  ...            1              4   \n",
       "4               6.443939e-05  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "213355          2.088613e-05  ...            1              2   \n",
       "213356         -1.615979e-05  ...            1              2   \n",
       "213357         -9.098024e-06  ...            1              2   \n",
       "213358          2.968647e-05  ...            1              2   \n",
       "213359         -4.053552e-06  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "213355                -1         2          1          1           3   \n",
       "213356                -1         2          1          1           3   \n",
       "213357                -1         2          1          1           3   \n",
       "213358                -1         2          1          1           3   \n",
       "213359                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "213355           12         1          1  \n",
       "213356           12         1          1  \n",
       "213357           12         1          1  \n",
       "213358           12         1          1  \n",
       "213359           12         1          1  \n",
       "\n",
       "[213297 rows x 307 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extend = extend[extend['target_realized_volatility'] != 0]#.sort_values(['stock_id','time_id']).reset_index(drop=True)\n",
    "extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca9b31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extend[extend.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "180007e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple[simple.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4d287",
   "metadata": {},
   "source": [
    " * Same as NaN rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e53e7",
   "metadata": {},
   "source": [
    "### Defining X for LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8572b99",
   "metadata": {},
   "source": [
    "NaN is ignored by StandardScaler\n",
    "* 4th point: https://scikit-learn.org/stable/whats_new/v0.20.html#id37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54684aef",
   "metadata": {},
   "source": [
    "#### Drop target \"label\" of realized volatility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bbf7710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simpleX = simple.drop(['time_id','target_realized_volatility'], axis=1)  # leave stock id as feature \n",
    "extendX = extend.drop(['time_id','target_realized_volatility'], axis=1)  # leave stock id as feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3cc96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>log_returns2_realized_volatility_300</th>\n",
       "      <th>log_returns2_weighted_volatility_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wap_mean_300  wap_std_300  wap2_mean_300  wap2_std_300  \\\n",
       "0         194.495455     0.164908     194.479014      0.196558   \n",
       "1         199.598260     0.031047     199.597336      0.036259   \n",
       "2         209.021831     0.092861     209.053034      0.098250   \n",
       "3         216.281256     0.183025     216.198136      0.164985   \n",
       "4         214.542788     0.051133     214.524415      0.071793   \n",
       "...              ...          ...            ...           ...   \n",
       "428927    306.672174     0.163919     306.730549      0.206509   \n",
       "428928    291.124934     0.147318     291.137380      0.164838   \n",
       "428929    202.972820     0.064758     202.958539      0.080424   \n",
       "428930    152.478929     0.068413     152.480008      0.075165   \n",
       "428931    194.982143     0.040268     195.012846      0.055999   \n",
       "\n",
       "        log_returns_realized_volatility_300  \\\n",
       "0                                  0.003394   \n",
       "1                                  0.000699   \n",
       "2                                  0.001983   \n",
       "3                                  0.001863   \n",
       "4                                  0.001131   \n",
       "...                                     ...   \n",
       "428927                             0.002284   \n",
       "428928                             0.002217   \n",
       "428929                             0.001386   \n",
       "428930                             0.002783   \n",
       "428931                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000132                2.117007e-10   \n",
       "428928                             0.000128                9.484441e-11   \n",
       "428929                             0.000080                3.031629e-11   \n",
       "428930                             0.000161                1.960849e-10   \n",
       "428931                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  log_returns2_realized_volatility_300  \\\n",
       "0               7.138250e-06                              0.005032   \n",
       "1               8.823633e-07                              0.001448   \n",
       "2               1.729093e-06                              0.003583   \n",
       "3              -5.516464e-06                              0.002422   \n",
       "4              -2.164288e-06                              0.002412   \n",
       "...                      ...                                   ...   \n",
       "428927         -2.858852e-06                              0.004503   \n",
       "428928          3.674218e-06                              0.003652   \n",
       "428929         -2.437732e-06                              0.002686   \n",
       "428930          6.737309e-06                              0.004316   \n",
       "428931         -1.059089e-06                              0.001784   \n",
       "\n",
       "        log_returns2_weighted_volatility_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0                                   0.000291  ...            1              4   \n",
       "1                                   0.000084  ...            1              4   \n",
       "2                                   0.000207  ...            1              4   \n",
       "3                                   0.000140  ...            1              4   \n",
       "4                                   0.000140  ...            1              4   \n",
       "...                                      ...  ...          ...            ...   \n",
       "428927                              0.000260  ...            1              2   \n",
       "428928                              0.000211  ...            1              2   \n",
       "428929                              0.000155  ...            1              2   \n",
       "428930                              0.000250  ...            1              2   \n",
       "428931                              0.000103  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428791 rows x 193 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5bab9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_mean_900</th>\n",
       "      <th>wap_std_900</th>\n",
       "      <th>wap2_mean_900</th>\n",
       "      <th>wap2_std_900</th>\n",
       "      <th>log_returns_realized_volatility_900</th>\n",
       "      <th>log_returns_weighted_volatility_900</th>\n",
       "      <th>log_returns_quarticity_900</th>\n",
       "      <th>log_returns_mean_900</th>\n",
       "      <th>log_returns2_realized_volatility_900</th>\n",
       "      <th>log_returns2_weighted_volatility_900</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194.523405</td>\n",
       "      <td>0.137002</td>\n",
       "      <td>194.494943</td>\n",
       "      <td>0.153934</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>3.455208e-09</td>\n",
       "      <td>-9.753985e-07</td>\n",
       "      <td>0.007827</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209.519609</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>209.558521</td>\n",
       "      <td>0.383354</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>1.099042e-07</td>\n",
       "      <td>-2.355713e-06</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191.553810</td>\n",
       "      <td>0.364277</td>\n",
       "      <td>191.534358</td>\n",
       "      <td>0.383770</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>1.346483e-07</td>\n",
       "      <td>-3.936685e-06</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.765232</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>192.735618</td>\n",
       "      <td>0.657042</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>1.127322e-06</td>\n",
       "      <td>-6.358684e-06</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.219228</td>\n",
       "      <td>5.420681</td>\n",
       "      <td>192.208617</td>\n",
       "      <td>5.426199</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>3.980222e-03</td>\n",
       "      <td>6.443939e-05</td>\n",
       "      <td>0.060721</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213355</th>\n",
       "      <td>191.609684</td>\n",
       "      <td>1.258293</td>\n",
       "      <td>191.604521</td>\n",
       "      <td>1.254466</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>6.904742e-06</td>\n",
       "      <td>2.088613e-05</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213356</th>\n",
       "      <td>244.526554</td>\n",
       "      <td>1.607117</td>\n",
       "      <td>244.538262</td>\n",
       "      <td>1.625978</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>5.504136e-06</td>\n",
       "      <td>-1.615979e-05</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213357</th>\n",
       "      <td>308.076728</td>\n",
       "      <td>1.006311</td>\n",
       "      <td>308.107535</td>\n",
       "      <td>0.989751</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>5.021137e-07</td>\n",
       "      <td>-9.098024e-06</td>\n",
       "      <td>0.010255</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213358</th>\n",
       "      <td>199.448299</td>\n",
       "      <td>2.496420</td>\n",
       "      <td>199.399570</td>\n",
       "      <td>2.520482</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>1.302272e-04</td>\n",
       "      <td>2.968647e-05</td>\n",
       "      <td>0.026246</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213359</th>\n",
       "      <td>195.658217</td>\n",
       "      <td>0.521323</td>\n",
       "      <td>195.671518</td>\n",
       "      <td>0.508953</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>6.154658e-08</td>\n",
       "      <td>-4.053552e-06</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213297 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wap_mean_900  wap_std_900  wap2_mean_900  wap2_std_900  \\\n",
       "0         194.523405     0.137002     194.494943      0.153934   \n",
       "1         209.519609     0.379075     209.558521      0.383354   \n",
       "2         191.553810     0.364277     191.534358      0.383770   \n",
       "3         192.765232     0.641515     192.735618      0.657042   \n",
       "4         192.219228     5.420681     192.208617      5.426199   \n",
       "...              ...          ...            ...           ...   \n",
       "213355    191.609684     1.258293     191.604521      1.254466   \n",
       "213356    244.526554     1.607117     244.538262      1.625978   \n",
       "213357    308.076728     1.006311     308.107535      0.989751   \n",
       "213358    199.448299     2.496420     199.399570      2.520482   \n",
       "213359    195.658217     0.521323     195.671518      0.508953   \n",
       "\n",
       "        log_returns_realized_volatility_900  \\\n",
       "0                                  0.004921   \n",
       "1                                  0.005600   \n",
       "2                                  0.007720   \n",
       "3                                  0.011335   \n",
       "4                                  0.060467   \n",
       "...                                     ...   \n",
       "213355                             0.013254   \n",
       "213356                             0.012686   \n",
       "213357                             0.007706   \n",
       "213358                             0.026106   \n",
       "213359                             0.006634   \n",
       "\n",
       "        log_returns_weighted_volatility_900  log_returns_quarticity_900  \\\n",
       "0                                  0.000164                3.455208e-09   \n",
       "1                                  0.000187                1.099042e-07   \n",
       "2                                  0.000257                1.346483e-07   \n",
       "3                                  0.000378                1.127322e-06   \n",
       "4                                  0.002017                3.980222e-03   \n",
       "...                                     ...                         ...   \n",
       "213355                             0.000442                6.904742e-06   \n",
       "213356                             0.000423                5.504136e-06   \n",
       "213357                             0.000257                5.021137e-07   \n",
       "213358                             0.000871                1.302272e-04   \n",
       "213359                             0.000221                6.154658e-08   \n",
       "\n",
       "        log_returns_mean_900  log_returns2_realized_volatility_900  \\\n",
       "0              -9.753985e-07                              0.007827   \n",
       "1              -2.355713e-06                              0.007525   \n",
       "2              -3.936685e-06                              0.010917   \n",
       "3              -6.358684e-06                              0.014531   \n",
       "4               6.443939e-05                              0.060721   \n",
       "...                      ...                                   ...   \n",
       "213355          2.088613e-05                              0.014580   \n",
       "213356         -1.615979e-05                              0.012666   \n",
       "213357         -9.098024e-06                              0.010255   \n",
       "213358          2.968647e-05                              0.026246   \n",
       "213359         -4.053552e-06                              0.008471   \n",
       "\n",
       "        log_returns2_weighted_volatility_900  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0                                   0.000261  ...            1              4   \n",
       "1                                   0.000251  ...            1              4   \n",
       "2                                   0.000364  ...            1              4   \n",
       "3                                   0.000485  ...            1              4   \n",
       "4                                   0.002025  ...            1              4   \n",
       "...                                      ...  ...          ...            ...   \n",
       "213355                              0.000486  ...            1              2   \n",
       "213356                              0.000422  ...            1              2   \n",
       "213357                              0.000342  ...            1              2   \n",
       "213358                              0.000875  ...            1              2   \n",
       "213359                              0.000283  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "213355                -1         2          1          1           3   \n",
       "213356                -1         2          1          1           3   \n",
       "213357                -1         2          1          1           3   \n",
       "213358                -1         2          1          1           3   \n",
       "213359                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "213355           12         1          1  \n",
       "213356           12         1          1  \n",
       "213357           12         1          1  \n",
       "213358           12         1          1  \n",
       "213359           12         1          1  \n",
       "\n",
       "[213297 rows x 305 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extendX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53c341",
   "metadata": {},
   "source": [
    "### Defining y for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ad82c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002954</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000981</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001295</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001776</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001520</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.002899</td>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.002792</td>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.002379</td>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>0.001414</td>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  stock_id  time_id\n",
       "0                         0.002954         0        5\n",
       "1                         0.000981         0       11\n",
       "2                         0.001295         0       16\n",
       "3                         0.001776         0       31\n",
       "4                         0.001520         0       62\n",
       "...                            ...       ...      ...\n",
       "428927                    0.002899       126    32751\n",
       "428928                    0.003454       126    32753\n",
       "428929                    0.002792       126    32758\n",
       "428930                    0.002379       126    32763\n",
       "428931                    0.001414       126    32767\n",
       "\n",
       "[428791 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleY = simple[['target_realized_volatility', 'stock_id', 'time_id']]\n",
    "simpleY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09cace17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002954</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001295</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006357</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001498</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213355</th>\n",
       "      <td>0.003057</td>\n",
       "      <td>126</td>\n",
       "      <td>32748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213356</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>126</td>\n",
       "      <td>32750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213357</th>\n",
       "      <td>0.002899</td>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213358</th>\n",
       "      <td>0.002792</td>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213359</th>\n",
       "      <td>0.001414</td>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213297 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  stock_id  time_id\n",
       "0                         0.002954         0        5\n",
       "1                         0.001295         0       16\n",
       "2                         0.006357         0       72\n",
       "3                         0.002993         0      146\n",
       "4                         0.001498         0      169\n",
       "...                            ...       ...      ...\n",
       "213355                    0.003057       126    32748\n",
       "213356                    0.001827       126    32750\n",
       "213357                    0.002899       126    32751\n",
       "213358                    0.002792       126    32758\n",
       "213359                    0.001414       126    32767\n",
       "\n",
       "[213297 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extendY = extend[['target_realized_volatility', 'stock_id', 'time_id']]\n",
    "extendY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559bedb",
   "metadata": {},
   "source": [
    "Note: Including stock_id and time_id for logging predictions for app (target_realized_volatility is only target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab8eff",
   "metadata": {},
   "source": [
    "## Training LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d57cb7",
   "metadata": {},
   "source": [
    "### Model Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae000099",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10 #\n",
    "\n",
    "def compute_importance(model, features, typ='gain'):\n",
    "    return pd.DataFrame(\n",
    "        model.feature_importance(importance_type=typ),\n",
    "        index=features,\n",
    "        columns=['importance']\n",
    "    ).sort_values('importance')\n",
    "    \n",
    "def rmspe(y_true, y_pred, n=6):\n",
    "    return  round(np.sqrt(np.mean(np.square((y_true - y_pred) / (y_true + EPSILON)))), n)\n",
    "\n",
    "def feval_RMSPE(preds, lgbm_train, n=4):\n",
    "    labels = lgbm_train.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds), n), False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83637cf0",
   "metadata": {},
   "source": [
    "### Tuning Parameters \n",
    "* Takes too much time to be effective\n",
    "* Better to choose GOSS for faster convergence and decent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c085cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import warnings\n",
    "# import optuna.integration.lightgbm as lgb\n",
    "\n",
    "# kfold = KFold(5, random_state=42, shuffle=True)\n",
    "# params = {'metric': 'rmse',\n",
    "#           'boosting_type': 'goss',\n",
    "#           'n_jobs': -1,\n",
    "#           \"force_col_wise\": True,\n",
    "#           'seed': 42} \n",
    "\n",
    "# param_tuner = optuna.create_study(direction='minimize')\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "# target = simpleY.reset_index(drop=True)['target_realized_volatility'].values.flatten()\n",
    "# lgbm_train = lgbm.Dataset(simpleX,target.flatten(),\n",
    "#                           weight=1/(np.square(target)+EPSILON))\n",
    "\n",
    "# tuner = lgb.LightGBMTunerCV(params, \n",
    "#                             lgbm_train, \n",
    "#                             study=param_tuner,\n",
    "#                             early_stopping_rounds=250,\n",
    "#                             time_budget=19800,\n",
    "#                             seed = 42,\n",
    "#                             folds=kfold,\n",
    "#                             num_boost_round=10000,\n",
    "#                             callbacks=[\n",
    "#                               lgb.reset_parameter(\n",
    "#                                   learning_rate = [0.005]*200 + [0.001]*9800\n",
    "#                               ),\n",
    "#                               lgb.log_evaluation(period=100)\n",
    "#                             ])\n",
    "# tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tuner.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_optimization_history(param_tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cdac0",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "\n",
    "### Train Simple - Note: Overlap with Extend (which improves prediction on harder time buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fa16bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "Outer Fold : 1\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274425, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001185\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487702\ttraining's RMSPE: 0.3331\tvalid_1's rmse: 0.000487704\tvalid_1's RMSPE: 0.3291\n",
      "[200]\ttraining's rmse: 0.00039457\ttraining's RMSPE: 0.2695\tvalid_1's rmse: 0.000399279\tvalid_1's RMSPE: 0.2694\n",
      "[300]\ttraining's rmse: 0.000372613\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000380251\tvalid_1's RMSPE: 0.2566\n",
      "[400]\ttraining's rmse: 0.000364958\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.000374473\tvalid_1's RMSPE: 0.2527\n",
      "[500]\ttraining's rmse: 0.000360761\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.000371661\tvalid_1's RMSPE: 0.2508\n",
      "[600]\ttraining's rmse: 0.000357735\ttraining's RMSPE: 0.2444\tvalid_1's rmse: 0.000369839\tvalid_1's RMSPE: 0.2496\n",
      "[700]\ttraining's rmse: 0.000355369\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000368462\tvalid_1's RMSPE: 0.2486\n",
      "[800]\ttraining's rmse: 0.000353347\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000367293\tvalid_1's RMSPE: 0.2478\n",
      "[900]\ttraining's rmse: 0.000351605\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000366343\tvalid_1's RMSPE: 0.2472\n",
      "[1000]\ttraining's rmse: 0.000350087\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000365562\tvalid_1's RMSPE: 0.2467\n",
      "[1100]\ttraining's rmse: 0.000348721\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.00036489\tvalid_1's RMSPE: 0.2462\n",
      "[1200]\ttraining's rmse: 0.000347484\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000364279\tvalid_1's RMSPE: 0.2458\n",
      "[1300]\ttraining's rmse: 0.000346368\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000363798\tvalid_1's RMSPE: 0.2455\n",
      "[1400]\ttraining's rmse: 0.000345355\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000363407\tvalid_1's RMSPE: 0.2452\n",
      "[1500]\ttraining's rmse: 0.000344395\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000363051\tvalid_1's RMSPE: 0.245\n",
      "[1600]\ttraining's rmse: 0.000343528\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000362698\tvalid_1's RMSPE: 0.2447\n",
      "[1700]\ttraining's rmse: 0.000342715\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000362408\tvalid_1's RMSPE: 0.2445\n",
      "[1800]\ttraining's rmse: 0.000341938\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000362163\tvalid_1's RMSPE: 0.2444\n",
      "[1900]\ttraining's rmse: 0.000341201\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000361931\tvalid_1's RMSPE: 0.2442\n",
      "[2000]\ttraining's rmse: 0.000340482\ttraining's RMSPE: 0.2326\tvalid_1's rmse: 0.000361778\tvalid_1's RMSPE: 0.2441\n",
      "[2100]\ttraining's rmse: 0.000339801\ttraining's RMSPE: 0.2321\tvalid_1's rmse: 0.0003616\tvalid_1's RMSPE: 0.244\n",
      "[2200]\ttraining's rmse: 0.000339139\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000361466\tvalid_1's RMSPE: 0.2439\n",
      "Early stopping, best iteration is:\n",
      "[2133]\ttraining's rmse: 0.000339579\ttraining's RMSPE: 0.232\tvalid_1's rmse: 0.000361521\tvalid_1's RMSPE: 0.2439\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2439\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274425, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487656\ttraining's RMSPE: 0.3327\tvalid_1's rmse: 0.000489692\tvalid_1's RMSPE: 0.3322\n",
      "[200]\ttraining's rmse: 0.000394471\ttraining's RMSPE: 0.2691\tvalid_1's rmse: 0.000400849\tvalid_1's RMSPE: 0.2719\n",
      "[300]\ttraining's rmse: 0.000372637\ttraining's RMSPE: 0.2542\tvalid_1's rmse: 0.000380997\tvalid_1's RMSPE: 0.2585\n",
      "[400]\ttraining's rmse: 0.000365096\ttraining's RMSPE: 0.2491\tvalid_1's rmse: 0.000374605\tvalid_1's RMSPE: 0.2541\n",
      "[500]\ttraining's rmse: 0.000360981\ttraining's RMSPE: 0.2463\tvalid_1's rmse: 0.000371427\tvalid_1's RMSPE: 0.252\n",
      "[600]\ttraining's rmse: 0.00035807\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000369423\tvalid_1's RMSPE: 0.2506\n",
      "[700]\ttraining's rmse: 0.000355748\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.00036792\tvalid_1's RMSPE: 0.2496\n",
      "[800]\ttraining's rmse: 0.000353772\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000366854\tvalid_1's RMSPE: 0.2489\n",
      "[900]\ttraining's rmse: 0.000352059\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000365946\tvalid_1's RMSPE: 0.2483\n",
      "[1000]\ttraining's rmse: 0.000350593\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.00036521\tvalid_1's RMSPE: 0.2478\n",
      "[1100]\ttraining's rmse: 0.000349264\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000364545\tvalid_1's RMSPE: 0.2473\n",
      "[1200]\ttraining's rmse: 0.000348108\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000364072\tvalid_1's RMSPE: 0.247\n",
      "[1300]\ttraining's rmse: 0.000347007\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000363572\tvalid_1's RMSPE: 0.2466\n",
      "[1400]\ttraining's rmse: 0.000346002\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000363197\tvalid_1's RMSPE: 0.2464\n",
      "[1500]\ttraining's rmse: 0.000345079\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000362872\tvalid_1's RMSPE: 0.2462\n",
      "[1600]\ttraining's rmse: 0.00034424\ttraining's RMSPE: 0.2348\tvalid_1's rmse: 0.000362576\tvalid_1's RMSPE: 0.246\n",
      "[1700]\ttraining's rmse: 0.00034343\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000362232\tvalid_1's RMSPE: 0.2457\n",
      "[1800]\ttraining's rmse: 0.000342649\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000361982\tvalid_1's RMSPE: 0.2456\n",
      "[1900]\ttraining's rmse: 0.000341923\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.00036174\tvalid_1's RMSPE: 0.2454\n",
      "[2000]\ttraining's rmse: 0.000341232\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000361551\tvalid_1's RMSPE: 0.2453\n",
      "[2100]\ttraining's rmse: 0.000340539\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000361406\tvalid_1's RMSPE: 0.2452\n",
      "Early stopping, best iteration is:\n",
      "[2012]\ttraining's rmse: 0.000341135\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000361517\tvalid_1's RMSPE: 0.2452\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2452\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001209\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487017\ttraining's RMSPE: 0.3292\tvalid_1's rmse: 0.000520724\tvalid_1's RMSPE: 0.3659\n",
      "[200]\ttraining's rmse: 0.00039639\ttraining's RMSPE: 0.268\tvalid_1's rmse: 0.000422192\tvalid_1's RMSPE: 0.2966\n",
      "[300]\ttraining's rmse: 0.000375072\ttraining's RMSPE: 0.2536\tvalid_1's rmse: 0.000395677\tvalid_1's RMSPE: 0.278\n",
      "[400]\ttraining's rmse: 0.000367608\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.000387114\tvalid_1's RMSPE: 0.272\n",
      "[500]\ttraining's rmse: 0.000363406\ttraining's RMSPE: 0.2457\tvalid_1's rmse: 0.000381229\tvalid_1's RMSPE: 0.2678\n",
      "[600]\ttraining's rmse: 0.000360466\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000377152\tvalid_1's RMSPE: 0.265\n",
      "[700]\ttraining's rmse: 0.000357928\ttraining's RMSPE: 0.242\tvalid_1's rmse: 0.000374793\tvalid_1's RMSPE: 0.2633\n",
      "[800]\ttraining's rmse: 0.000355911\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000372495\tvalid_1's RMSPE: 0.2617\n",
      "[900]\ttraining's rmse: 0.000354114\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000371191\tvalid_1's RMSPE: 0.2608\n",
      "[1000]\ttraining's rmse: 0.000352612\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000369723\tvalid_1's RMSPE: 0.2598\n",
      "[1100]\ttraining's rmse: 0.000351276\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000368949\tvalid_1's RMSPE: 0.2592\n",
      "[1200]\ttraining's rmse: 0.000350037\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000368107\tvalid_1's RMSPE: 0.2586\n",
      "[1300]\ttraining's rmse: 0.000348917\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000368205\tvalid_1's RMSPE: 0.2587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1208]\ttraining's rmse: 0.000349948\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000367964\tvalid_1's RMSPE: 0.2585\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2585\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001186\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486992\ttraining's RMSPE: 0.3324\tvalid_1's rmse: 0.000501354\tvalid_1's RMSPE: 0.3393\n",
      "[200]\ttraining's rmse: 0.000393581\ttraining's RMSPE: 0.2687\tvalid_1's rmse: 0.000422423\tvalid_1's RMSPE: 0.2859\n",
      "[300]\ttraining's rmse: 0.000372037\ttraining's RMSPE: 0.2539\tvalid_1's rmse: 0.000406916\tvalid_1's RMSPE: 0.2754\n",
      "[400]\ttraining's rmse: 0.00036471\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.00040198\tvalid_1's RMSPE: 0.2721\n",
      "[500]\ttraining's rmse: 0.000360663\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000399401\tvalid_1's RMSPE: 0.2703\n",
      "[600]\ttraining's rmse: 0.000357786\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.00039792\tvalid_1's RMSPE: 0.2693\n",
      "[700]\ttraining's rmse: 0.000355492\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.000396224\tvalid_1's RMSPE: 0.2682\n",
      "[800]\ttraining's rmse: 0.000353555\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000395387\tvalid_1's RMSPE: 0.2676\n",
      "[900]\ttraining's rmse: 0.000351823\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000393991\tvalid_1's RMSPE: 0.2667\n",
      "[1000]\ttraining's rmse: 0.000350333\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000393629\tvalid_1's RMSPE: 0.2664\n",
      "[1100]\ttraining's rmse: 0.000349015\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000392979\tvalid_1's RMSPE: 0.266\n",
      "[1200]\ttraining's rmse: 0.000347803\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000392543\tvalid_1's RMSPE: 0.2657\n",
      "[1300]\ttraining's rmse: 0.00034674\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000392317\tvalid_1's RMSPE: 0.2655\n",
      "[1400]\ttraining's rmse: 0.000345737\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000391948\tvalid_1's RMSPE: 0.2653\n",
      "[1500]\ttraining's rmse: 0.000344754\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000391511\tvalid_1's RMSPE: 0.265\n",
      "[1600]\ttraining's rmse: 0.00034392\ttraining's RMSPE: 0.2348\tvalid_1's rmse: 0.000391095\tvalid_1's RMSPE: 0.2647\n",
      "Early stopping, best iteration is:\n",
      "[1582]\ttraining's rmse: 0.000344063\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000391114\tvalid_1's RMSPE: 0.2647\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2647\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001185\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486287\ttraining's RMSPE: 0.3323\tvalid_1's rmse: 0.000498584\tvalid_1's RMSPE: 0.3361\n",
      "[200]\ttraining's rmse: 0.000393338\ttraining's RMSPE: 0.2687\tvalid_1's rmse: 0.000415043\tvalid_1's RMSPE: 0.2798\n",
      "[300]\ttraining's rmse: 0.000371944\ttraining's RMSPE: 0.2541\tvalid_1's rmse: 0.000397607\tvalid_1's RMSPE: 0.2681\n",
      "[400]\ttraining's rmse: 0.000364565\ttraining's RMSPE: 0.2491\tvalid_1's rmse: 0.00039214\tvalid_1's RMSPE: 0.2644\n",
      "[500]\ttraining's rmse: 0.000360567\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.000389629\tvalid_1's RMSPE: 0.2627\n",
      "[600]\ttraining's rmse: 0.000357686\ttraining's RMSPE: 0.2444\tvalid_1's rmse: 0.000387805\tvalid_1's RMSPE: 0.2615\n",
      "[700]\ttraining's rmse: 0.000355362\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000386308\tvalid_1's RMSPE: 0.2604\n",
      "[800]\ttraining's rmse: 0.000353359\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000385134\tvalid_1's RMSPE: 0.2597\n",
      "[900]\ttraining's rmse: 0.000351617\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000384242\tvalid_1's RMSPE: 0.2591\n",
      "[1000]\ttraining's rmse: 0.000350128\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000383516\tvalid_1's RMSPE: 0.2586\n",
      "[1100]\ttraining's rmse: 0.000348797\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000382886\tvalid_1's RMSPE: 0.2581\n",
      "[1200]\ttraining's rmse: 0.000347584\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000382377\tvalid_1's RMSPE: 0.2578\n",
      "[1300]\ttraining's rmse: 0.000346494\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000381899\tvalid_1's RMSPE: 0.2575\n",
      "[1400]\ttraining's rmse: 0.000345496\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000381507\tvalid_1's RMSPE: 0.2572\n",
      "[1500]\ttraining's rmse: 0.000344541\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000381151\tvalid_1's RMSPE: 0.257\n",
      "[1600]\ttraining's rmse: 0.000343639\ttraining's RMSPE: 0.2348\tvalid_1's rmse: 0.000380783\tvalid_1's RMSPE: 0.2567\n",
      "[1700]\ttraining's rmse: 0.000342836\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.00038053\tvalid_1's RMSPE: 0.2566\n",
      "[1800]\ttraining's rmse: 0.000342056\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000380267\tvalid_1's RMSPE: 0.2564\n",
      "[1900]\ttraining's rmse: 0.000341297\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000380046\tvalid_1's RMSPE: 0.2562\n",
      "Early stopping, best iteration is:\n",
      "[1881]\ttraining's rmse: 0.00034144\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000380078\tvalid_1's RMSPE: 0.2562\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2562\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 2\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001214\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000485192\ttraining's RMSPE: 0.3272\tvalid_1's rmse: 0.000495899\tvalid_1's RMSPE: 0.336\n",
      "[200]\ttraining's rmse: 0.000394592\ttraining's RMSPE: 0.2661\tvalid_1's rmse: 0.000410625\tvalid_1's RMSPE: 0.2782\n",
      "[300]\ttraining's rmse: 0.000374224\ttraining's RMSPE: 0.2524\tvalid_1's rmse: 0.000392906\tvalid_1's RMSPE: 0.2662\n",
      "[400]\ttraining's rmse: 0.000367349\ttraining's RMSPE: 0.2477\tvalid_1's rmse: 0.000387764\tvalid_1's RMSPE: 0.2627\n",
      "[500]\ttraining's rmse: 0.000363495\ttraining's RMSPE: 0.2451\tvalid_1's rmse: 0.000385021\tvalid_1's RMSPE: 0.2608\n",
      "[600]\ttraining's rmse: 0.000360737\ttraining's RMSPE: 0.2433\tvalid_1's rmse: 0.000383301\tvalid_1's RMSPE: 0.2597\n",
      "[700]\ttraining's rmse: 0.000358543\ttraining's RMSPE: 0.2418\tvalid_1's rmse: 0.000382059\tvalid_1's RMSPE: 0.2588\n",
      "[800]\ttraining's rmse: 0.000356691\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000381164\tvalid_1's RMSPE: 0.2582\n",
      "[900]\ttraining's rmse: 0.000355065\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000380275\tvalid_1's RMSPE: 0.2576\n",
      "[1000]\ttraining's rmse: 0.000353686\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.00037961\tvalid_1's RMSPE: 0.2572\n",
      "[1100]\ttraining's rmse: 0.000352415\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000379152\tvalid_1's RMSPE: 0.2569\n",
      "[1200]\ttraining's rmse: 0.000351257\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000378595\tvalid_1's RMSPE: 0.2565\n",
      "[1300]\ttraining's rmse: 0.000350184\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000378055\tvalid_1's RMSPE: 0.2561\n",
      "[1400]\ttraining's rmse: 0.00034925\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000377633\tvalid_1's RMSPE: 0.2558\n",
      "[1500]\ttraining's rmse: 0.000348346\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.00037734\tvalid_1's RMSPE: 0.2556\n",
      "[1600]\ttraining's rmse: 0.000347497\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000376999\tvalid_1's RMSPE: 0.2554\n",
      "[1700]\ttraining's rmse: 0.00034669\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000376826\tvalid_1's RMSPE: 0.2553\n",
      "Early stopping, best iteration is:\n",
      "[1621]\ttraining's rmse: 0.000347329\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000376911\tvalid_1's RMSPE: 0.2553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2553\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001210\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487117\ttraining's RMSPE: 0.3289\tvalid_1's rmse: 0.000485873\tvalid_1's RMSPE: 0.3273\n",
      "[200]\ttraining's rmse: 0.00039644\ttraining's RMSPE: 0.2677\tvalid_1's rmse: 0.00039815\tvalid_1's RMSPE: 0.2682\n",
      "[300]\ttraining's rmse: 0.000375311\ttraining's RMSPE: 0.2534\tvalid_1's rmse: 0.00037864\tvalid_1's RMSPE: 0.2551\n",
      "[400]\ttraining's rmse: 0.000368009\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.000372536\tvalid_1's RMSPE: 0.251\n",
      "[500]\ttraining's rmse: 0.000363911\ttraining's RMSPE: 0.2457\tvalid_1's rmse: 0.000369703\tvalid_1's RMSPE: 0.2491\n",
      "[600]\ttraining's rmse: 0.00036098\ttraining's RMSPE: 0.2438\tvalid_1's rmse: 0.000367919\tvalid_1's RMSPE: 0.2479\n",
      "[700]\ttraining's rmse: 0.000358683\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000366452\tvalid_1's RMSPE: 0.2469\n",
      "[800]\ttraining's rmse: 0.000356699\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000365331\tvalid_1's RMSPE: 0.2461\n",
      "[900]\ttraining's rmse: 0.000354983\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000364485\tvalid_1's RMSPE: 0.2456\n",
      "[1000]\ttraining's rmse: 0.00035349\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000363787\tvalid_1's RMSPE: 0.2451\n",
      "[1100]\ttraining's rmse: 0.000352127\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000363141\tvalid_1's RMSPE: 0.2447\n",
      "[1200]\ttraining's rmse: 0.000350952\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000362575\tvalid_1's RMSPE: 0.2443\n",
      "[1300]\ttraining's rmse: 0.000349908\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000362178\tvalid_1's RMSPE: 0.244\n",
      "[1400]\ttraining's rmse: 0.000348932\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000361732\tvalid_1's RMSPE: 0.2437\n",
      "[1500]\ttraining's rmse: 0.000347973\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000361407\tvalid_1's RMSPE: 0.2435\n",
      "[1600]\ttraining's rmse: 0.000347097\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.00036103\tvalid_1's RMSPE: 0.2432\n",
      "[1700]\ttraining's rmse: 0.000346291\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.00036075\tvalid_1's RMSPE: 0.243\n",
      "[1800]\ttraining's rmse: 0.000345556\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000360592\tvalid_1's RMSPE: 0.2429\n",
      "[1900]\ttraining's rmse: 0.00034483\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.00036043\tvalid_1's RMSPE: 0.2428\n",
      "[2000]\ttraining's rmse: 0.000344154\ttraining's RMSPE: 0.2324\tvalid_1's rmse: 0.000360222\tvalid_1's RMSPE: 0.2427\n",
      "Early stopping, best iteration is:\n",
      "[1952]\ttraining's rmse: 0.000344467\ttraining's RMSPE: 0.2326\tvalid_1's rmse: 0.000360304\tvalid_1's RMSPE: 0.2427\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2427\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001211\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486647\ttraining's RMSPE: 0.3286\tvalid_1's rmse: 0.000492255\tvalid_1's RMSPE: 0.3317\n",
      "[200]\ttraining's rmse: 0.000396009\ttraining's RMSPE: 0.2674\tvalid_1's rmse: 0.000403022\tvalid_1's RMSPE: 0.2716\n",
      "[300]\ttraining's rmse: 0.000374779\ttraining's RMSPE: 0.2531\tvalid_1's rmse: 0.00038303\tvalid_1's RMSPE: 0.2581\n",
      "[400]\ttraining's rmse: 0.000367478\ttraining's RMSPE: 0.2481\tvalid_1's rmse: 0.000376728\tvalid_1's RMSPE: 0.2539\n",
      "[500]\ttraining's rmse: 0.000363417\ttraining's RMSPE: 0.2454\tvalid_1's rmse: 0.000373811\tvalid_1's RMSPE: 0.2519\n",
      "[600]\ttraining's rmse: 0.000360499\ttraining's RMSPE: 0.2434\tvalid_1's rmse: 0.000371835\tvalid_1's RMSPE: 0.2506\n",
      "[700]\ttraining's rmse: 0.000358176\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000370393\tvalid_1's RMSPE: 0.2496\n",
      "[800]\ttraining's rmse: 0.000356202\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000369355\tvalid_1's RMSPE: 0.2489\n",
      "[900]\ttraining's rmse: 0.000354511\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000368667\tvalid_1's RMSPE: 0.2484\n",
      "[1000]\ttraining's rmse: 0.000353001\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000367846\tvalid_1's RMSPE: 0.2479\n",
      "[1100]\ttraining's rmse: 0.000351669\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000367269\tvalid_1's RMSPE: 0.2475\n",
      "[1200]\ttraining's rmse: 0.000350492\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.00036674\tvalid_1's RMSPE: 0.2471\n",
      "[1300]\ttraining's rmse: 0.000349394\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000366412\tvalid_1's RMSPE: 0.2469\n",
      "[1400]\ttraining's rmse: 0.000348377\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.00036619\tvalid_1's RMSPE: 0.2468\n",
      "[1500]\ttraining's rmse: 0.000347442\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000365822\tvalid_1's RMSPE: 0.2465\n",
      "[1600]\ttraining's rmse: 0.000346549\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000365563\tvalid_1's RMSPE: 0.2463\n",
      "Early stopping, best iteration is:\n",
      "[1592]\ttraining's rmse: 0.000346615\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000365572\tvalid_1's RMSPE: 0.2463\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2463\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001210\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000485651\ttraining's RMSPE: 0.328\tvalid_1's rmse: 0.000491778\tvalid_1's RMSPE: 0.3312\n",
      "[200]\ttraining's rmse: 0.00039485\ttraining's RMSPE: 0.2667\tvalid_1's rmse: 0.000410767\tvalid_1's RMSPE: 0.2767\n",
      "[300]\ttraining's rmse: 0.000374095\ttraining's RMSPE: 0.2526\tvalid_1's rmse: 0.000395055\tvalid_1's RMSPE: 0.2661\n",
      "[400]\ttraining's rmse: 0.00036706\ttraining's RMSPE: 0.2479\tvalid_1's rmse: 0.00039058\tvalid_1's RMSPE: 0.2631\n",
      "[500]\ttraining's rmse: 0.000363148\ttraining's RMSPE: 0.2452\tvalid_1's rmse: 0.000388203\tvalid_1's RMSPE: 0.2615\n",
      "[600]\ttraining's rmse: 0.000360463\ttraining's RMSPE: 0.2434\tvalid_1's rmse: 0.000386327\tvalid_1's RMSPE: 0.2602\n",
      "[700]\ttraining's rmse: 0.000358251\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000385087\tvalid_1's RMSPE: 0.2594\n",
      "[800]\ttraining's rmse: 0.000356429\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000383785\tvalid_1's RMSPE: 0.2585\n",
      "[900]\ttraining's rmse: 0.000354802\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000382853\tvalid_1's RMSPE: 0.2579\n",
      "[1000]\ttraining's rmse: 0.000353391\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000382152\tvalid_1's RMSPE: 0.2574\n",
      "[1100]\ttraining's rmse: 0.000352128\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000381589\tvalid_1's RMSPE: 0.257\n",
      "[1200]\ttraining's rmse: 0.000350958\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000381109\tvalid_1's RMSPE: 0.2567\n",
      "[1300]\ttraining's rmse: 0.000349909\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000380829\tvalid_1's RMSPE: 0.2565\n",
      "[1400]\ttraining's rmse: 0.000348959\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000380492\tvalid_1's RMSPE: 0.2563\n",
      "[1500]\ttraining's rmse: 0.000348021\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.00038005\tvalid_1's RMSPE: 0.256\n",
      "[1600]\ttraining's rmse: 0.000347137\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000379776\tvalid_1's RMSPE: 0.2558\n",
      "Early stopping, best iteration is:\n",
      "[1561]\ttraining's rmse: 0.000347484\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000379851\tvalid_1's RMSPE: 0.2558\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2558\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.001213\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487363\ttraining's RMSPE: 0.3288\tvalid_1's rmse: 0.000489835\tvalid_1's RMSPE: 0.3312\n",
      "[200]\ttraining's rmse: 0.000396987\ttraining's RMSPE: 0.2678\tvalid_1's rmse: 0.000399179\tvalid_1's RMSPE: 0.2699\n",
      "[300]\ttraining's rmse: 0.000375801\ttraining's RMSPE: 0.2535\tvalid_1's rmse: 0.000378847\tvalid_1's RMSPE: 0.2562\n",
      "[400]\ttraining's rmse: 0.000368475\ttraining's RMSPE: 0.2486\tvalid_1's rmse: 0.000372662\tvalid_1's RMSPE: 0.252\n",
      "[500]\ttraining's rmse: 0.000364382\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000369749\tvalid_1's RMSPE: 0.25\n",
      "[600]\ttraining's rmse: 0.00036145\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000367913\tvalid_1's RMSPE: 0.2488\n",
      "[700]\ttraining's rmse: 0.000359124\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000366608\tvalid_1's RMSPE: 0.2479\n",
      "[800]\ttraining's rmse: 0.000357099\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.00036567\tvalid_1's RMSPE: 0.2473\n",
      "[900]\ttraining's rmse: 0.000355414\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000364775\tvalid_1's RMSPE: 0.2467\n",
      "[1000]\ttraining's rmse: 0.000353934\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000364046\tvalid_1's RMSPE: 0.2462\n",
      "[1100]\ttraining's rmse: 0.000352619\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.000363452\tvalid_1's RMSPE: 0.2458\n",
      "[1200]\ttraining's rmse: 0.000351461\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000363042\tvalid_1's RMSPE: 0.2455\n",
      "[1300]\ttraining's rmse: 0.00035035\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.000362702\tvalid_1's RMSPE: 0.2453\n",
      "[1400]\ttraining's rmse: 0.000349337\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000362319\tvalid_1's RMSPE: 0.245\n",
      "[1500]\ttraining's rmse: 0.000348428\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000362077\tvalid_1's RMSPE: 0.2448\n",
      "[1600]\ttraining's rmse: 0.000347576\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000361819\tvalid_1's RMSPE: 0.2447\n",
      "[1700]\ttraining's rmse: 0.000346739\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000361579\tvalid_1's RMSPE: 0.2445\n",
      "[1800]\ttraining's rmse: 0.000345969\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000361384\tvalid_1's RMSPE: 0.2444\n",
      "[1900]\ttraining's rmse: 0.000345238\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000361181\tvalid_1's RMSPE: 0.2442\n",
      "Early stopping, best iteration is:\n",
      "[1898]\ttraining's rmse: 0.000345252\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000361192\tvalid_1's RMSPE: 0.2442\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2442\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 3\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001189\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048677\ttraining's RMSPE: 0.3318\tvalid_1's rmse: 0.000495366\tvalid_1's RMSPE: 0.3353\n",
      "[200]\ttraining's rmse: 0.000393663\ttraining's RMSPE: 0.2683\tvalid_1's rmse: 0.000408835\tvalid_1's RMSPE: 0.2767\n",
      "[300]\ttraining's rmse: 0.000372243\ttraining's RMSPE: 0.2537\tvalid_1's rmse: 0.000389722\tvalid_1's RMSPE: 0.2638\n",
      "[400]\ttraining's rmse: 0.000364986\ttraining's RMSPE: 0.2488\tvalid_1's rmse: 0.000383891\tvalid_1's RMSPE: 0.2598\n",
      "[500]\ttraining's rmse: 0.000360899\ttraining's RMSPE: 0.246\tvalid_1's rmse: 0.000381125\tvalid_1's RMSPE: 0.258\n",
      "[600]\ttraining's rmse: 0.000357982\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000379368\tvalid_1's RMSPE: 0.2568\n",
      "[700]\ttraining's rmse: 0.000355685\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000378035\tvalid_1's RMSPE: 0.2559\n",
      "[800]\ttraining's rmse: 0.000353727\ttraining's RMSPE: 0.2411\tvalid_1's rmse: 0.000376939\tvalid_1's RMSPE: 0.2551\n",
      "[900]\ttraining's rmse: 0.000352036\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000376106\tvalid_1's RMSPE: 0.2546\n",
      "[1000]\ttraining's rmse: 0.000350504\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000375427\tvalid_1's RMSPE: 0.2541\n",
      "[1100]\ttraining's rmse: 0.000349202\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.0003749\tvalid_1's RMSPE: 0.2537\n",
      "[1200]\ttraining's rmse: 0.000348001\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000374383\tvalid_1's RMSPE: 0.2534\n",
      "[1300]\ttraining's rmse: 0.000346914\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000373963\tvalid_1's RMSPE: 0.2531\n",
      "[1400]\ttraining's rmse: 0.00034589\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000373642\tvalid_1's RMSPE: 0.2529\n",
      "[1500]\ttraining's rmse: 0.000344949\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000373346\tvalid_1's RMSPE: 0.2527\n",
      "[1600]\ttraining's rmse: 0.000344072\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000373102\tvalid_1's RMSPE: 0.2525\n",
      "[1700]\ttraining's rmse: 0.000343239\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000372785\tvalid_1's RMSPE: 0.2523\n",
      "[1800]\ttraining's rmse: 0.000342479\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000372676\tvalid_1's RMSPE: 0.2522\n",
      "Early stopping, best iteration is:\n",
      "[1770]\ttraining's rmse: 0.000342707\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000372685\tvalid_1's RMSPE: 0.2522\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2522\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001210\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487054\ttraining's RMSPE: 0.3289\tvalid_1's rmse: 0.000512998\tvalid_1's RMSPE: 0.3601\n",
      "[200]\ttraining's rmse: 0.000396178\ttraining's RMSPE: 0.2675\tvalid_1's rmse: 0.000412949\tvalid_1's RMSPE: 0.2899\n",
      "[300]\ttraining's rmse: 0.000375174\ttraining's RMSPE: 0.2533\tvalid_1's rmse: 0.000388481\tvalid_1's RMSPE: 0.2727\n",
      "[400]\ttraining's rmse: 0.000367989\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.0003794\tvalid_1's RMSPE: 0.2663\n",
      "[500]\ttraining's rmse: 0.000363971\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.00037427\tvalid_1's RMSPE: 0.2627\n",
      "[600]\ttraining's rmse: 0.000361127\ttraining's RMSPE: 0.2438\tvalid_1's rmse: 0.000370721\tvalid_1's RMSPE: 0.2602\n",
      "[700]\ttraining's rmse: 0.00035884\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000368832\tvalid_1's RMSPE: 0.2589\n",
      "[800]\ttraining's rmse: 0.000356894\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000366829\tvalid_1's RMSPE: 0.2575\n",
      "[900]\ttraining's rmse: 0.000355214\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000366466\tvalid_1's RMSPE: 0.2573\n",
      "[1000]\ttraining's rmse: 0.000353745\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000365725\tvalid_1's RMSPE: 0.2567\n",
      "Early stopping, best iteration is:\n",
      "[994]\ttraining's rmse: 0.000353822\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000365728\tvalid_1's RMSPE: 0.2567\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2567\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001187\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486754\ttraining's RMSPE: 0.3321\tvalid_1's rmse: 0.000489243\tvalid_1's RMSPE: 0.3299\n",
      "[200]\ttraining's rmse: 0.000393701\ttraining's RMSPE: 0.2686\tvalid_1's rmse: 0.000400931\tvalid_1's RMSPE: 0.2704\n",
      "[300]\ttraining's rmse: 0.000371939\ttraining's RMSPE: 0.2537\tvalid_1's rmse: 0.000381469\tvalid_1's RMSPE: 0.2573\n",
      "[400]\ttraining's rmse: 0.00036455\ttraining's RMSPE: 0.2487\tvalid_1's rmse: 0.000375568\tvalid_1's RMSPE: 0.2533\n",
      "[500]\ttraining's rmse: 0.000360485\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000372845\tvalid_1's RMSPE: 0.2514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 0.000357596\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000371061\tvalid_1's RMSPE: 0.2502\n",
      "[700]\ttraining's rmse: 0.000355306\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.00036969\tvalid_1's RMSPE: 0.2493\n",
      "[800]\ttraining's rmse: 0.000353332\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000368609\tvalid_1's RMSPE: 0.2486\n",
      "[900]\ttraining's rmse: 0.000351663\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000367783\tvalid_1's RMSPE: 0.248\n",
      "[1000]\ttraining's rmse: 0.000350197\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000367075\tvalid_1's RMSPE: 0.2475\n",
      "[1100]\ttraining's rmse: 0.000348878\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000366459\tvalid_1's RMSPE: 0.2471\n",
      "[1200]\ttraining's rmse: 0.000347704\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000365887\tvalid_1's RMSPE: 0.2467\n",
      "[1300]\ttraining's rmse: 0.000346616\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000365384\tvalid_1's RMSPE: 0.2464\n",
      "[1400]\ttraining's rmse: 0.000345623\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000365049\tvalid_1's RMSPE: 0.2462\n",
      "[1500]\ttraining's rmse: 0.000344674\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000364683\tvalid_1's RMSPE: 0.2459\n",
      "[1600]\ttraining's rmse: 0.000343774\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000364397\tvalid_1's RMSPE: 0.2457\n",
      "[1700]\ttraining's rmse: 0.000342968\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.00036413\tvalid_1's RMSPE: 0.2456\n",
      "[1800]\ttraining's rmse: 0.000342205\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000363943\tvalid_1's RMSPE: 0.2454\n",
      "Early stopping, best iteration is:\n",
      "[1786]\ttraining's rmse: 0.00034232\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000363964\tvalid_1's RMSPE: 0.2454\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2454\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001184\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487382\ttraining's RMSPE: 0.3329\tvalid_1's rmse: 0.000487013\tvalid_1's RMSPE: 0.327\n",
      "[200]\ttraining's rmse: 0.000394668\ttraining's RMSPE: 0.2695\tvalid_1's rmse: 0.000401381\tvalid_1's RMSPE: 0.2695\n",
      "[300]\ttraining's rmse: 0.000372997\ttraining's RMSPE: 0.2547\tvalid_1's rmse: 0.000383893\tvalid_1's RMSPE: 0.2577\n",
      "[400]\ttraining's rmse: 0.000365594\ttraining's RMSPE: 0.2497\tvalid_1's rmse: 0.00037877\tvalid_1's RMSPE: 0.2543\n",
      "[500]\ttraining's rmse: 0.000361427\ttraining's RMSPE: 0.2468\tvalid_1's rmse: 0.000376332\tvalid_1's RMSPE: 0.2527\n",
      "[600]\ttraining's rmse: 0.000358509\ttraining's RMSPE: 0.2448\tvalid_1's rmse: 0.00037502\tvalid_1's RMSPE: 0.2518\n",
      "[700]\ttraining's rmse: 0.000356133\ttraining's RMSPE: 0.2432\tvalid_1's rmse: 0.000373737\tvalid_1's RMSPE: 0.2509\n",
      "[800]\ttraining's rmse: 0.000354151\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000372742\tvalid_1's RMSPE: 0.2503\n",
      "[900]\ttraining's rmse: 0.000352458\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000371857\tvalid_1's RMSPE: 0.2497\n",
      "[1000]\ttraining's rmse: 0.000350957\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000370825\tvalid_1's RMSPE: 0.249\n",
      "[1100]\ttraining's rmse: 0.000349681\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000370242\tvalid_1's RMSPE: 0.2486\n",
      "[1200]\ttraining's rmse: 0.000348474\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000369897\tvalid_1's RMSPE: 0.2483\n",
      "[1300]\ttraining's rmse: 0.000347372\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000369356\tvalid_1's RMSPE: 0.248\n",
      "[1400]\ttraining's rmse: 0.000346309\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000368938\tvalid_1's RMSPE: 0.2477\n",
      "[1500]\ttraining's rmse: 0.000345346\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000368692\tvalid_1's RMSPE: 0.2475\n",
      "[1600]\ttraining's rmse: 0.000344439\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000368459\tvalid_1's RMSPE: 0.2474\n",
      "[1700]\ttraining's rmse: 0.000343605\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000368141\tvalid_1's RMSPE: 0.2472\n",
      "[1800]\ttraining's rmse: 0.000342844\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000367556\tvalid_1's RMSPE: 0.2468\n",
      "[1900]\ttraining's rmse: 0.000342092\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000367468\tvalid_1's RMSPE: 0.2467\n",
      "Early stopping, best iteration is:\n",
      "[1808]\ttraining's rmse: 0.000342777\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000367499\tvalid_1's RMSPE: 0.2467\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2467\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001190\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486123\ttraining's RMSPE: 0.3312\tvalid_1's rmse: 0.000502003\tvalid_1's RMSPE: 0.3404\n",
      "[200]\ttraining's rmse: 0.000393288\ttraining's RMSPE: 0.2679\tvalid_1's rmse: 0.000418895\tvalid_1's RMSPE: 0.2841\n",
      "[300]\ttraining's rmse: 0.000371985\ttraining's RMSPE: 0.2534\tvalid_1's rmse: 0.000401533\tvalid_1's RMSPE: 0.2723\n",
      "[400]\ttraining's rmse: 0.000364821\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.0003956\tvalid_1's RMSPE: 0.2683\n",
      "[500]\ttraining's rmse: 0.000360813\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000392185\tvalid_1's RMSPE: 0.266\n",
      "[600]\ttraining's rmse: 0.000357943\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.00039034\tvalid_1's RMSPE: 0.2647\n",
      "[700]\ttraining's rmse: 0.000355653\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000388892\tvalid_1's RMSPE: 0.2637\n",
      "[800]\ttraining's rmse: 0.000353694\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000387679\tvalid_1's RMSPE: 0.2629\n",
      "[900]\ttraining's rmse: 0.000351973\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000387058\tvalid_1's RMSPE: 0.2625\n",
      "[1000]\ttraining's rmse: 0.000350481\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000386417\tvalid_1's RMSPE: 0.2621\n",
      "[1100]\ttraining's rmse: 0.000349224\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.00038587\tvalid_1's RMSPE: 0.2617\n",
      "[1200]\ttraining's rmse: 0.000348095\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000385468\tvalid_1's RMSPE: 0.2614\n",
      "[1300]\ttraining's rmse: 0.000347002\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.0003851\tvalid_1's RMSPE: 0.2612\n",
      "[1400]\ttraining's rmse: 0.000346014\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000384709\tvalid_1's RMSPE: 0.2609\n",
      "[1500]\ttraining's rmse: 0.000345093\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000383899\tvalid_1's RMSPE: 0.2603\n",
      "[1600]\ttraining's rmse: 0.000344234\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000384043\tvalid_1's RMSPE: 0.2604\n",
      "Early stopping, best iteration is:\n",
      "[1500]\ttraining's rmse: 0.000345093\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000383899\tvalid_1's RMSPE: 0.2603\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2603\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 4\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001189\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487317\ttraining's RMSPE: 0.3322\tvalid_1's rmse: 0.000496218\tvalid_1's RMSPE: 0.336\n",
      "[200]\ttraining's rmse: 0.00039403\ttraining's RMSPE: 0.2686\tvalid_1's rmse: 0.000411238\tvalid_1's RMSPE: 0.2785\n",
      "[300]\ttraining's rmse: 0.000372523\ttraining's RMSPE: 0.2539\tvalid_1's rmse: 0.000393017\tvalid_1's RMSPE: 0.2661\n",
      "[400]\ttraining's rmse: 0.000365331\ttraining's RMSPE: 0.249\tvalid_1's rmse: 0.00038745\tvalid_1's RMSPE: 0.2624\n",
      "[500]\ttraining's rmse: 0.000361336\ttraining's RMSPE: 0.2463\tvalid_1's rmse: 0.000384411\tvalid_1's RMSPE: 0.2603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 0.000358433\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000382387\tvalid_1's RMSPE: 0.2589\n",
      "[700]\ttraining's rmse: 0.000356111\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.00038084\tvalid_1's RMSPE: 0.2579\n",
      "[800]\ttraining's rmse: 0.000354199\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000379597\tvalid_1's RMSPE: 0.257\n",
      "[900]\ttraining's rmse: 0.000352492\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000378553\tvalid_1's RMSPE: 0.2563\n",
      "[1000]\ttraining's rmse: 0.000351016\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000377683\tvalid_1's RMSPE: 0.2557\n",
      "[1100]\ttraining's rmse: 0.000349729\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000376997\tvalid_1's RMSPE: 0.2553\n",
      "[1200]\ttraining's rmse: 0.000348552\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000376487\tvalid_1's RMSPE: 0.2549\n",
      "[1300]\ttraining's rmse: 0.000347435\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000375923\tvalid_1's RMSPE: 0.2546\n",
      "[1400]\ttraining's rmse: 0.000346433\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000375367\tvalid_1's RMSPE: 0.2542\n",
      "[1500]\ttraining's rmse: 0.000345496\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000374949\tvalid_1's RMSPE: 0.2539\n",
      "[1600]\ttraining's rmse: 0.00034461\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000374453\tvalid_1's RMSPE: 0.2536\n",
      "[1700]\ttraining's rmse: 0.000343769\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.00037403\tvalid_1's RMSPE: 0.2533\n",
      "[1800]\ttraining's rmse: 0.000343014\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000373686\tvalid_1's RMSPE: 0.253\n",
      "[1900]\ttraining's rmse: 0.000342286\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000373486\tvalid_1's RMSPE: 0.2529\n",
      "[2000]\ttraining's rmse: 0.000341583\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000373257\tvalid_1's RMSPE: 0.2527\n",
      "[2100]\ttraining's rmse: 0.00034093\ttraining's RMSPE: 0.2324\tvalid_1's rmse: 0.000372997\tvalid_1's RMSPE: 0.2526\n",
      "[2200]\ttraining's rmse: 0.000340264\ttraining's RMSPE: 0.2319\tvalid_1's rmse: 0.000372776\tvalid_1's RMSPE: 0.2524\n",
      "Early stopping, best iteration is:\n",
      "[2169]\ttraining's rmse: 0.00034047\ttraining's RMSPE: 0.2321\tvalid_1's rmse: 0.000372803\tvalid_1's RMSPE: 0.2524\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2524\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001189\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488531\ttraining's RMSPE: 0.333\tvalid_1's rmse: 0.000495106\tvalid_1's RMSPE: 0.3354\n",
      "[200]\ttraining's rmse: 0.000395239\ttraining's RMSPE: 0.2694\tvalid_1's rmse: 0.000410253\tvalid_1's RMSPE: 0.2779\n",
      "[300]\ttraining's rmse: 0.000373355\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000392085\tvalid_1's RMSPE: 0.2656\n",
      "[400]\ttraining's rmse: 0.000365826\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.000386173\tvalid_1's RMSPE: 0.2616\n",
      "[500]\ttraining's rmse: 0.000361618\ttraining's RMSPE: 0.2465\tvalid_1's rmse: 0.000383272\tvalid_1's RMSPE: 0.2596\n",
      "[600]\ttraining's rmse: 0.000358614\ttraining's RMSPE: 0.2444\tvalid_1's rmse: 0.000381726\tvalid_1's RMSPE: 0.2586\n",
      "[700]\ttraining's rmse: 0.000356228\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000380476\tvalid_1's RMSPE: 0.2577\n",
      "[800]\ttraining's rmse: 0.000354229\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000379338\tvalid_1's RMSPE: 0.257\n",
      "[900]\ttraining's rmse: 0.000352477\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000378361\tvalid_1's RMSPE: 0.2563\n",
      "[1000]\ttraining's rmse: 0.000350956\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000377745\tvalid_1's RMSPE: 0.2559\n",
      "[1100]\ttraining's rmse: 0.000349563\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000377123\tvalid_1's RMSPE: 0.2555\n",
      "[1200]\ttraining's rmse: 0.000348343\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.00037659\tvalid_1's RMSPE: 0.2551\n",
      "[1300]\ttraining's rmse: 0.000347247\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.00037614\tvalid_1's RMSPE: 0.2548\n",
      "[1400]\ttraining's rmse: 0.000346247\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000376047\tvalid_1's RMSPE: 0.2547\n",
      "Early stopping, best iteration is:\n",
      "[1316]\ttraining's rmse: 0.000347083\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000376056\tvalid_1's RMSPE: 0.2547\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2547\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001184\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487581\ttraining's RMSPE: 0.3331\tvalid_1's rmse: 0.000487734\tvalid_1's RMSPE: 0.3273\n",
      "[200]\ttraining's rmse: 0.000394382\ttraining's RMSPE: 0.2694\tvalid_1's rmse: 0.000400676\tvalid_1's RMSPE: 0.2689\n",
      "[300]\ttraining's rmse: 0.000372392\ttraining's RMSPE: 0.2544\tvalid_1's rmse: 0.000381732\tvalid_1's RMSPE: 0.2562\n",
      "[400]\ttraining's rmse: 0.000364763\ttraining's RMSPE: 0.2492\tvalid_1's rmse: 0.000375916\tvalid_1's RMSPE: 0.2523\n",
      "[500]\ttraining's rmse: 0.000360515\ttraining's RMSPE: 0.2463\tvalid_1's rmse: 0.000373011\tvalid_1's RMSPE: 0.2503\n",
      "[600]\ttraining's rmse: 0.000357527\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000371219\tvalid_1's RMSPE: 0.2491\n",
      "[700]\ttraining's rmse: 0.000355184\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000369916\tvalid_1's RMSPE: 0.2483\n",
      "[800]\ttraining's rmse: 0.000353184\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000368835\tvalid_1's RMSPE: 0.2475\n",
      "[900]\ttraining's rmse: 0.000351433\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000367968\tvalid_1's RMSPE: 0.247\n",
      "[1000]\ttraining's rmse: 0.000349911\ttraining's RMSPE: 0.239\tvalid_1's rmse: 0.000367251\tvalid_1's RMSPE: 0.2465\n",
      "[1100]\ttraining's rmse: 0.00034858\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000366661\tvalid_1's RMSPE: 0.2461\n",
      "[1200]\ttraining's rmse: 0.000347407\ttraining's RMSPE: 0.2373\tvalid_1's rmse: 0.00036614\tvalid_1's RMSPE: 0.2457\n",
      "[1300]\ttraining's rmse: 0.000346273\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.00036569\tvalid_1's RMSPE: 0.2454\n",
      "[1400]\ttraining's rmse: 0.000345262\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000365265\tvalid_1's RMSPE: 0.2451\n",
      "[1500]\ttraining's rmse: 0.000344296\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000364947\tvalid_1's RMSPE: 0.2449\n",
      "[1600]\ttraining's rmse: 0.000343452\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000364672\tvalid_1's RMSPE: 0.2447\n",
      "[1700]\ttraining's rmse: 0.000342613\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.00036444\tvalid_1's RMSPE: 0.2446\n",
      "[1800]\ttraining's rmse: 0.000341808\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.0003642\tvalid_1's RMSPE: 0.2444\n",
      "[1900]\ttraining's rmse: 0.000341045\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000364039\tvalid_1's RMSPE: 0.2443\n",
      "[2000]\ttraining's rmse: 0.000340311\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000363851\tvalid_1's RMSPE: 0.2442\n",
      "[2100]\ttraining's rmse: 0.000339664\ttraining's RMSPE: 0.232\tvalid_1's rmse: 0.000363647\tvalid_1's RMSPE: 0.2441\n",
      "[2200]\ttraining's rmse: 0.000339014\ttraining's RMSPE: 0.2316\tvalid_1's rmse: 0.000363492\tvalid_1's RMSPE: 0.244\n",
      "[2300]\ttraining's rmse: 0.000338387\ttraining's RMSPE: 0.2312\tvalid_1's rmse: 0.000363315\tvalid_1's RMSPE: 0.2438\n",
      "[2400]\ttraining's rmse: 0.000337758\ttraining's RMSPE: 0.2307\tvalid_1's rmse: 0.000363166\tvalid_1's RMSPE: 0.2437\n",
      "Early stopping, best iteration is:\n",
      "[2380]\ttraining's rmse: 0.000337883\ttraining's RMSPE: 0.2308\tvalid_1's rmse: 0.000363187\tvalid_1's RMSPE: 0.2437\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2437\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001191\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487023\ttraining's RMSPE: 0.3316\tvalid_1's rmse: 0.000496683\tvalid_1's RMSPE: 0.3378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.000394178\ttraining's RMSPE: 0.2684\tvalid_1's rmse: 0.000411414\tvalid_1's RMSPE: 0.2798\n",
      "[300]\ttraining's rmse: 0.000372668\ttraining's RMSPE: 0.2537\tvalid_1's rmse: 0.000392565\tvalid_1's RMSPE: 0.267\n",
      "[400]\ttraining's rmse: 0.000365436\ttraining's RMSPE: 0.2488\tvalid_1's rmse: 0.000386465\tvalid_1's RMSPE: 0.2628\n",
      "[500]\ttraining's rmse: 0.000361486\ttraining's RMSPE: 0.2461\tvalid_1's rmse: 0.000383616\tvalid_1's RMSPE: 0.2609\n",
      "[600]\ttraining's rmse: 0.00035867\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.00038171\tvalid_1's RMSPE: 0.2596\n",
      "[700]\ttraining's rmse: 0.000356256\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000380249\tvalid_1's RMSPE: 0.2586\n",
      "[800]\ttraining's rmse: 0.000354291\ttraining's RMSPE: 0.2412\tvalid_1's rmse: 0.000379188\tvalid_1's RMSPE: 0.2579\n",
      "[900]\ttraining's rmse: 0.000352574\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000378196\tvalid_1's RMSPE: 0.2572\n",
      "[1000]\ttraining's rmse: 0.000351066\ttraining's RMSPE: 0.239\tvalid_1's rmse: 0.000377454\tvalid_1's RMSPE: 0.2567\n",
      "[1100]\ttraining's rmse: 0.000349718\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000376907\tvalid_1's RMSPE: 0.2563\n",
      "[1200]\ttraining's rmse: 0.000348511\ttraining's RMSPE: 0.2373\tvalid_1's rmse: 0.000376394\tvalid_1's RMSPE: 0.256\n",
      "[1300]\ttraining's rmse: 0.000347394\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000376078\tvalid_1's RMSPE: 0.2558\n",
      "[1400]\ttraining's rmse: 0.000346369\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000375567\tvalid_1's RMSPE: 0.2554\n",
      "[1500]\ttraining's rmse: 0.000345384\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000375116\tvalid_1's RMSPE: 0.2551\n",
      "[1600]\ttraining's rmse: 0.000344483\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000374896\tvalid_1's RMSPE: 0.255\n",
      "[1700]\ttraining's rmse: 0.000343655\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000374779\tvalid_1's RMSPE: 0.2549\n",
      "Early stopping, best iteration is:\n",
      "[1618]\ttraining's rmse: 0.000344328\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000374868\tvalid_1's RMSPE: 0.2549\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2549\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001208\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048723\ttraining's RMSPE: 0.3296\tvalid_1's rmse: 0.000515062\tvalid_1's RMSPE: 0.3593\n",
      "[200]\ttraining's rmse: 0.000396425\ttraining's RMSPE: 0.2682\tvalid_1's rmse: 0.000416429\tvalid_1's RMSPE: 0.2905\n",
      "[300]\ttraining's rmse: 0.000375343\ttraining's RMSPE: 0.2539\tvalid_1's rmse: 0.000392182\tvalid_1's RMSPE: 0.2736\n",
      "[400]\ttraining's rmse: 0.000368126\ttraining's RMSPE: 0.249\tvalid_1's rmse: 0.000384344\tvalid_1's RMSPE: 0.2681\n",
      "[500]\ttraining's rmse: 0.000363958\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000379852\tvalid_1's RMSPE: 0.265\n",
      "[600]\ttraining's rmse: 0.000360987\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000376493\tvalid_1's RMSPE: 0.2626\n",
      "[700]\ttraining's rmse: 0.000358673\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000374843\tvalid_1's RMSPE: 0.2615\n",
      "[800]\ttraining's rmse: 0.000356666\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000374178\tvalid_1's RMSPE: 0.261\n",
      "[900]\ttraining's rmse: 0.000354917\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000373005\tvalid_1's RMSPE: 0.2602\n",
      "[1000]\ttraining's rmse: 0.000353385\ttraining's RMSPE: 0.239\tvalid_1's rmse: 0.000372375\tvalid_1's RMSPE: 0.2598\n",
      "[1100]\ttraining's rmse: 0.000351984\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000371963\tvalid_1's RMSPE: 0.2595\n",
      "[1200]\ttraining's rmse: 0.000350713\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.00037121\tvalid_1's RMSPE: 0.259\n",
      "[1300]\ttraining's rmse: 0.000349567\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000370714\tvalid_1's RMSPE: 0.2586\n",
      "[1400]\ttraining's rmse: 0.000348522\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000370131\tvalid_1's RMSPE: 0.2582\n",
      "[1500]\ttraining's rmse: 0.000347586\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000369687\tvalid_1's RMSPE: 0.2579\n",
      "[1600]\ttraining's rmse: 0.000346671\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000369779\tvalid_1's RMSPE: 0.258\n",
      "Early stopping, best iteration is:\n",
      "[1576]\ttraining's rmse: 0.000346882\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000369473\tvalid_1's RMSPE: 0.2577\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2577\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 5\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001190\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486551\ttraining's RMSPE: 0.3316\tvalid_1's rmse: 0.000494563\tvalid_1's RMSPE: 0.3355\n",
      "[200]\ttraining's rmse: 0.000393386\ttraining's RMSPE: 0.2681\tvalid_1's rmse: 0.000410175\tvalid_1's RMSPE: 0.2783\n",
      "[300]\ttraining's rmse: 0.000371502\ttraining's RMSPE: 0.2532\tvalid_1's rmse: 0.000392426\tvalid_1's RMSPE: 0.2662\n",
      "[400]\ttraining's rmse: 0.000364109\ttraining's RMSPE: 0.2482\tvalid_1's rmse: 0.000386722\tvalid_1's RMSPE: 0.2623\n",
      "[500]\ttraining's rmse: 0.000360126\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000383749\tvalid_1's RMSPE: 0.2603\n",
      "[600]\ttraining's rmse: 0.000357227\ttraining's RMSPE: 0.2435\tvalid_1's rmse: 0.000382292\tvalid_1's RMSPE: 0.2593\n",
      "[700]\ttraining's rmse: 0.000354959\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000381118\tvalid_1's RMSPE: 0.2585\n",
      "[800]\ttraining's rmse: 0.000353055\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000380128\tvalid_1's RMSPE: 0.2579\n",
      "[900]\ttraining's rmse: 0.000351383\ttraining's RMSPE: 0.2395\tvalid_1's rmse: 0.000379293\tvalid_1's RMSPE: 0.2573\n",
      "[1000]\ttraining's rmse: 0.000349874\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000378006\tvalid_1's RMSPE: 0.2564\n",
      "[1100]\ttraining's rmse: 0.000348559\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000377258\tvalid_1's RMSPE: 0.2559\n",
      "[1200]\ttraining's rmse: 0.000347382\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000376588\tvalid_1's RMSPE: 0.2555\n",
      "[1300]\ttraining's rmse: 0.000346274\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000375978\tvalid_1's RMSPE: 0.2551\n",
      "[1400]\ttraining's rmse: 0.000345263\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000375217\tvalid_1's RMSPE: 0.2545\n",
      "[1500]\ttraining's rmse: 0.000344304\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000374944\tvalid_1's RMSPE: 0.2544\n",
      "[1600]\ttraining's rmse: 0.000343462\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000374537\tvalid_1's RMSPE: 0.2541\n",
      "[1700]\ttraining's rmse: 0.000342661\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000374267\tvalid_1's RMSPE: 0.2539\n",
      "Early stopping, best iteration is:\n",
      "[1658]\ttraining's rmse: 0.000342985\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000374342\tvalid_1's RMSPE: 0.2539\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2539\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001211\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486158\ttraining's RMSPE: 0.3284\tvalid_1's rmse: 0.000508541\tvalid_1's RMSPE: 0.3572\n",
      "[200]\ttraining's rmse: 0.000395697\ttraining's RMSPE: 0.2673\tvalid_1's rmse: 0.000411657\tvalid_1's RMSPE: 0.2892\n",
      "[300]\ttraining's rmse: 0.000374893\ttraining's RMSPE: 0.2532\tvalid_1's rmse: 0.000387848\tvalid_1's RMSPE: 0.2725\n",
      "[400]\ttraining's rmse: 0.000367694\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000379504\tvalid_1's RMSPE: 0.2666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 0.000363726\ttraining's RMSPE: 0.2457\tvalid_1's rmse: 0.000374946\tvalid_1's RMSPE: 0.2634\n",
      "[600]\ttraining's rmse: 0.000360807\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000372133\tvalid_1's RMSPE: 0.2614\n",
      "[700]\ttraining's rmse: 0.00035853\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000370018\tvalid_1's RMSPE: 0.2599\n",
      "[800]\ttraining's rmse: 0.000356555\ttraining's RMSPE: 0.2408\tvalid_1's rmse: 0.00036843\tvalid_1's RMSPE: 0.2588\n",
      "[900]\ttraining's rmse: 0.000354884\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000367842\tvalid_1's RMSPE: 0.2584\n",
      "[1000]\ttraining's rmse: 0.000353369\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000367066\tvalid_1's RMSPE: 0.2579\n",
      "[1100]\ttraining's rmse: 0.000352066\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000366405\tvalid_1's RMSPE: 0.2574\n",
      "[1200]\ttraining's rmse: 0.000350844\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000365701\tvalid_1's RMSPE: 0.2569\n",
      "[1300]\ttraining's rmse: 0.000349726\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000365078\tvalid_1's RMSPE: 0.2565\n",
      "[1400]\ttraining's rmse: 0.000348699\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000364829\tvalid_1's RMSPE: 0.2563\n",
      "[1500]\ttraining's rmse: 0.000347805\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000364897\tvalid_1's RMSPE: 0.2563\n",
      "Early stopping, best iteration is:\n",
      "[1410]\ttraining's rmse: 0.0003486\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000364782\tvalid_1's RMSPE: 0.2562\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2562\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274426, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001185\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000485433\ttraining's RMSPE: 0.3318\tvalid_1's rmse: 0.000491756\tvalid_1's RMSPE: 0.3298\n",
      "[200]\ttraining's rmse: 0.000392233\ttraining's RMSPE: 0.2681\tvalid_1's rmse: 0.000405091\tvalid_1's RMSPE: 0.2717\n",
      "[300]\ttraining's rmse: 0.00037042\ttraining's RMSPE: 0.2532\tvalid_1's rmse: 0.000386162\tvalid_1's RMSPE: 0.259\n",
      "[400]\ttraining's rmse: 0.000363098\ttraining's RMSPE: 0.2482\tvalid_1's rmse: 0.000380493\tvalid_1's RMSPE: 0.2552\n",
      "[500]\ttraining's rmse: 0.000359066\ttraining's RMSPE: 0.2454\tvalid_1's rmse: 0.000377531\tvalid_1's RMSPE: 0.2532\n",
      "[600]\ttraining's rmse: 0.000356258\ttraining's RMSPE: 0.2435\tvalid_1's rmse: 0.000375534\tvalid_1's RMSPE: 0.2519\n",
      "[700]\ttraining's rmse: 0.000353939\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.00037413\tvalid_1's RMSPE: 0.2509\n",
      "[800]\ttraining's rmse: 0.000351973\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000372836\tvalid_1's RMSPE: 0.2501\n",
      "[900]\ttraining's rmse: 0.000350332\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000371946\tvalid_1's RMSPE: 0.2495\n",
      "[1000]\ttraining's rmse: 0.000348848\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000371078\tvalid_1's RMSPE: 0.2489\n",
      "[1100]\ttraining's rmse: 0.000347512\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.00037045\tvalid_1's RMSPE: 0.2485\n",
      "[1200]\ttraining's rmse: 0.000346324\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000369895\tvalid_1's RMSPE: 0.2481\n",
      "[1300]\ttraining's rmse: 0.00034524\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000369474\tvalid_1's RMSPE: 0.2478\n",
      "[1400]\ttraining's rmse: 0.000344246\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000369154\tvalid_1's RMSPE: 0.2476\n",
      "[1500]\ttraining's rmse: 0.000343322\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000368714\tvalid_1's RMSPE: 0.2473\n",
      "[1600]\ttraining's rmse: 0.000342504\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000368446\tvalid_1's RMSPE: 0.2471\n",
      "[1700]\ttraining's rmse: 0.000341685\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000368211\tvalid_1's RMSPE: 0.247\n",
      "[1800]\ttraining's rmse: 0.000340911\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000367962\tvalid_1's RMSPE: 0.2468\n",
      "[1900]\ttraining's rmse: 0.000340179\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000367792\tvalid_1's RMSPE: 0.2467\n",
      "[2000]\ttraining's rmse: 0.000339476\ttraining's RMSPE: 0.232\tvalid_1's rmse: 0.000367512\tvalid_1's RMSPE: 0.2465\n",
      "[2100]\ttraining's rmse: 0.000338841\ttraining's RMSPE: 0.2316\tvalid_1's rmse: 0.000367298\tvalid_1's RMSPE: 0.2464\n",
      "[2200]\ttraining's rmse: 0.000338197\ttraining's RMSPE: 0.2311\tvalid_1's rmse: 0.000367188\tvalid_1's RMSPE: 0.2463\n",
      "Early stopping, best iteration is:\n",
      "[2125]\ttraining's rmse: 0.000338686\ttraining's RMSPE: 0.2315\tvalid_1's rmse: 0.000367279\tvalid_1's RMSPE: 0.2463\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2463\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001190\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486692\ttraining's RMSPE: 0.3316\tvalid_1's rmse: 0.000494353\tvalid_1's RMSPE: 0.3358\n",
      "[200]\ttraining's rmse: 0.000393742\ttraining's RMSPE: 0.2683\tvalid_1's rmse: 0.000406651\tvalid_1's RMSPE: 0.2762\n",
      "[300]\ttraining's rmse: 0.00037205\ttraining's RMSPE: 0.2535\tvalid_1's rmse: 0.000386952\tvalid_1's RMSPE: 0.2629\n",
      "[400]\ttraining's rmse: 0.000364693\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.000381045\tvalid_1's RMSPE: 0.2588\n",
      "[500]\ttraining's rmse: 0.000360699\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000378283\tvalid_1's RMSPE: 0.257\n",
      "[600]\ttraining's rmse: 0.000357865\ttraining's RMSPE: 0.2438\tvalid_1's rmse: 0.000376495\tvalid_1's RMSPE: 0.2557\n",
      "[700]\ttraining's rmse: 0.00035553\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000375077\tvalid_1's RMSPE: 0.2548\n",
      "[800]\ttraining's rmse: 0.000353591\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000373995\tvalid_1's RMSPE: 0.254\n",
      "[900]\ttraining's rmse: 0.000351969\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000373143\tvalid_1's RMSPE: 0.2535\n",
      "[1000]\ttraining's rmse: 0.00035053\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000372631\tvalid_1's RMSPE: 0.2531\n",
      "[1100]\ttraining's rmse: 0.000349266\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.00037224\tvalid_1's RMSPE: 0.2529\n",
      "[1200]\ttraining's rmse: 0.000348104\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000371777\tvalid_1's RMSPE: 0.2525\n",
      "[1300]\ttraining's rmse: 0.000347061\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000371343\tvalid_1's RMSPE: 0.2522\n",
      "[1400]\ttraining's rmse: 0.000346054\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000370947\tvalid_1's RMSPE: 0.252\n",
      "[1500]\ttraining's rmse: 0.000345155\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000370533\tvalid_1's RMSPE: 0.2517\n",
      "[1600]\ttraining's rmse: 0.000344293\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000370276\tvalid_1's RMSPE: 0.2515\n",
      "[1700]\ttraining's rmse: 0.00034346\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000370073\tvalid_1's RMSPE: 0.2514\n",
      "[1800]\ttraining's rmse: 0.000342703\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000369809\tvalid_1's RMSPE: 0.2512\n",
      "Early stopping, best iteration is:\n",
      "[1750]\ttraining's rmse: 0.000343068\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000369872\tvalid_1's RMSPE: 0.2512\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2512\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274427, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001186\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048683\ttraining's RMSPE: 0.3324\tvalid_1's rmse: 0.000489909\tvalid_1's RMSPE: 0.3299\n",
      "[200]\ttraining's rmse: 0.00039373\ttraining's RMSPE: 0.2688\tvalid_1's rmse: 0.000401975\tvalid_1's RMSPE: 0.2707\n",
      "[300]\ttraining's rmse: 0.000371875\ttraining's RMSPE: 0.2539\tvalid_1's rmse: 0.000382564\tvalid_1's RMSPE: 0.2576\n",
      "[400]\ttraining's rmse: 0.000364504\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.000376458\tvalid_1's RMSPE: 0.2535\n",
      "[500]\ttraining's rmse: 0.000360454\ttraining's RMSPE: 0.2461\tvalid_1's rmse: 0.000373428\tvalid_1's RMSPE: 0.2514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 0.000357554\ttraining's RMSPE: 0.2441\tvalid_1's rmse: 0.000371397\tvalid_1's RMSPE: 0.2501\n",
      "[700]\ttraining's rmse: 0.000355244\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000369803\tvalid_1's RMSPE: 0.249\n",
      "[800]\ttraining's rmse: 0.000353315\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000368612\tvalid_1's RMSPE: 0.2482\n",
      "[900]\ttraining's rmse: 0.000351618\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000367651\tvalid_1's RMSPE: 0.2475\n",
      "[1000]\ttraining's rmse: 0.000350151\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000366735\tvalid_1's RMSPE: 0.2469\n",
      "[1100]\ttraining's rmse: 0.000348837\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000366119\tvalid_1's RMSPE: 0.2465\n",
      "[1200]\ttraining's rmse: 0.000347654\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000365585\tvalid_1's RMSPE: 0.2462\n",
      "[1300]\ttraining's rmse: 0.000346577\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000365212\tvalid_1's RMSPE: 0.2459\n",
      "[1400]\ttraining's rmse: 0.000345605\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000364815\tvalid_1's RMSPE: 0.2456\n",
      "[1500]\ttraining's rmse: 0.000344642\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000364446\tvalid_1's RMSPE: 0.2454\n",
      "[1600]\ttraining's rmse: 0.000343764\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000364076\tvalid_1's RMSPE: 0.2451\n",
      "[1700]\ttraining's rmse: 0.000342975\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000363804\tvalid_1's RMSPE: 0.245\n",
      "[1800]\ttraining's rmse: 0.000342207\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000363596\tvalid_1's RMSPE: 0.2448\n",
      "[1900]\ttraining's rmse: 0.000341501\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000363383\tvalid_1's RMSPE: 0.2447\n",
      "[2000]\ttraining's rmse: 0.000340816\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000363197\tvalid_1's RMSPE: 0.2446\n",
      "[2100]\ttraining's rmse: 0.00034011\ttraining's RMSPE: 0.2322\tvalid_1's rmse: 0.000363006\tvalid_1's RMSPE: 0.2444\n",
      "[2200]\ttraining's rmse: 0.00033947\ttraining's RMSPE: 0.2318\tvalid_1's rmse: 0.000362815\tvalid_1's RMSPE: 0.2443\n",
      "[2300]\ttraining's rmse: 0.000338827\ttraining's RMSPE: 0.2314\tvalid_1's rmse: 0.000362679\tvalid_1's RMSPE: 0.2442\n",
      "Early stopping, best iteration is:\n",
      "[2260]\ttraining's rmse: 0.000339085\ttraining's RMSPE: 0.2315\tvalid_1's rmse: 0.000362751\tvalid_1's RMSPE: 0.2442\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2442\n",
      "\t**********************************************************************\n",
      "Wall time: 23min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPSILON = 0\n",
    "simple_test = pd.DataFrame(\n",
    "    {'target_realized_volatility':[],'predicted_volatility_simple': [], 'time_id':[], 'stock_id':[]}\n",
    "\n",
    ")\n",
    "simple_models = []\n",
    "split_importance = []\n",
    "gain_importance = []\n",
    "train_scores = []\n",
    "inner_k = 5\n",
    "outer_k = 5\n",
    "\n",
    "params =  {\n",
    "    'boosting_type': 'goss',\n",
    "    'learning_rate': 0.01,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8, \n",
    "    'bagging_fraction': 0.8,\n",
    "    'lambda_l1': 1.2,\n",
    "    'lambda_l2': 1.2,\n",
    "    'n_jobs': -1,\n",
    "    'force_col_wise': True,\n",
    "    'extra_trees': True,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "outer_kfold = KFold(n_splits=outer_k, random_state=42, shuffle=True)\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_kfold.split(simpleX, simpleY)):\n",
    "    print('*'*120)\n",
    "    print(\"Outer Fold :\", outer_fold + 1)\n",
    "    print('*'*120)\n",
    "\n",
    "    X_outer_train = simpleX.iloc[outer_train_idx].reset_index(drop=True)  \n",
    "    X_outer_test = simpleX.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    y_outer_train = simpleY.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    y_outer_test = simpleY.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    \n",
    "    target = np.zeros(len(y_outer_test))\n",
    "    inner_scores = 0.0\n",
    "    models = []\n",
    "    \n",
    "    inner_kfold = KFold(n_splits= inner_k, random_state=42, shuffle=True)\n",
    "    for inner_fold, (inner_train_idx, inner_valid_idx) in enumerate(inner_kfold.split(X_outer_train, y_outer_train)):\n",
    "        print(\"\\n\\t\"+\"*\"*20)\n",
    "        print(f\"\\t*  Inner Fold : {inner_fold + 1}  *\")\n",
    "        print(\"\\t\"+\"*\"*20+\"\\n\")\n",
    "    \n",
    "        # inner train data and valid data\n",
    "        X_inner_train = X_outer_train.iloc[inner_train_idx].reset_index(drop=True)\n",
    "        X_inner_valid = X_outer_train.iloc[inner_valid_idx].reset_index(drop=True)\n",
    "\n",
    "        y_inner_train = y_outer_train.iloc[inner_train_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "        y_inner_valid = y_outer_train.iloc[inner_valid_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "            \n",
    "        lgbm_train = lgbm.Dataset(X_inner_train,y_inner_train,weight=1/(np.square(y_inner_train.values)+EPSILON))\n",
    "        lgbm_valid = lgbm.Dataset(\n",
    "            X_inner_valid,y_inner_valid,reference=lgbm_train,weight=1/(np.square(y_inner_valid.values)+EPSILON))\n",
    "        \n",
    "        # model training\n",
    "        model = lgbm.train(\n",
    "            params=params, #tuner.best_params,\n",
    "            train_set=lgbm_train,\n",
    "            valid_sets=[lgbm_train, lgbm_valid],\n",
    "            num_boost_round=10000,       \n",
    "            feval=feval_RMSPE,\n",
    "            callbacks=[lgbm.log_evaluation(period=100), lgbm.early_stopping(100)]\n",
    "        )\n",
    "        # validation \n",
    "        y_inner_pred = model.predict(X_inner_valid, num_iteration=model.best_iteration)\n",
    "        RMSPE = rmspe(\n",
    "            y_true=(y_inner_valid.values.flatten()), \n",
    "            y_pred=(y_inner_pred), n=4\n",
    "        )\n",
    "        \n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "        print(f'\\tInner Validation RMSPE: \\t{RMSPE}')\n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "\n",
    "        # keep training validation score\n",
    "        inner_scores += RMSPE / inner_k\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "        # record feature importances by gain and split\n",
    "        features = list(X_inner_train.columns.values)\n",
    "        \n",
    "        gain_importance.append(compute_importance(model, features, typ='gain'))\n",
    "        split_importance.append(compute_importance(model, features, typ='split'))\n",
    "        \n",
    "    # store all models for prediction in oof evaluation\n",
    "    simple_models.append(models)\n",
    "    train_scores.append(inner_scores)\n",
    "    \n",
    "    # out of fold test set\n",
    "    for model in simple_models[outer_fold]:\n",
    "        y_outer_pred = model.predict(X_outer_test,num_iteration=model.best_iteration)\n",
    "        target += y_outer_pred / len(simple_models[outer_fold])\n",
    "    \n",
    "    y_outer_test = y_outer_test.assign(predicted_volatility_simple = target)\n",
    " \n",
    "    simple_test = pd.concat([simple_test, y_outer_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2eced56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.251854"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(simple_test['target_realized_volatility'], simple_test['predicted_volatility_simple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70a5b13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>predicted_volatility_simple</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>32653.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  predicted_volatility_simple  time_id  \\\n",
       "0                         0.003397                     0.002806    103.0   \n",
       "1                         0.002993                     0.004295    146.0   \n",
       "2                         0.001094                     0.001244    250.0   \n",
       "3                         0.001711                     0.001679    297.0   \n",
       "4                         0.001197                     0.002181    319.0   \n",
       "...                            ...                          ...      ...   \n",
       "428786                    0.001746                     0.002272  32653.0   \n",
       "428787                    0.003511                     0.004173  32724.0   \n",
       "428788                    0.010431                     0.010131  32746.0   \n",
       "428789                    0.001827                     0.002191  32750.0   \n",
       "428790                    0.003454                     0.002073  32753.0   \n",
       "\n",
       "        stock_id  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "...          ...  \n",
       "428786     126.0  \n",
       "428787     126.0  \n",
       "428788     126.0  \n",
       "428789     126.0  \n",
       "428790     126.0  \n",
       "\n",
       "[428791 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e7bcd",
   "metadata": {},
   "source": [
    "### Train Extend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45446567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "Outer Fold : 1\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136509, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001202\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516669\ttraining's RMSPE: 0.3441\tvalid_1's rmse: 0.000524894\tvalid_1's RMSPE: 0.3445\n",
      "[200]\ttraining's rmse: 0.000418333\ttraining's RMSPE: 0.2786\tvalid_1's rmse: 0.000435921\tvalid_1's RMSPE: 0.2861\n",
      "[300]\ttraining's rmse: 0.000393334\ttraining's RMSPE: 0.2619\tvalid_1's rmse: 0.000415269\tvalid_1's RMSPE: 0.2726\n",
      "[400]\ttraining's rmse: 0.000383688\ttraining's RMSPE: 0.2555\tvalid_1's rmse: 0.000408104\tvalid_1's RMSPE: 0.2679\n",
      "[500]\ttraining's rmse: 0.000377941\ttraining's RMSPE: 0.2517\tvalid_1's rmse: 0.000404277\tvalid_1's RMSPE: 0.2654\n",
      "[600]\ttraining's rmse: 0.000373513\ttraining's RMSPE: 0.2487\tvalid_1's rmse: 0.000401797\tvalid_1's RMSPE: 0.2637\n",
      "[700]\ttraining's rmse: 0.000369813\ttraining's RMSPE: 0.2463\tvalid_1's rmse: 0.000400049\tvalid_1's RMSPE: 0.2626\n",
      "[800]\ttraining's rmse: 0.000366719\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000398919\tvalid_1's RMSPE: 0.2618\n",
      "[900]\ttraining's rmse: 0.000364086\ttraining's RMSPE: 0.2425\tvalid_1's rmse: 0.000398008\tvalid_1's RMSPE: 0.2613\n",
      "[1000]\ttraining's rmse: 0.00036168\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000397376\tvalid_1's RMSPE: 0.2608\n",
      "[1100]\ttraining's rmse: 0.000359461\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000396682\tvalid_1's RMSPE: 0.2604\n",
      "[1200]\ttraining's rmse: 0.000357432\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.0003963\tvalid_1's RMSPE: 0.2601\n",
      "[1300]\ttraining's rmse: 0.000355643\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000395744\tvalid_1's RMSPE: 0.2598\n",
      "[1400]\ttraining's rmse: 0.000354009\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.00039542\tvalid_1's RMSPE: 0.2596\n",
      "[1500]\ttraining's rmse: 0.000352409\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000395107\tvalid_1's RMSPE: 0.2593\n",
      "[1600]\ttraining's rmse: 0.000350908\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000394928\tvalid_1's RMSPE: 0.2592\n",
      "[1700]\ttraining's rmse: 0.000349499\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000394685\tvalid_1's RMSPE: 0.2591\n",
      "Early stopping, best iteration is:\n",
      "[1622]\ttraining's rmse: 0.000350569\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000394805\tvalid_1's RMSPE: 0.2591\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2591\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136509, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001199\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516418\ttraining's RMSPE: 0.3444\tvalid_1's rmse: 0.000523585\tvalid_1's RMSPE: 0.3415\n",
      "[200]\ttraining's rmse: 0.000417683\ttraining's RMSPE: 0.2786\tvalid_1's rmse: 0.000435381\tvalid_1's RMSPE: 0.284\n",
      "[300]\ttraining's rmse: 0.000392134\ttraining's RMSPE: 0.2615\tvalid_1's rmse: 0.000414318\tvalid_1's RMSPE: 0.2702\n",
      "[400]\ttraining's rmse: 0.000382267\ttraining's RMSPE: 0.255\tvalid_1's rmse: 0.000407056\tvalid_1's RMSPE: 0.2655\n",
      "[500]\ttraining's rmse: 0.000376396\ttraining's RMSPE: 0.251\tvalid_1's rmse: 0.000403058\tvalid_1's RMSPE: 0.2629\n",
      "[600]\ttraining's rmse: 0.000371929\ttraining's RMSPE: 0.2481\tvalid_1's rmse: 0.000400237\tvalid_1's RMSPE: 0.2611\n",
      "[700]\ttraining's rmse: 0.000368179\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.00039799\tvalid_1's RMSPE: 0.2596\n",
      "[800]\ttraining's rmse: 0.000365144\ttraining's RMSPE: 0.2435\tvalid_1's rmse: 0.000396156\tvalid_1's RMSPE: 0.2584\n",
      "[900]\ttraining's rmse: 0.000362517\ttraining's RMSPE: 0.2418\tvalid_1's rmse: 0.000394789\tvalid_1's RMSPE: 0.2575\n",
      "[1000]\ttraining's rmse: 0.00036018\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000393834\tvalid_1's RMSPE: 0.2569\n",
      "[1100]\ttraining's rmse: 0.000358024\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000392751\tvalid_1's RMSPE: 0.2562\n",
      "[1200]\ttraining's rmse: 0.000356092\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000391971\tvalid_1's RMSPE: 0.2557\n",
      "[1300]\ttraining's rmse: 0.000354274\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000391177\tvalid_1's RMSPE: 0.2551\n",
      "[1400]\ttraining's rmse: 0.000352605\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000390454\tvalid_1's RMSPE: 0.2547\n",
      "[1500]\ttraining's rmse: 0.00035108\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000389907\tvalid_1's RMSPE: 0.2543\n",
      "[1600]\ttraining's rmse: 0.000349647\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000389519\tvalid_1's RMSPE: 0.2541\n",
      "[1700]\ttraining's rmse: 0.00034832\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000389204\tvalid_1's RMSPE: 0.2539\n",
      "[1800]\ttraining's rmse: 0.0003471\ttraining's RMSPE: 0.2315\tvalid_1's rmse: 0.000388814\tvalid_1's RMSPE: 0.2536\n",
      "[1900]\ttraining's rmse: 0.000345899\ttraining's RMSPE: 0.2307\tvalid_1's rmse: 0.000388477\tvalid_1's RMSPE: 0.2534\n",
      "[2000]\ttraining's rmse: 0.000344801\ttraining's RMSPE: 0.23\tvalid_1's rmse: 0.000388293\tvalid_1's RMSPE: 0.2533\n",
      "[2100]\ttraining's rmse: 0.000343681\ttraining's RMSPE: 0.2292\tvalid_1's rmse: 0.000388037\tvalid_1's RMSPE: 0.2531\n",
      "[2200]\ttraining's rmse: 0.000342623\ttraining's RMSPE: 0.2285\tvalid_1's rmse: 0.000387783\tvalid_1's RMSPE: 0.2529\n",
      "Early stopping, best iteration is:\n",
      "[2161]\ttraining's rmse: 0.000343039\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000387812\tvalid_1's RMSPE: 0.2529\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2529\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001200\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516733\ttraining's RMSPE: 0.3446\tvalid_1's rmse: 0.000518679\tvalid_1's RMSPE: 0.3384\n",
      "[200]\ttraining's rmse: 0.000418998\ttraining's RMSPE: 0.2794\tvalid_1's rmse: 0.000428291\tvalid_1's RMSPE: 0.2794\n",
      "[300]\ttraining's rmse: 0.000393915\ttraining's RMSPE: 0.2627\tvalid_1's rmse: 0.000406496\tvalid_1's RMSPE: 0.2652\n",
      "[400]\ttraining's rmse: 0.000384214\ttraining's RMSPE: 0.2562\tvalid_1's rmse: 0.000398948\tvalid_1's RMSPE: 0.2603\n",
      "[500]\ttraining's rmse: 0.000378406\ttraining's RMSPE: 0.2524\tvalid_1's rmse: 0.000394576\tvalid_1's RMSPE: 0.2574\n",
      "[600]\ttraining's rmse: 0.000374063\ttraining's RMSPE: 0.2495\tvalid_1's rmse: 0.000391325\tvalid_1's RMSPE: 0.2553\n",
      "[700]\ttraining's rmse: 0.000370331\ttraining's RMSPE: 0.247\tvalid_1's rmse: 0.000388668\tvalid_1's RMSPE: 0.2536\n",
      "[800]\ttraining's rmse: 0.00036721\ttraining's RMSPE: 0.2449\tvalid_1's rmse: 0.000386364\tvalid_1's RMSPE: 0.2521\n",
      "[900]\ttraining's rmse: 0.000364382\ttraining's RMSPE: 0.243\tvalid_1's rmse: 0.000384732\tvalid_1's RMSPE: 0.251\n",
      "[1000]\ttraining's rmse: 0.000361919\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000383266\tvalid_1's RMSPE: 0.2501\n",
      "[1100]\ttraining's rmse: 0.000359732\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000382069\tvalid_1's RMSPE: 0.2493\n",
      "[1200]\ttraining's rmse: 0.000357661\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.00038084\tvalid_1's RMSPE: 0.2485\n",
      "[1300]\ttraining's rmse: 0.000355893\ttraining's RMSPE: 0.2373\tvalid_1's rmse: 0.000380124\tvalid_1's RMSPE: 0.248\n",
      "[1400]\ttraining's rmse: 0.000354187\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000379404\tvalid_1's RMSPE: 0.2475\n",
      "[1500]\ttraining's rmse: 0.000352654\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000378558\tvalid_1's RMSPE: 0.247\n",
      "[1600]\ttraining's rmse: 0.000351156\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.00037808\tvalid_1's RMSPE: 0.2467\n",
      "[1700]\ttraining's rmse: 0.000349811\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000377599\tvalid_1's RMSPE: 0.2464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1800]\ttraining's rmse: 0.000348488\ttraining's RMSPE: 0.2324\tvalid_1's rmse: 0.00037708\tvalid_1's RMSPE: 0.246\n",
      "[1900]\ttraining's rmse: 0.000347226\ttraining's RMSPE: 0.2316\tvalid_1's rmse: 0.000376758\tvalid_1's RMSPE: 0.2458\n",
      "[2000]\ttraining's rmse: 0.000346028\ttraining's RMSPE: 0.2308\tvalid_1's rmse: 0.000376554\tvalid_1's RMSPE: 0.2457\n",
      "[2100]\ttraining's rmse: 0.000344885\ttraining's RMSPE: 0.23\tvalid_1's rmse: 0.000376267\tvalid_1's RMSPE: 0.2455\n",
      "[2200]\ttraining's rmse: 0.000343802\ttraining's RMSPE: 0.2293\tvalid_1's rmse: 0.000375965\tvalid_1's RMSPE: 0.2453\n",
      "[2300]\ttraining's rmse: 0.000342764\ttraining's RMSPE: 0.2286\tvalid_1's rmse: 0.000375765\tvalid_1's RMSPE: 0.2452\n",
      "[2400]\ttraining's rmse: 0.000341772\ttraining's RMSPE: 0.2279\tvalid_1's rmse: 0.000375562\tvalid_1's RMSPE: 0.245\n",
      "[2500]\ttraining's rmse: 0.000340826\ttraining's RMSPE: 0.2273\tvalid_1's rmse: 0.000375316\tvalid_1's RMSPE: 0.2449\n",
      "[2600]\ttraining's rmse: 0.000339937\ttraining's RMSPE: 0.2267\tvalid_1's rmse: 0.000375171\tvalid_1's RMSPE: 0.2448\n",
      "Early stopping, best iteration is:\n",
      "[2515]\ttraining's rmse: 0.000340686\ttraining's RMSPE: 0.2272\tvalid_1's rmse: 0.00037527\tvalid_1's RMSPE: 0.2448\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2448\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001202\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516323\ttraining's RMSPE: 0.344\tvalid_1's rmse: 0.000525276\tvalid_1's RMSPE: 0.3442\n",
      "[200]\ttraining's rmse: 0.000417008\ttraining's RMSPE: 0.2778\tvalid_1's rmse: 0.000434342\tvalid_1's RMSPE: 0.2846\n",
      "[300]\ttraining's rmse: 0.00039209\ttraining's RMSPE: 0.2612\tvalid_1's rmse: 0.000412698\tvalid_1's RMSPE: 0.2704\n",
      "[400]\ttraining's rmse: 0.000382451\ttraining's RMSPE: 0.2548\tvalid_1's rmse: 0.000404896\tvalid_1's RMSPE: 0.2653\n",
      "[500]\ttraining's rmse: 0.000376794\ttraining's RMSPE: 0.251\tvalid_1's rmse: 0.000400767\tvalid_1's RMSPE: 0.2626\n",
      "[600]\ttraining's rmse: 0.000372409\ttraining's RMSPE: 0.2481\tvalid_1's rmse: 0.000397664\tvalid_1's RMSPE: 0.2606\n",
      "[700]\ttraining's rmse: 0.000368821\ttraining's RMSPE: 0.2457\tvalid_1's rmse: 0.00039531\tvalid_1's RMSPE: 0.259\n",
      "[800]\ttraining's rmse: 0.000365825\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000393604\tvalid_1's RMSPE: 0.2579\n",
      "[900]\ttraining's rmse: 0.000363148\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.00039206\tvalid_1's RMSPE: 0.2569\n",
      "[1000]\ttraining's rmse: 0.000360871\ttraining's RMSPE: 0.2404\tvalid_1's rmse: 0.000390936\tvalid_1's RMSPE: 0.2562\n",
      "[1100]\ttraining's rmse: 0.000358718\ttraining's RMSPE: 0.239\tvalid_1's rmse: 0.000389876\tvalid_1's RMSPE: 0.2555\n",
      "[1200]\ttraining's rmse: 0.000356803\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.00038901\tvalid_1's RMSPE: 0.2549\n",
      "[1300]\ttraining's rmse: 0.000354924\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000388136\tvalid_1's RMSPE: 0.2543\n",
      "[1400]\ttraining's rmse: 0.00035326\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000387479\tvalid_1's RMSPE: 0.2539\n",
      "[1500]\ttraining's rmse: 0.000351773\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000386809\tvalid_1's RMSPE: 0.2534\n",
      "[1600]\ttraining's rmse: 0.00035036\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000386261\tvalid_1's RMSPE: 0.2531\n",
      "[1700]\ttraining's rmse: 0.000349022\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000385876\tvalid_1's RMSPE: 0.2528\n",
      "[1800]\ttraining's rmse: 0.000347774\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000385424\tvalid_1's RMSPE: 0.2525\n",
      "[1900]\ttraining's rmse: 0.000346534\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000385033\tvalid_1's RMSPE: 0.2523\n",
      "[2000]\ttraining's rmse: 0.000345351\ttraining's RMSPE: 0.2301\tvalid_1's rmse: 0.000384578\tvalid_1's RMSPE: 0.252\n",
      "[2100]\ttraining's rmse: 0.000344244\ttraining's RMSPE: 0.2293\tvalid_1's rmse: 0.000384318\tvalid_1's RMSPE: 0.2518\n",
      "[2200]\ttraining's rmse: 0.000343201\ttraining's RMSPE: 0.2286\tvalid_1's rmse: 0.000384131\tvalid_1's RMSPE: 0.2517\n",
      "[2300]\ttraining's rmse: 0.000342204\ttraining's RMSPE: 0.228\tvalid_1's rmse: 0.0003839\tvalid_1's RMSPE: 0.2515\n",
      "[2400]\ttraining's rmse: 0.000341228\ttraining's RMSPE: 0.2273\tvalid_1's rmse: 0.000383685\tvalid_1's RMSPE: 0.2514\n",
      "[2500]\ttraining's rmse: 0.000340306\ttraining's RMSPE: 0.2267\tvalid_1's rmse: 0.000383551\tvalid_1's RMSPE: 0.2513\n",
      "[2600]\ttraining's rmse: 0.000339351\ttraining's RMSPE: 0.2261\tvalid_1's rmse: 0.000383268\tvalid_1's RMSPE: 0.2511\n",
      "[2700]\ttraining's rmse: 0.000338453\ttraining's RMSPE: 0.2255\tvalid_1's rmse: 0.000383069\tvalid_1's RMSPE: 0.251\n",
      "[2800]\ttraining's rmse: 0.000337631\ttraining's RMSPE: 0.2249\tvalid_1's rmse: 0.000382929\tvalid_1's RMSPE: 0.2509\n",
      "Early stopping, best iteration is:\n",
      "[2737]\ttraining's rmse: 0.00033812\ttraining's RMSPE: 0.2253\tvalid_1's rmse: 0.000382992\tvalid_1's RMSPE: 0.2509\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2509\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001247\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516124\ttraining's RMSPE: 0.3376\tvalid_1's rmse: 0.000587522\tvalid_1's RMSPE: 0.4127\n",
      "[200]\ttraining's rmse: 0.000422428\ttraining's RMSPE: 0.2763\tvalid_1's rmse: 0.00048471\tvalid_1's RMSPE: 0.3405\n",
      "[300]\ttraining's rmse: 0.0003985\ttraining's RMSPE: 0.2607\tvalid_1's rmse: 0.000451268\tvalid_1's RMSPE: 0.317\n",
      "[400]\ttraining's rmse: 0.000388972\ttraining's RMSPE: 0.2544\tvalid_1's rmse: 0.000435654\tvalid_1's RMSPE: 0.306\n",
      "[500]\ttraining's rmse: 0.00038322\ttraining's RMSPE: 0.2507\tvalid_1's rmse: 0.000427388\tvalid_1's RMSPE: 0.3002\n",
      "[600]\ttraining's rmse: 0.000378824\ttraining's RMSPE: 0.2478\tvalid_1's rmse: 0.000424679\tvalid_1's RMSPE: 0.2983\n",
      "[700]\ttraining's rmse: 0.000375151\ttraining's RMSPE: 0.2454\tvalid_1's rmse: 0.000420948\tvalid_1's RMSPE: 0.2957\n",
      "[800]\ttraining's rmse: 0.000372071\ttraining's RMSPE: 0.2434\tvalid_1's rmse: 0.000418322\tvalid_1's RMSPE: 0.2938\n",
      "[900]\ttraining's rmse: 0.000369422\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.00041683\tvalid_1's RMSPE: 0.2928\n",
      "[1000]\ttraining's rmse: 0.000367049\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000416549\tvalid_1's RMSPE: 0.2926\n",
      "Early stopping, best iteration is:\n",
      "[942]\ttraining's rmse: 0.000368372\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000415483\tvalid_1's RMSPE: 0.2918\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2918\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 2\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136509, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001198\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517342\ttraining's RMSPE: 0.3454\tvalid_1's rmse: 0.000519883\tvalid_1's RMSPE: 0.3381\n",
      "[200]\ttraining's rmse: 0.000418999\ttraining's RMSPE: 0.2798\tvalid_1's rmse: 0.000426981\tvalid_1's RMSPE: 0.2777\n",
      "[300]\ttraining's rmse: 0.000393674\ttraining's RMSPE: 0.2629\tvalid_1's rmse: 0.000403684\tvalid_1's RMSPE: 0.2625\n",
      "[400]\ttraining's rmse: 0.00038381\ttraining's RMSPE: 0.2563\tvalid_1's rmse: 0.000395338\tvalid_1's RMSPE: 0.2571\n",
      "[500]\ttraining's rmse: 0.000377908\ttraining's RMSPE: 0.2523\tvalid_1's rmse: 0.0003909\tvalid_1's RMSPE: 0.2542\n",
      "[600]\ttraining's rmse: 0.000373468\ttraining's RMSPE: 0.2494\tvalid_1's rmse: 0.000387722\tvalid_1's RMSPE: 0.2521\n",
      "[700]\ttraining's rmse: 0.000369683\ttraining's RMSPE: 0.2468\tvalid_1's rmse: 0.000385038\tvalid_1's RMSPE: 0.2504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttraining's rmse: 0.000366578\ttraining's RMSPE: 0.2448\tvalid_1's rmse: 0.000383082\tvalid_1's RMSPE: 0.2491\n",
      "[900]\ttraining's rmse: 0.000363851\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000381251\tvalid_1's RMSPE: 0.2479\n",
      "[1000]\ttraining's rmse: 0.000361386\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000379782\tvalid_1's RMSPE: 0.247\n",
      "[1100]\ttraining's rmse: 0.000359249\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000378573\tvalid_1's RMSPE: 0.2462\n",
      "[1200]\ttraining's rmse: 0.000357309\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000377492\tvalid_1's RMSPE: 0.2455\n",
      "[1300]\ttraining's rmse: 0.000355517\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000376617\tvalid_1's RMSPE: 0.2449\n",
      "[1400]\ttraining's rmse: 0.000353835\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000375678\tvalid_1's RMSPE: 0.2443\n",
      "[1500]\ttraining's rmse: 0.000352286\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000375021\tvalid_1's RMSPE: 0.2439\n",
      "[1600]\ttraining's rmse: 0.000350881\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000374643\tvalid_1's RMSPE: 0.2436\n",
      "[1700]\ttraining's rmse: 0.000349526\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000374222\tvalid_1's RMSPE: 0.2434\n",
      "[1800]\ttraining's rmse: 0.000348278\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000373693\tvalid_1's RMSPE: 0.243\n",
      "[1900]\ttraining's rmse: 0.000347032\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000373185\tvalid_1's RMSPE: 0.2427\n",
      "[2000]\ttraining's rmse: 0.000345865\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.00037278\tvalid_1's RMSPE: 0.2424\n",
      "[2100]\ttraining's rmse: 0.000344721\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.000372455\tvalid_1's RMSPE: 0.2422\n",
      "[2200]\ttraining's rmse: 0.000343692\ttraining's RMSPE: 0.2295\tvalid_1's rmse: 0.000372084\tvalid_1's RMSPE: 0.242\n",
      "[2300]\ttraining's rmse: 0.000342617\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000371823\tvalid_1's RMSPE: 0.2418\n",
      "[2400]\ttraining's rmse: 0.000341631\ttraining's RMSPE: 0.2281\tvalid_1's rmse: 0.000371594\tvalid_1's RMSPE: 0.2417\n",
      "Early stopping, best iteration is:\n",
      "[2388]\ttraining's rmse: 0.000341756\ttraining's RMSPE: 0.2282\tvalid_1's rmse: 0.000371583\tvalid_1's RMSPE: 0.2416\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2417\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136509, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001248\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516693\ttraining's RMSPE: 0.338\tvalid_1's rmse: 0.000629209\tvalid_1's RMSPE: 0.4425\n",
      "[200]\ttraining's rmse: 0.000422712\ttraining's RMSPE: 0.2765\tvalid_1's rmse: 0.000546761\tvalid_1's RMSPE: 0.3845\n",
      "[300]\ttraining's rmse: 0.000398806\ttraining's RMSPE: 0.2609\tvalid_1's rmse: 0.000513657\tvalid_1's RMSPE: 0.3612\n",
      "[400]\ttraining's rmse: 0.000389068\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000491074\tvalid_1's RMSPE: 0.3453\n",
      "[500]\ttraining's rmse: 0.00038298\ttraining's RMSPE: 0.2505\tvalid_1's rmse: 0.000476778\tvalid_1's RMSPE: 0.3353\n",
      "[600]\ttraining's rmse: 0.000378377\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000470415\tvalid_1's RMSPE: 0.3308\n",
      "[700]\ttraining's rmse: 0.000374655\ttraining's RMSPE: 0.2451\tvalid_1's rmse: 0.000466278\tvalid_1's RMSPE: 0.3279\n",
      "[800]\ttraining's rmse: 0.000371459\ttraining's RMSPE: 0.243\tvalid_1's rmse: 0.000465553\tvalid_1's RMSPE: 0.3274\n",
      "[900]\ttraining's rmse: 0.000368665\ttraining's RMSPE: 0.2412\tvalid_1's rmse: 0.000461179\tvalid_1's RMSPE: 0.3243\n",
      "[1000]\ttraining's rmse: 0.000366221\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000459255\tvalid_1's RMSPE: 0.323\n",
      "[1100]\ttraining's rmse: 0.000364082\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000459862\tvalid_1's RMSPE: 0.3234\n",
      "Early stopping, best iteration is:\n",
      "[1016]\ttraining's rmse: 0.000365848\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000458396\tvalid_1's RMSPE: 0.3223\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.3223\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001200\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516531\ttraining's RMSPE: 0.3444\tvalid_1's rmse: 0.000518776\tvalid_1's RMSPE: 0.3393\n",
      "[200]\ttraining's rmse: 0.000418392\ttraining's RMSPE: 0.279\tvalid_1's rmse: 0.000429728\tvalid_1's RMSPE: 0.2811\n",
      "[300]\ttraining's rmse: 0.000393089\ttraining's RMSPE: 0.2621\tvalid_1's rmse: 0.000408291\tvalid_1's RMSPE: 0.267\n",
      "[400]\ttraining's rmse: 0.000383255\ttraining's RMSPE: 0.2556\tvalid_1's rmse: 0.000400585\tvalid_1's RMSPE: 0.262\n",
      "[500]\ttraining's rmse: 0.000377403\ttraining's RMSPE: 0.2516\tvalid_1's rmse: 0.000396642\tvalid_1's RMSPE: 0.2594\n",
      "[600]\ttraining's rmse: 0.000372966\ttraining's RMSPE: 0.2487\tvalid_1's rmse: 0.000393814\tvalid_1's RMSPE: 0.2576\n",
      "[700]\ttraining's rmse: 0.000369236\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000391669\tvalid_1's RMSPE: 0.2562\n",
      "[800]\ttraining's rmse: 0.000366024\ttraining's RMSPE: 0.2441\tvalid_1's rmse: 0.00039\tvalid_1's RMSPE: 0.2551\n",
      "[900]\ttraining's rmse: 0.000363253\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000388314\tvalid_1's RMSPE: 0.254\n",
      "[1000]\ttraining's rmse: 0.000360866\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000387111\tvalid_1's RMSPE: 0.2532\n",
      "[1100]\ttraining's rmse: 0.000358621\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.0003862\tvalid_1's RMSPE: 0.2526\n",
      "[1200]\ttraining's rmse: 0.000356711\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.000385316\tvalid_1's RMSPE: 0.252\n",
      "[1300]\ttraining's rmse: 0.000354887\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000384537\tvalid_1's RMSPE: 0.2515\n",
      "[1400]\ttraining's rmse: 0.000353188\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.00038387\tvalid_1's RMSPE: 0.2511\n",
      "[1500]\ttraining's rmse: 0.000351622\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000383243\tvalid_1's RMSPE: 0.2507\n",
      "[1600]\ttraining's rmse: 0.000350227\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000382641\tvalid_1's RMSPE: 0.2503\n",
      "[1700]\ttraining's rmse: 0.000348915\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000382304\tvalid_1's RMSPE: 0.25\n",
      "[1800]\ttraining's rmse: 0.000347565\ttraining's RMSPE: 0.2318\tvalid_1's rmse: 0.000381936\tvalid_1's RMSPE: 0.2498\n",
      "[1900]\ttraining's rmse: 0.000346334\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000381602\tvalid_1's RMSPE: 0.2496\n",
      "[2000]\ttraining's rmse: 0.000345215\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.000381213\tvalid_1's RMSPE: 0.2493\n",
      "[2100]\ttraining's rmse: 0.000344128\ttraining's RMSPE: 0.2295\tvalid_1's rmse: 0.00038095\tvalid_1's RMSPE: 0.2492\n",
      "[2200]\ttraining's rmse: 0.000343087\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000380605\tvalid_1's RMSPE: 0.2489\n",
      "Early stopping, best iteration is:\n",
      "[2189]\ttraining's rmse: 0.000343206\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000380623\tvalid_1's RMSPE: 0.2489\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2489\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001200\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000514912\ttraining's RMSPE: 0.3433\tvalid_1's rmse: 0.000533216\tvalid_1's RMSPE: 0.3488\n",
      "[200]\ttraining's rmse: 0.00041662\ttraining's RMSPE: 0.2778\tvalid_1's rmse: 0.000455338\tvalid_1's RMSPE: 0.2978\n",
      "[300]\ttraining's rmse: 0.000391844\ttraining's RMSPE: 0.2613\tvalid_1's rmse: 0.000440137\tvalid_1's RMSPE: 0.2879\n",
      "[400]\ttraining's rmse: 0.000382396\ttraining's RMSPE: 0.255\tvalid_1's rmse: 0.00043556\tvalid_1's RMSPE: 0.2849\n",
      "[500]\ttraining's rmse: 0.000376709\ttraining's RMSPE: 0.2512\tvalid_1's rmse: 0.000432979\tvalid_1's RMSPE: 0.2832\n",
      "[600]\ttraining's rmse: 0.000372366\ttraining's RMSPE: 0.2483\tvalid_1's rmse: 0.000429969\tvalid_1's RMSPE: 0.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\ttraining's rmse: 0.000368766\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000428529\tvalid_1's RMSPE: 0.2803\n",
      "[800]\ttraining's rmse: 0.000365726\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000427647\tvalid_1's RMSPE: 0.2797\n",
      "[900]\ttraining's rmse: 0.000363052\ttraining's RMSPE: 0.2421\tvalid_1's rmse: 0.000427095\tvalid_1's RMSPE: 0.2793\n",
      "[1000]\ttraining's rmse: 0.000360683\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000426464\tvalid_1's RMSPE: 0.2789\n",
      "[1100]\ttraining's rmse: 0.000358508\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.0004258\tvalid_1's RMSPE: 0.2785\n",
      "Early stopping, best iteration is:\n",
      "[1090]\ttraining's rmse: 0.000358719\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000425851\tvalid_1's RMSPE: 0.2785\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2785\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001204\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000515962\ttraining's RMSPE: 0.3435\tvalid_1's rmse: 0.00052229\tvalid_1's RMSPE: 0.3438\n",
      "[200]\ttraining's rmse: 0.000417441\ttraining's RMSPE: 0.2779\tvalid_1's rmse: 0.000430532\tvalid_1's RMSPE: 0.2834\n",
      "[300]\ttraining's rmse: 0.000392148\ttraining's RMSPE: 0.2611\tvalid_1's rmse: 0.00040813\tvalid_1's RMSPE: 0.2686\n",
      "[400]\ttraining's rmse: 0.000382323\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000400247\tvalid_1's RMSPE: 0.2634\n",
      "[500]\ttraining's rmse: 0.000376446\ttraining's RMSPE: 0.2506\tvalid_1's rmse: 0.000396257\tvalid_1's RMSPE: 0.2608\n",
      "[600]\ttraining's rmse: 0.000372106\ttraining's RMSPE: 0.2477\tvalid_1's rmse: 0.000393403\tvalid_1's RMSPE: 0.2589\n",
      "[700]\ttraining's rmse: 0.000368316\ttraining's RMSPE: 0.2452\tvalid_1's rmse: 0.000391176\tvalid_1's RMSPE: 0.2575\n",
      "[800]\ttraining's rmse: 0.000365217\ttraining's RMSPE: 0.2432\tvalid_1's rmse: 0.000389513\tvalid_1's RMSPE: 0.2564\n",
      "[900]\ttraining's rmse: 0.00036246\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000388109\tvalid_1's RMSPE: 0.2555\n",
      "[1000]\ttraining's rmse: 0.000360062\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000387009\tvalid_1's RMSPE: 0.2547\n",
      "[1100]\ttraining's rmse: 0.000357892\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.00038603\tvalid_1's RMSPE: 0.2541\n",
      "[1200]\ttraining's rmse: 0.000355883\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000385267\tvalid_1's RMSPE: 0.2536\n",
      "[1300]\ttraining's rmse: 0.000354106\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000384539\tvalid_1's RMSPE: 0.2531\n",
      "[1400]\ttraining's rmse: 0.000352487\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.00038387\tvalid_1's RMSPE: 0.2527\n",
      "[1500]\ttraining's rmse: 0.000351018\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000383415\tvalid_1's RMSPE: 0.2524\n",
      "[1600]\ttraining's rmse: 0.000349557\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000383012\tvalid_1's RMSPE: 0.2521\n",
      "[1700]\ttraining's rmse: 0.000348209\ttraining's RMSPE: 0.2318\tvalid_1's rmse: 0.000382586\tvalid_1's RMSPE: 0.2518\n",
      "[1800]\ttraining's rmse: 0.000346964\ttraining's RMSPE: 0.231\tvalid_1's rmse: 0.000382268\tvalid_1's RMSPE: 0.2516\n",
      "[1900]\ttraining's rmse: 0.000345783\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.000382014\tvalid_1's RMSPE: 0.2514\n",
      "[2000]\ttraining's rmse: 0.000344641\ttraining's RMSPE: 0.2295\tvalid_1's rmse: 0.000381795\tvalid_1's RMSPE: 0.2513\n",
      "[2100]\ttraining's rmse: 0.00034355\ttraining's RMSPE: 0.2287\tvalid_1's rmse: 0.000381559\tvalid_1's RMSPE: 0.2511\n",
      "[2200]\ttraining's rmse: 0.000342492\ttraining's RMSPE: 0.228\tvalid_1's rmse: 0.000381359\tvalid_1's RMSPE: 0.251\n",
      "[2300]\ttraining's rmse: 0.000341506\ttraining's RMSPE: 0.2274\tvalid_1's rmse: 0.000381179\tvalid_1's RMSPE: 0.2509\n",
      "[2400]\ttraining's rmse: 0.00034054\ttraining's RMSPE: 0.2267\tvalid_1's rmse: 0.000381026\tvalid_1's RMSPE: 0.2508\n",
      "Early stopping, best iteration is:\n",
      "[2340]\ttraining's rmse: 0.000341101\ttraining's RMSPE: 0.2271\tvalid_1's rmse: 0.000381101\tvalid_1's RMSPE: 0.2508\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2508\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 3\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001204\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517982\ttraining's RMSPE: 0.3447\tvalid_1's rmse: 0.000519905\tvalid_1's RMSPE: 0.3411\n",
      "[200]\ttraining's rmse: 0.00041955\ttraining's RMSPE: 0.2792\tvalid_1's rmse: 0.000432842\tvalid_1's RMSPE: 0.284\n",
      "[300]\ttraining's rmse: 0.000394293\ttraining's RMSPE: 0.2624\tvalid_1's rmse: 0.000412666\tvalid_1's RMSPE: 0.2708\n",
      "[400]\ttraining's rmse: 0.00038438\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000405375\tvalid_1's RMSPE: 0.266\n",
      "[500]\ttraining's rmse: 0.000378453\ttraining's RMSPE: 0.2518\tvalid_1's rmse: 0.000401017\tvalid_1's RMSPE: 0.2631\n",
      "[600]\ttraining's rmse: 0.000373994\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.000398046\tvalid_1's RMSPE: 0.2612\n",
      "[700]\ttraining's rmse: 0.000370286\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.000395536\tvalid_1's RMSPE: 0.2595\n",
      "[800]\ttraining's rmse: 0.000367137\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000393703\tvalid_1's RMSPE: 0.2583\n",
      "[900]\ttraining's rmse: 0.000364323\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000392143\tvalid_1's RMSPE: 0.2573\n",
      "[1000]\ttraining's rmse: 0.000361819\ttraining's RMSPE: 0.2408\tvalid_1's rmse: 0.000390723\tvalid_1's RMSPE: 0.2564\n",
      "[1100]\ttraining's rmse: 0.000359703\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.0003896\tvalid_1's RMSPE: 0.2556\n",
      "[1200]\ttraining's rmse: 0.000357702\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000388846\tvalid_1's RMSPE: 0.2551\n",
      "[1300]\ttraining's rmse: 0.000355941\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000388065\tvalid_1's RMSPE: 0.2546\n",
      "[1400]\ttraining's rmse: 0.00035423\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000387431\tvalid_1's RMSPE: 0.2542\n",
      "[1500]\ttraining's rmse: 0.000352649\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000386854\tvalid_1's RMSPE: 0.2538\n",
      "[1600]\ttraining's rmse: 0.000351169\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.00038644\tvalid_1's RMSPE: 0.2536\n",
      "[1700]\ttraining's rmse: 0.000349803\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.00038589\tvalid_1's RMSPE: 0.2532\n",
      "[1800]\ttraining's rmse: 0.000348506\ttraining's RMSPE: 0.2319\tvalid_1's rmse: 0.00038553\tvalid_1's RMSPE: 0.253\n",
      "[1900]\ttraining's rmse: 0.000347279\ttraining's RMSPE: 0.2311\tvalid_1's rmse: 0.000385362\tvalid_1's RMSPE: 0.2529\n",
      "[2000]\ttraining's rmse: 0.000346112\ttraining's RMSPE: 0.2303\tvalid_1's rmse: 0.000385049\tvalid_1's RMSPE: 0.2527\n",
      "[2100]\ttraining's rmse: 0.00034502\ttraining's RMSPE: 0.2296\tvalid_1's rmse: 0.000384714\tvalid_1's RMSPE: 0.2524\n",
      "[2200]\ttraining's rmse: 0.000343932\ttraining's RMSPE: 0.2289\tvalid_1's rmse: 0.000384515\tvalid_1's RMSPE: 0.2523\n",
      "[2300]\ttraining's rmse: 0.000342901\ttraining's RMSPE: 0.2282\tvalid_1's rmse: 0.000384395\tvalid_1's RMSPE: 0.2522\n",
      "Early stopping, best iteration is:\n",
      "[2251]\ttraining's rmse: 0.000343388\ttraining's RMSPE: 0.2285\tvalid_1's rmse: 0.000384428\tvalid_1's RMSPE: 0.2522\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2522\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001201\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516203\ttraining's RMSPE: 0.3442\tvalid_1's rmse: 0.000530299\tvalid_1's RMSPE: 0.3452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.000418534\ttraining's RMSPE: 0.279\tvalid_1's rmse: 0.00044962\tvalid_1's RMSPE: 0.2927\n",
      "[300]\ttraining's rmse: 0.000393355\ttraining's RMSPE: 0.2623\tvalid_1's rmse: 0.000432343\tvalid_1's RMSPE: 0.2814\n",
      "[400]\ttraining's rmse: 0.000383716\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000426893\tvalid_1's RMSPE: 0.2779\n",
      "[500]\ttraining's rmse: 0.000377844\ttraining's RMSPE: 0.2519\tvalid_1's rmse: 0.000423716\tvalid_1's RMSPE: 0.2758\n",
      "[600]\ttraining's rmse: 0.000373382\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.00042139\tvalid_1's RMSPE: 0.2743\n",
      "[700]\ttraining's rmse: 0.000369688\ttraining's RMSPE: 0.2465\tvalid_1's rmse: 0.000419937\tvalid_1's RMSPE: 0.2733\n",
      "[800]\ttraining's rmse: 0.000366552\ttraining's RMSPE: 0.2444\tvalid_1's rmse: 0.000418556\tvalid_1's RMSPE: 0.2724\n",
      "[900]\ttraining's rmse: 0.000363751\ttraining's RMSPE: 0.2425\tvalid_1's rmse: 0.000417801\tvalid_1's RMSPE: 0.272\n",
      "[1000]\ttraining's rmse: 0.000361274\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000416844\tvalid_1's RMSPE: 0.2713\n",
      "[1100]\ttraining's rmse: 0.000359037\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000415966\tvalid_1's RMSPE: 0.2708\n",
      "[1200]\ttraining's rmse: 0.000356992\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000415732\tvalid_1's RMSPE: 0.2706\n",
      "[1300]\ttraining's rmse: 0.000355216\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000415641\tvalid_1's RMSPE: 0.2705\n",
      "Early stopping, best iteration is:\n",
      "[1249]\ttraining's rmse: 0.000356075\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000415332\tvalid_1's RMSPE: 0.2703\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2703\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001205\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00051741\ttraining's RMSPE: 0.3444\tvalid_1's rmse: 0.000526349\tvalid_1's RMSPE: 0.3448\n",
      "[200]\ttraining's rmse: 0.00042089\ttraining's RMSPE: 0.2802\tvalid_1's rmse: 0.000433278\tvalid_1's RMSPE: 0.2839\n",
      "[300]\ttraining's rmse: 0.000395688\ttraining's RMSPE: 0.2634\tvalid_1's rmse: 0.000409352\tvalid_1's RMSPE: 0.2682\n",
      "[400]\ttraining's rmse: 0.000385833\ttraining's RMSPE: 0.2568\tvalid_1's rmse: 0.000401082\tvalid_1's RMSPE: 0.2628\n",
      "[500]\ttraining's rmse: 0.000379866\ttraining's RMSPE: 0.2529\tvalid_1's rmse: 0.000396704\tvalid_1's RMSPE: 0.2599\n",
      "[600]\ttraining's rmse: 0.000375327\ttraining's RMSPE: 0.2498\tvalid_1's rmse: 0.000394038\tvalid_1's RMSPE: 0.2582\n",
      "[700]\ttraining's rmse: 0.000371576\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.00039149\tvalid_1's RMSPE: 0.2565\n",
      "[800]\ttraining's rmse: 0.000368368\ttraining's RMSPE: 0.2452\tvalid_1's rmse: 0.000389482\tvalid_1's RMSPE: 0.2552\n",
      "[900]\ttraining's rmse: 0.000365487\ttraining's RMSPE: 0.2433\tvalid_1's rmse: 0.000387835\tvalid_1's RMSPE: 0.2541\n",
      "[1000]\ttraining's rmse: 0.000363037\ttraining's RMSPE: 0.2417\tvalid_1's rmse: 0.000386769\tvalid_1's RMSPE: 0.2534\n",
      "[1100]\ttraining's rmse: 0.000360687\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000385866\tvalid_1's RMSPE: 0.2528\n",
      "[1200]\ttraining's rmse: 0.0003587\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000384955\tvalid_1's RMSPE: 0.2522\n",
      "[1300]\ttraining's rmse: 0.00035678\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000383853\tvalid_1's RMSPE: 0.2515\n",
      "[1400]\ttraining's rmse: 0.000355033\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000383162\tvalid_1's RMSPE: 0.251\n",
      "[1500]\ttraining's rmse: 0.000353434\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.00038263\tvalid_1's RMSPE: 0.2507\n",
      "[1600]\ttraining's rmse: 0.00035193\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000382136\tvalid_1's RMSPE: 0.2504\n",
      "[1700]\ttraining's rmse: 0.000350493\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000381463\tvalid_1's RMSPE: 0.2499\n",
      "[1800]\ttraining's rmse: 0.00034922\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000381084\tvalid_1's RMSPE: 0.2497\n",
      "[1900]\ttraining's rmse: 0.00034804\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000380812\tvalid_1's RMSPE: 0.2495\n",
      "[2000]\ttraining's rmse: 0.000346871\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000380636\tvalid_1's RMSPE: 0.2494\n",
      "[2100]\ttraining's rmse: 0.000345746\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.00038027\tvalid_1's RMSPE: 0.2491\n",
      "[2200]\ttraining's rmse: 0.000344613\ttraining's RMSPE: 0.2294\tvalid_1's rmse: 0.000380015\tvalid_1's RMSPE: 0.249\n",
      "Early stopping, best iteration is:\n",
      "[2125]\ttraining's rmse: 0.00034545\ttraining's RMSPE: 0.23\tvalid_1's rmse: 0.000380142\tvalid_1's RMSPE: 0.249\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.249\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001203\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517383\ttraining's RMSPE: 0.3445\tvalid_1's rmse: 0.000520387\tvalid_1's RMSPE: 0.3406\n",
      "[200]\ttraining's rmse: 0.000419467\ttraining's RMSPE: 0.2793\tvalid_1's rmse: 0.000431385\tvalid_1's RMSPE: 0.2823\n",
      "[300]\ttraining's rmse: 0.000393871\ttraining's RMSPE: 0.2623\tvalid_1's rmse: 0.000409935\tvalid_1's RMSPE: 0.2683\n",
      "[400]\ttraining's rmse: 0.000384003\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.000402657\tvalid_1's RMSPE: 0.2635\n",
      "[500]\ttraining's rmse: 0.000378052\ttraining's RMSPE: 0.2517\tvalid_1's rmse: 0.000398722\tvalid_1's RMSPE: 0.2609\n",
      "[600]\ttraining's rmse: 0.000373574\ttraining's RMSPE: 0.2487\tvalid_1's rmse: 0.00039587\tvalid_1's RMSPE: 0.2591\n",
      "[700]\ttraining's rmse: 0.000369795\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000393557\tvalid_1's RMSPE: 0.2576\n",
      "[800]\ttraining's rmse: 0.000366609\ttraining's RMSPE: 0.2441\tvalid_1's rmse: 0.000391726\tvalid_1's RMSPE: 0.2564\n",
      "[900]\ttraining's rmse: 0.000363882\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000390339\tvalid_1's RMSPE: 0.2555\n",
      "[1000]\ttraining's rmse: 0.000361395\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000389168\tvalid_1's RMSPE: 0.2547\n",
      "[1100]\ttraining's rmse: 0.000359204\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.0003881\tvalid_1's RMSPE: 0.254\n",
      "[1200]\ttraining's rmse: 0.000357203\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000387167\tvalid_1's RMSPE: 0.2534\n",
      "[1300]\ttraining's rmse: 0.0003553\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000386368\tvalid_1's RMSPE: 0.2529\n",
      "[1400]\ttraining's rmse: 0.000353594\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000385769\tvalid_1's RMSPE: 0.2525\n",
      "[1500]\ttraining's rmse: 0.000352037\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000385149\tvalid_1's RMSPE: 0.2521\n",
      "[1600]\ttraining's rmse: 0.000350584\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000384779\tvalid_1's RMSPE: 0.2518\n",
      "[1700]\ttraining's rmse: 0.000349261\ttraining's RMSPE: 0.2326\tvalid_1's rmse: 0.000384382\tvalid_1's RMSPE: 0.2516\n",
      "[1800]\ttraining's rmse: 0.000347972\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000383973\tvalid_1's RMSPE: 0.2513\n",
      "[1900]\ttraining's rmse: 0.000346712\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000383605\tvalid_1's RMSPE: 0.251\n",
      "[2000]\ttraining's rmse: 0.00034556\ttraining's RMSPE: 0.2301\tvalid_1's rmse: 0.000383331\tvalid_1's RMSPE: 0.2509\n",
      "[2100]\ttraining's rmse: 0.000344433\ttraining's RMSPE: 0.2293\tvalid_1's rmse: 0.000383041\tvalid_1's RMSPE: 0.2507\n",
      "[2200]\ttraining's rmse: 0.000343388\ttraining's RMSPE: 0.2286\tvalid_1's rmse: 0.000382722\tvalid_1's RMSPE: 0.2505\n",
      "[2300]\ttraining's rmse: 0.000342308\ttraining's RMSPE: 0.2279\tvalid_1's rmse: 0.000382413\tvalid_1's RMSPE: 0.2503\n",
      "[2400]\ttraining's rmse: 0.000341302\ttraining's RMSPE: 0.2273\tvalid_1's rmse: 0.000382186\tvalid_1's RMSPE: 0.2501\n",
      "[2500]\ttraining's rmse: 0.000340308\ttraining's RMSPE: 0.2266\tvalid_1's rmse: 0.000382038\tvalid_1's RMSPE: 0.25\n",
      "[2600]\ttraining's rmse: 0.00033936\ttraining's RMSPE: 0.226\tvalid_1's rmse: 0.000381827\tvalid_1's RMSPE: 0.2499\n",
      "[2700]\ttraining's rmse: 0.000338478\ttraining's RMSPE: 0.2254\tvalid_1's rmse: 0.000381636\tvalid_1's RMSPE: 0.2498\n",
      "[2800]\ttraining's rmse: 0.000337607\ttraining's RMSPE: 0.2248\tvalid_1's rmse: 0.000381557\tvalid_1's RMSPE: 0.2497\n",
      "Early stopping, best iteration is:\n",
      "[2709]\ttraining's rmse: 0.000338401\ttraining's RMSPE: 0.2253\tvalid_1's rmse: 0.000381625\tvalid_1's RMSPE: 0.2497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2497\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001248\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516316\ttraining's RMSPE: 0.3378\tvalid_1's rmse: 0.000588461\tvalid_1's RMSPE: 0.4119\n",
      "[200]\ttraining's rmse: 0.000423563\ttraining's RMSPE: 0.2771\tvalid_1's rmse: 0.000482102\tvalid_1's RMSPE: 0.3375\n",
      "[300]\ttraining's rmse: 0.000399921\ttraining's RMSPE: 0.2616\tvalid_1's rmse: 0.000444448\tvalid_1's RMSPE: 0.3111\n",
      "[400]\ttraining's rmse: 0.000390152\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000425825\tvalid_1's RMSPE: 0.2981\n",
      "[500]\ttraining's rmse: 0.000384215\ttraining's RMSPE: 0.2513\tvalid_1's rmse: 0.000416213\tvalid_1's RMSPE: 0.2913\n",
      "[600]\ttraining's rmse: 0.000379794\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000412721\tvalid_1's RMSPE: 0.2889\n",
      "[700]\ttraining's rmse: 0.00037604\ttraining's RMSPE: 0.246\tvalid_1's rmse: 0.000409205\tvalid_1's RMSPE: 0.2864\n",
      "[800]\ttraining's rmse: 0.00037284\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000407947\tvalid_1's RMSPE: 0.2856\n",
      "[900]\ttraining's rmse: 0.000369997\ttraining's RMSPE: 0.242\tvalid_1's rmse: 0.000404614\tvalid_1's RMSPE: 0.2832\n",
      "[1000]\ttraining's rmse: 0.000367493\ttraining's RMSPE: 0.2404\tvalid_1's rmse: 0.000403051\tvalid_1's RMSPE: 0.2821\n",
      "[1100]\ttraining's rmse: 0.000365256\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000400952\tvalid_1's RMSPE: 0.2807\n",
      "[1200]\ttraining's rmse: 0.000363149\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.00040019\tvalid_1's RMSPE: 0.2801\n",
      "Early stopping, best iteration is:\n",
      "[1192]\ttraining's rmse: 0.000363315\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.000400023\tvalid_1's RMSPE: 0.28\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.28\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 4\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001253\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517984\ttraining's RMSPE: 0.3376\tvalid_1's rmse: 0.000523519\tvalid_1's RMSPE: 0.3452\n",
      "[200]\ttraining's rmse: 0.000424626\ttraining's RMSPE: 0.2768\tvalid_1's rmse: 0.000438977\tvalid_1's RMSPE: 0.2894\n",
      "[300]\ttraining's rmse: 0.000400882\ttraining's RMSPE: 0.2613\tvalid_1's rmse: 0.000421306\tvalid_1's RMSPE: 0.2778\n",
      "[400]\ttraining's rmse: 0.000391325\ttraining's RMSPE: 0.2551\tvalid_1's rmse: 0.000415404\tvalid_1's RMSPE: 0.2739\n",
      "[500]\ttraining's rmse: 0.000385446\ttraining's RMSPE: 0.2512\tvalid_1's rmse: 0.000412091\tvalid_1's RMSPE: 0.2717\n",
      "[600]\ttraining's rmse: 0.000381076\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000409851\tvalid_1's RMSPE: 0.2702\n",
      "[700]\ttraining's rmse: 0.000377273\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000408068\tvalid_1's RMSPE: 0.269\n",
      "[800]\ttraining's rmse: 0.000374195\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000407454\tvalid_1's RMSPE: 0.2686\n",
      "[900]\ttraining's rmse: 0.000371438\ttraining's RMSPE: 0.2421\tvalid_1's rmse: 0.000406303\tvalid_1's RMSPE: 0.2679\n",
      "[1000]\ttraining's rmse: 0.000368934\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000405662\tvalid_1's RMSPE: 0.2675\n",
      "[1100]\ttraining's rmse: 0.000366815\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000404763\tvalid_1's RMSPE: 0.2669\n",
      "[1200]\ttraining's rmse: 0.000364792\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000404288\tvalid_1's RMSPE: 0.2665\n",
      "[1300]\ttraining's rmse: 0.00036291\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.00040441\tvalid_1's RMSPE: 0.2666\n",
      "Early stopping, best iteration is:\n",
      "[1220]\ttraining's rmse: 0.000364419\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000403991\tvalid_1's RMSPE: 0.2663\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2663\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001248\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000518001\ttraining's RMSPE: 0.3385\tvalid_1's rmse: 0.000521379\tvalid_1's RMSPE: 0.3403\n",
      "[200]\ttraining's rmse: 0.000424744\ttraining's RMSPE: 0.2775\tvalid_1's rmse: 0.000429901\tvalid_1's RMSPE: 0.2806\n",
      "[300]\ttraining's rmse: 0.000401155\ttraining's RMSPE: 0.2621\tvalid_1's rmse: 0.00040731\tvalid_1's RMSPE: 0.2659\n",
      "[400]\ttraining's rmse: 0.000391438\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000399114\tvalid_1's RMSPE: 0.2605\n",
      "[500]\ttraining's rmse: 0.000385413\ttraining's RMSPE: 0.2518\tvalid_1's rmse: 0.000394759\tvalid_1's RMSPE: 0.2577\n",
      "[600]\ttraining's rmse: 0.00038089\ttraining's RMSPE: 0.2489\tvalid_1's rmse: 0.000391886\tvalid_1's RMSPE: 0.2558\n",
      "[700]\ttraining's rmse: 0.000377041\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.000389728\tvalid_1's RMSPE: 0.2544\n",
      "[800]\ttraining's rmse: 0.000373818\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000388116\tvalid_1's RMSPE: 0.2533\n",
      "[900]\ttraining's rmse: 0.000371055\ttraining's RMSPE: 0.2425\tvalid_1's rmse: 0.000386673\tvalid_1's RMSPE: 0.2524\n",
      "[1000]\ttraining's rmse: 0.00036846\ttraining's RMSPE: 0.2408\tvalid_1's rmse: 0.000385433\tvalid_1's RMSPE: 0.2516\n",
      "[1100]\ttraining's rmse: 0.000366214\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000384438\tvalid_1's RMSPE: 0.2509\n",
      "[1200]\ttraining's rmse: 0.000364227\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000383665\tvalid_1's RMSPE: 0.2504\n",
      "[1300]\ttraining's rmse: 0.00036234\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000382918\tvalid_1's RMSPE: 0.2499\n",
      "[1400]\ttraining's rmse: 0.000360597\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000382272\tvalid_1's RMSPE: 0.2495\n",
      "[1500]\ttraining's rmse: 0.000359032\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000381748\tvalid_1's RMSPE: 0.2492\n",
      "[1600]\ttraining's rmse: 0.000357537\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000381346\tvalid_1's RMSPE: 0.2489\n",
      "[1700]\ttraining's rmse: 0.00035612\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000380955\tvalid_1's RMSPE: 0.2487\n",
      "[1800]\ttraining's rmse: 0.000354776\ttraining's RMSPE: 0.2318\tvalid_1's rmse: 0.00038055\tvalid_1's RMSPE: 0.2484\n",
      "[1900]\ttraining's rmse: 0.000353516\ttraining's RMSPE: 0.231\tvalid_1's rmse: 0.00038024\tvalid_1's RMSPE: 0.2482\n",
      "[2000]\ttraining's rmse: 0.000352334\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.00038006\tvalid_1's RMSPE: 0.2481\n",
      "[2100]\ttraining's rmse: 0.000351137\ttraining's RMSPE: 0.2294\tvalid_1's rmse: 0.000379761\tvalid_1's RMSPE: 0.2479\n",
      "[2200]\ttraining's rmse: 0.000350018\ttraining's RMSPE: 0.2287\tvalid_1's rmse: 0.000379517\tvalid_1's RMSPE: 0.2477\n",
      "[2300]\ttraining's rmse: 0.000349033\ttraining's RMSPE: 0.2281\tvalid_1's rmse: 0.000379312\tvalid_1's RMSPE: 0.2476\n",
      "Early stopping, best iteration is:\n",
      "[2227]\ttraining's rmse: 0.000349738\ttraining's RMSPE: 0.2285\tvalid_1's rmse: 0.000379414\tvalid_1's RMSPE: 0.2476\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2476\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001251\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516129\ttraining's RMSPE: 0.3369\tvalid_1's rmse: 0.000527543\tvalid_1's RMSPE: 0.3459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.00042318\ttraining's RMSPE: 0.2762\tvalid_1's rmse: 0.000438633\tvalid_1's RMSPE: 0.2876\n",
      "[300]\ttraining's rmse: 0.000399554\ttraining's RMSPE: 0.2608\tvalid_1's rmse: 0.000417843\tvalid_1's RMSPE: 0.2739\n",
      "[400]\ttraining's rmse: 0.000390043\ttraining's RMSPE: 0.2546\tvalid_1's rmse: 0.000411057\tvalid_1's RMSPE: 0.2695\n",
      "[500]\ttraining's rmse: 0.000384136\ttraining's RMSPE: 0.2507\tvalid_1's rmse: 0.000407642\tvalid_1's RMSPE: 0.2672\n",
      "[600]\ttraining's rmse: 0.000379697\ttraining's RMSPE: 0.2478\tvalid_1's rmse: 0.000405415\tvalid_1's RMSPE: 0.2658\n",
      "[700]\ttraining's rmse: 0.000376055\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000403549\tvalid_1's RMSPE: 0.2646\n",
      "[800]\ttraining's rmse: 0.000372882\ttraining's RMSPE: 0.2434\tvalid_1's rmse: 0.000402359\tvalid_1's RMSPE: 0.2638\n",
      "[900]\ttraining's rmse: 0.000370095\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.000401387\tvalid_1's RMSPE: 0.2631\n",
      "[1000]\ttraining's rmse: 0.000367585\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000400569\tvalid_1's RMSPE: 0.2626\n",
      "[1100]\ttraining's rmse: 0.000365343\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000399934\tvalid_1's RMSPE: 0.2622\n",
      "[1200]\ttraining's rmse: 0.000363397\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000399395\tvalid_1's RMSPE: 0.2618\n",
      "[1300]\ttraining's rmse: 0.000361571\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000399018\tvalid_1's RMSPE: 0.2616\n",
      "[1400]\ttraining's rmse: 0.000359869\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000398539\tvalid_1's RMSPE: 0.2613\n",
      "[1500]\ttraining's rmse: 0.000358289\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000398162\tvalid_1's RMSPE: 0.261\n",
      "[1600]\ttraining's rmse: 0.000356824\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000397894\tvalid_1's RMSPE: 0.2609\n",
      "Early stopping, best iteration is:\n",
      "[1588]\ttraining's rmse: 0.000356986\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000397872\tvalid_1's RMSPE: 0.2608\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2608\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001247\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517243\ttraining's RMSPE: 0.338\tvalid_1's rmse: 0.000519244\tvalid_1's RMSPE: 0.3387\n",
      "[200]\ttraining's rmse: 0.000423442\ttraining's RMSPE: 0.2767\tvalid_1's rmse: 0.000432252\tvalid_1's RMSPE: 0.282\n",
      "[300]\ttraining's rmse: 0.000399428\ttraining's RMSPE: 0.261\tvalid_1's rmse: 0.000411969\tvalid_1's RMSPE: 0.2687\n",
      "[400]\ttraining's rmse: 0.000389781\ttraining's RMSPE: 0.2547\tvalid_1's rmse: 0.000404513\tvalid_1's RMSPE: 0.2639\n",
      "[500]\ttraining's rmse: 0.000383841\ttraining's RMSPE: 0.2509\tvalid_1's rmse: 0.000400256\tvalid_1's RMSPE: 0.2611\n",
      "[600]\ttraining's rmse: 0.000379323\ttraining's RMSPE: 0.2479\tvalid_1's rmse: 0.000397018\tvalid_1's RMSPE: 0.259\n",
      "[700]\ttraining's rmse: 0.00037563\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000394559\tvalid_1's RMSPE: 0.2574\n",
      "[800]\ttraining's rmse: 0.000372515\ttraining's RMSPE: 0.2435\tvalid_1's rmse: 0.000392656\tvalid_1's RMSPE: 0.2561\n",
      "[900]\ttraining's rmse: 0.000369778\ttraining's RMSPE: 0.2417\tvalid_1's rmse: 0.000391018\tvalid_1's RMSPE: 0.2551\n",
      "[1000]\ttraining's rmse: 0.000367358\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000389783\tvalid_1's RMSPE: 0.2543\n",
      "[1100]\ttraining's rmse: 0.000365224\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.00038868\tvalid_1's RMSPE: 0.2535\n",
      "[1200]\ttraining's rmse: 0.000363217\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000387712\tvalid_1's RMSPE: 0.2529\n",
      "[1300]\ttraining's rmse: 0.000361358\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000386986\tvalid_1's RMSPE: 0.2524\n",
      "[1400]\ttraining's rmse: 0.000359699\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000386247\tvalid_1's RMSPE: 0.252\n",
      "[1500]\ttraining's rmse: 0.000358085\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000385543\tvalid_1's RMSPE: 0.2515\n",
      "[1600]\ttraining's rmse: 0.000356637\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000385111\tvalid_1's RMSPE: 0.2512\n",
      "[1700]\ttraining's rmse: 0.0003552\ttraining's RMSPE: 0.2321\tvalid_1's rmse: 0.00038475\tvalid_1's RMSPE: 0.251\n",
      "[1800]\ttraining's rmse: 0.000353898\ttraining's RMSPE: 0.2313\tvalid_1's rmse: 0.000384226\tvalid_1's RMSPE: 0.2506\n",
      "[1900]\ttraining's rmse: 0.000352672\ttraining's RMSPE: 0.2305\tvalid_1's rmse: 0.000383886\tvalid_1's RMSPE: 0.2504\n",
      "[2000]\ttraining's rmse: 0.000351501\ttraining's RMSPE: 0.2297\tvalid_1's rmse: 0.00038364\tvalid_1's RMSPE: 0.2503\n",
      "[2100]\ttraining's rmse: 0.00035042\ttraining's RMSPE: 0.229\tvalid_1's rmse: 0.000383287\tvalid_1's RMSPE: 0.25\n",
      "[2200]\ttraining's rmse: 0.00034929\ttraining's RMSPE: 0.2283\tvalid_1's rmse: 0.000383018\tvalid_1's RMSPE: 0.2499\n",
      "[2300]\ttraining's rmse: 0.000348249\ttraining's RMSPE: 0.2276\tvalid_1's rmse: 0.0003829\tvalid_1's RMSPE: 0.2498\n",
      "Early stopping, best iteration is:\n",
      "[2202]\ttraining's rmse: 0.000349268\ttraining's RMSPE: 0.2283\tvalid_1's rmse: 0.000383007\tvalid_1's RMSPE: 0.2498\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2498\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001244\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516485\ttraining's RMSPE: 0.3383\tvalid_1's rmse: 0.000520649\tvalid_1's RMSPE: 0.3366\n",
      "[200]\ttraining's rmse: 0.000423453\ttraining's RMSPE: 0.2774\tvalid_1's rmse: 0.00043166\tvalid_1's RMSPE: 0.2791\n",
      "[300]\ttraining's rmse: 0.000399505\ttraining's RMSPE: 0.2617\tvalid_1's rmse: 0.000411118\tvalid_1's RMSPE: 0.2658\n",
      "[400]\ttraining's rmse: 0.000389886\ttraining's RMSPE: 0.2554\tvalid_1's rmse: 0.00040401\tvalid_1's RMSPE: 0.2612\n",
      "[500]\ttraining's rmse: 0.000383962\ttraining's RMSPE: 0.2515\tvalid_1's rmse: 0.000400298\tvalid_1's RMSPE: 0.2588\n",
      "[600]\ttraining's rmse: 0.000379485\ttraining's RMSPE: 0.2486\tvalid_1's rmse: 0.000397537\tvalid_1's RMSPE: 0.257\n",
      "[700]\ttraining's rmse: 0.000375649\ttraining's RMSPE: 0.246\tvalid_1's rmse: 0.00039533\tvalid_1's RMSPE: 0.2556\n",
      "[800]\ttraining's rmse: 0.000372484\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000393592\tvalid_1's RMSPE: 0.2545\n",
      "[900]\ttraining's rmse: 0.000369689\ttraining's RMSPE: 0.2421\tvalid_1's rmse: 0.000392043\tvalid_1's RMSPE: 0.2535\n",
      "[1000]\ttraining's rmse: 0.000367249\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000391054\tvalid_1's RMSPE: 0.2528\n",
      "[1100]\ttraining's rmse: 0.000365043\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000390123\tvalid_1's RMSPE: 0.2522\n",
      "[1200]\ttraining's rmse: 0.000363007\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.00038918\tvalid_1's RMSPE: 0.2516\n",
      "[1300]\ttraining's rmse: 0.000361185\ttraining's RMSPE: 0.2366\tvalid_1's rmse: 0.000388552\tvalid_1's RMSPE: 0.2512\n",
      "[1400]\ttraining's rmse: 0.000359435\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000387831\tvalid_1's RMSPE: 0.2507\n",
      "[1500]\ttraining's rmse: 0.000357811\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.00038735\tvalid_1's RMSPE: 0.2504\n",
      "[1600]\ttraining's rmse: 0.000356328\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000386989\tvalid_1's RMSPE: 0.2502\n",
      "[1700]\ttraining's rmse: 0.000354942\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000386504\tvalid_1's RMSPE: 0.2499\n",
      "[1800]\ttraining's rmse: 0.000353667\ttraining's RMSPE: 0.2316\tvalid_1's rmse: 0.000386313\tvalid_1's RMSPE: 0.2498\n",
      "[1900]\ttraining's rmse: 0.000352424\ttraining's RMSPE: 0.2308\tvalid_1's rmse: 0.000386001\tvalid_1's RMSPE: 0.2496\n",
      "[2000]\ttraining's rmse: 0.000351256\ttraining's RMSPE: 0.2301\tvalid_1's rmse: 0.000385803\tvalid_1's RMSPE: 0.2494\n",
      "[2100]\ttraining's rmse: 0.000350109\ttraining's RMSPE: 0.2293\tvalid_1's rmse: 0.000385516\tvalid_1's RMSPE: 0.2492\n",
      "[2200]\ttraining's rmse: 0.000349009\ttraining's RMSPE: 0.2286\tvalid_1's rmse: 0.000385378\tvalid_1's RMSPE: 0.2491\n",
      "[2300]\ttraining's rmse: 0.000347974\ttraining's RMSPE: 0.2279\tvalid_1's rmse: 0.000385128\tvalid_1's RMSPE: 0.249\n",
      "[2400]\ttraining's rmse: 0.000346927\ttraining's RMSPE: 0.2272\tvalid_1's rmse: 0.000384866\tvalid_1's RMSPE: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500]\ttraining's rmse: 0.000345958\ttraining's RMSPE: 0.2266\tvalid_1's rmse: 0.000384603\tvalid_1's RMSPE: 0.2486\n",
      "Early stopping, best iteration is:\n",
      "[2482]\ttraining's rmse: 0.000346122\ttraining's RMSPE: 0.2267\tvalid_1's rmse: 0.000384604\tvalid_1's RMSPE: 0.2486\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2486\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 5\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001208\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000518238\ttraining's RMSPE: 0.3442\tvalid_1's rmse: 0.000522555\tvalid_1's RMSPE: 0.3446\n",
      "[200]\ttraining's rmse: 0.000420477\ttraining's RMSPE: 0.2793\tvalid_1's rmse: 0.000432724\tvalid_1's RMSPE: 0.2854\n",
      "[300]\ttraining's rmse: 0.000395362\ttraining's RMSPE: 0.2626\tvalid_1's rmse: 0.000410049\tvalid_1's RMSPE: 0.2704\n",
      "[400]\ttraining's rmse: 0.000385548\ttraining's RMSPE: 0.2561\tvalid_1's rmse: 0.000402029\tvalid_1's RMSPE: 0.2651\n",
      "[500]\ttraining's rmse: 0.000379698\ttraining's RMSPE: 0.2522\tvalid_1's rmse: 0.000398123\tvalid_1's RMSPE: 0.2625\n",
      "[600]\ttraining's rmse: 0.000375334\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.000395092\tvalid_1's RMSPE: 0.2605\n",
      "[700]\ttraining's rmse: 0.000371664\ttraining's RMSPE: 0.2469\tvalid_1's rmse: 0.000392771\tvalid_1's RMSPE: 0.259\n",
      "[800]\ttraining's rmse: 0.00036858\ttraining's RMSPE: 0.2448\tvalid_1's rmse: 0.000390972\tvalid_1's RMSPE: 0.2578\n",
      "[900]\ttraining's rmse: 0.00036585\ttraining's RMSPE: 0.243\tvalid_1's rmse: 0.000389555\tvalid_1's RMSPE: 0.2569\n",
      "[1000]\ttraining's rmse: 0.000363361\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000388379\tvalid_1's RMSPE: 0.2561\n",
      "[1100]\ttraining's rmse: 0.000361191\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000387394\tvalid_1's RMSPE: 0.2555\n",
      "[1200]\ttraining's rmse: 0.00035918\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000386456\tvalid_1's RMSPE: 0.2548\n",
      "[1300]\ttraining's rmse: 0.000357383\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000385917\tvalid_1's RMSPE: 0.2545\n",
      "[1400]\ttraining's rmse: 0.000355755\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000385216\tvalid_1's RMSPE: 0.254\n",
      "[1500]\ttraining's rmse: 0.000354178\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000384709\tvalid_1's RMSPE: 0.2537\n",
      "[1600]\ttraining's rmse: 0.000352642\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000384019\tvalid_1's RMSPE: 0.2532\n",
      "[1700]\ttraining's rmse: 0.000351249\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000383544\tvalid_1's RMSPE: 0.2529\n",
      "[1800]\ttraining's rmse: 0.000349942\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000383016\tvalid_1's RMSPE: 0.2526\n",
      "[1900]\ttraining's rmse: 0.000348729\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000382643\tvalid_1's RMSPE: 0.2523\n",
      "[2000]\ttraining's rmse: 0.00034752\ttraining's RMSPE: 0.2308\tvalid_1's rmse: 0.000382283\tvalid_1's RMSPE: 0.2521\n",
      "[2100]\ttraining's rmse: 0.000346392\ttraining's RMSPE: 0.2301\tvalid_1's rmse: 0.000382018\tvalid_1's RMSPE: 0.2519\n",
      "Early stopping, best iteration is:\n",
      "[2070]\ttraining's rmse: 0.000346758\ttraining's RMSPE: 0.2303\tvalid_1's rmse: 0.000382066\tvalid_1's RMSPE: 0.2519\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2519\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001250\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517379\ttraining's RMSPE: 0.3378\tvalid_1's rmse: 0.000587189\tvalid_1's RMSPE: 0.4131\n",
      "[200]\ttraining's rmse: 0.000424346\ttraining's RMSPE: 0.277\tvalid_1's rmse: 0.000490485\tvalid_1's RMSPE: 0.345\n",
      "[300]\ttraining's rmse: 0.000400687\ttraining's RMSPE: 0.2616\tvalid_1's rmse: 0.000449677\tvalid_1's RMSPE: 0.3163\n",
      "[400]\ttraining's rmse: 0.000391031\ttraining's RMSPE: 0.2553\tvalid_1's rmse: 0.000432553\tvalid_1's RMSPE: 0.3043\n",
      "[500]\ttraining's rmse: 0.000385152\ttraining's RMSPE: 0.2515\tvalid_1's rmse: 0.00042386\tvalid_1's RMSPE: 0.2982\n",
      "[600]\ttraining's rmse: 0.000380605\ttraining's RMSPE: 0.2485\tvalid_1's rmse: 0.00041941\tvalid_1's RMSPE: 0.295\n",
      "[700]\ttraining's rmse: 0.000377012\ttraining's RMSPE: 0.2461\tvalid_1's rmse: 0.00042021\tvalid_1's RMSPE: 0.2956\n",
      "Early stopping, best iteration is:\n",
      "[604]\ttraining's rmse: 0.000380447\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000418914\tvalid_1's RMSPE: 0.2947\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2947\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136510, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001200\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516758\ttraining's RMSPE: 0.3449\tvalid_1's rmse: 0.000527035\tvalid_1's RMSPE: 0.3406\n",
      "[200]\ttraining's rmse: 0.000418704\ttraining's RMSPE: 0.2795\tvalid_1's rmse: 0.000435853\tvalid_1's RMSPE: 0.2817\n",
      "[300]\ttraining's rmse: 0.000393591\ttraining's RMSPE: 0.2627\tvalid_1's rmse: 0.000413769\tvalid_1's RMSPE: 0.2674\n",
      "[400]\ttraining's rmse: 0.000383789\ttraining's RMSPE: 0.2562\tvalid_1's rmse: 0.000405813\tvalid_1's RMSPE: 0.2623\n",
      "[500]\ttraining's rmse: 0.000377938\ttraining's RMSPE: 0.2523\tvalid_1's rmse: 0.000401467\tvalid_1's RMSPE: 0.2595\n",
      "[600]\ttraining's rmse: 0.000373478\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.00039838\tvalid_1's RMSPE: 0.2575\n",
      "[700]\ttraining's rmse: 0.000369737\ttraining's RMSPE: 0.2468\tvalid_1's rmse: 0.00039595\tvalid_1's RMSPE: 0.2559\n",
      "[800]\ttraining's rmse: 0.000366626\ttraining's RMSPE: 0.2447\tvalid_1's rmse: 0.000394008\tvalid_1's RMSPE: 0.2547\n",
      "[900]\ttraining's rmse: 0.000363948\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000392403\tvalid_1's RMSPE: 0.2536\n",
      "[1000]\ttraining's rmse: 0.000361574\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000390784\tvalid_1's RMSPE: 0.2526\n",
      "[1100]\ttraining's rmse: 0.000359382\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000389844\tvalid_1's RMSPE: 0.252\n",
      "[1200]\ttraining's rmse: 0.000357422\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000388819\tvalid_1's RMSPE: 0.2513\n",
      "[1300]\ttraining's rmse: 0.000355671\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000387917\tvalid_1's RMSPE: 0.2507\n",
      "[1400]\ttraining's rmse: 0.000354067\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000387203\tvalid_1's RMSPE: 0.2503\n",
      "[1500]\ttraining's rmse: 0.000352533\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.00038655\tvalid_1's RMSPE: 0.2498\n",
      "[1600]\ttraining's rmse: 0.000351049\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000386029\tvalid_1's RMSPE: 0.2495\n",
      "[1700]\ttraining's rmse: 0.000349639\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000385535\tvalid_1's RMSPE: 0.2492\n",
      "[1800]\ttraining's rmse: 0.000348379\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000385144\tvalid_1's RMSPE: 0.2489\n",
      "[1900]\ttraining's rmse: 0.0003471\ttraining's RMSPE: 0.2317\tvalid_1's rmse: 0.000384644\tvalid_1's RMSPE: 0.2486\n",
      "[2000]\ttraining's rmse: 0.000345959\ttraining's RMSPE: 0.2309\tvalid_1's rmse: 0.000384164\tvalid_1's RMSPE: 0.2483\n",
      "[2100]\ttraining's rmse: 0.000344813\ttraining's RMSPE: 0.2302\tvalid_1's rmse: 0.000383826\tvalid_1's RMSPE: 0.2481\n",
      "[2200]\ttraining's rmse: 0.000343745\ttraining's RMSPE: 0.2294\tvalid_1's rmse: 0.000383541\tvalid_1's RMSPE: 0.2479\n",
      "[2300]\ttraining's rmse: 0.000342735\ttraining's RMSPE: 0.2288\tvalid_1's rmse: 0.000383432\tvalid_1's RMSPE: 0.2478\n",
      "Early stopping, best iteration is:\n",
      "[2238]\ttraining's rmse: 0.000343364\ttraining's RMSPE: 0.2292\tvalid_1's rmse: 0.000383474\tvalid_1's RMSPE: 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2478\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001205\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000517708\ttraining's RMSPE: 0.3445\tvalid_1's rmse: 0.00052452\tvalid_1's RMSPE: 0.3434\n",
      "[200]\ttraining's rmse: 0.000419513\ttraining's RMSPE: 0.2792\tvalid_1's rmse: 0.000440433\tvalid_1's RMSPE: 0.2884\n",
      "[300]\ttraining's rmse: 0.000394738\ttraining's RMSPE: 0.2627\tvalid_1's rmse: 0.000422891\tvalid_1's RMSPE: 0.2769\n",
      "[400]\ttraining's rmse: 0.000384988\ttraining's RMSPE: 0.2562\tvalid_1's rmse: 0.000416798\tvalid_1's RMSPE: 0.2729\n",
      "[500]\ttraining's rmse: 0.000379113\ttraining's RMSPE: 0.2523\tvalid_1's rmse: 0.000413879\tvalid_1's RMSPE: 0.271\n",
      "[600]\ttraining's rmse: 0.000374647\ttraining's RMSPE: 0.2493\tvalid_1's rmse: 0.000411636\tvalid_1's RMSPE: 0.2695\n",
      "[700]\ttraining's rmse: 0.000370961\ttraining's RMSPE: 0.2469\tvalid_1's rmse: 0.000409836\tvalid_1's RMSPE: 0.2683\n",
      "[800]\ttraining's rmse: 0.000367778\ttraining's RMSPE: 0.2447\tvalid_1's rmse: 0.000408247\tvalid_1's RMSPE: 0.2673\n",
      "[900]\ttraining's rmse: 0.000365082\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000406878\tvalid_1's RMSPE: 0.2664\n",
      "[1000]\ttraining's rmse: 0.000362711\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.0004062\tvalid_1's RMSPE: 0.2659\n",
      "[1100]\ttraining's rmse: 0.000360635\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000405998\tvalid_1's RMSPE: 0.2658\n",
      "[1200]\ttraining's rmse: 0.000358644\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000405328\tvalid_1's RMSPE: 0.2654\n",
      "[1300]\ttraining's rmse: 0.000356844\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000405041\tvalid_1's RMSPE: 0.2652\n",
      "[1400]\ttraining's rmse: 0.000355128\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000404302\tvalid_1's RMSPE: 0.2647\n",
      "[1500]\ttraining's rmse: 0.000353579\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000404009\tvalid_1's RMSPE: 0.2645\n",
      "[1600]\ttraining's rmse: 0.000352128\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.000403718\tvalid_1's RMSPE: 0.2643\n",
      "Early stopping, best iteration is:\n",
      "[1521]\ttraining's rmse: 0.000353264\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000403756\tvalid_1's RMSPE: 0.2643\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2643\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 74209\n",
      "[LightGBM] [Info] Number of data points in the train set: 136511, number of used features: 305\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001203\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000516261\ttraining's RMSPE: 0.344\tvalid_1's rmse: 0.000528746\tvalid_1's RMSPE: 0.3442\n",
      "[200]\ttraining's rmse: 0.000418191\ttraining's RMSPE: 0.2787\tvalid_1's rmse: 0.000442334\tvalid_1's RMSPE: 0.288\n",
      "[300]\ttraining's rmse: 0.000393294\ttraining's RMSPE: 0.2621\tvalid_1's rmse: 0.000421665\tvalid_1's RMSPE: 0.2745\n",
      "[400]\ttraining's rmse: 0.000383781\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.000414605\tvalid_1's RMSPE: 0.2699\n",
      "[500]\ttraining's rmse: 0.000378055\ttraining's RMSPE: 0.2519\tvalid_1's rmse: 0.000410288\tvalid_1's RMSPE: 0.2671\n",
      "[600]\ttraining's rmse: 0.000373692\ttraining's RMSPE: 0.249\tvalid_1's rmse: 0.000407475\tvalid_1's RMSPE: 0.2653\n",
      "[700]\ttraining's rmse: 0.000370004\ttraining's RMSPE: 0.2466\tvalid_1's rmse: 0.000405394\tvalid_1's RMSPE: 0.2639\n",
      "[800]\ttraining's rmse: 0.000366871\ttraining's RMSPE: 0.2445\tvalid_1's rmse: 0.000403469\tvalid_1's RMSPE: 0.2626\n",
      "[900]\ttraining's rmse: 0.000364209\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.000402229\tvalid_1's RMSPE: 0.2618\n",
      "[1000]\ttraining's rmse: 0.000361766\ttraining's RMSPE: 0.2411\tvalid_1's rmse: 0.000401001\tvalid_1's RMSPE: 0.261\n",
      "[1100]\ttraining's rmse: 0.000359546\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000400039\tvalid_1's RMSPE: 0.2604\n",
      "[1200]\ttraining's rmse: 0.000357583\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000399782\tvalid_1's RMSPE: 0.2602\n",
      "[1300]\ttraining's rmse: 0.000355796\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000399483\tvalid_1's RMSPE: 0.2601\n",
      "[1400]\ttraining's rmse: 0.000354164\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.000398941\tvalid_1's RMSPE: 0.2597\n",
      "[1500]\ttraining's rmse: 0.000352677\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000398562\tvalid_1's RMSPE: 0.2595\n",
      "[1600]\ttraining's rmse: 0.000351216\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000398216\tvalid_1's RMSPE: 0.2592\n",
      "[1700]\ttraining's rmse: 0.000349871\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000397906\tvalid_1's RMSPE: 0.259\n",
      "Early stopping, best iteration is:\n",
      "[1696]\ttraining's rmse: 0.000349912\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.00039793\tvalid_1's RMSPE: 0.259\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.259\n",
      "\t**********************************************************************\n",
      "Wall time: 16min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPSILON = 0\n",
    "extend_test = pd.DataFrame(\n",
    "    {'target_realized_volatility':[],'predicted_volatility_extend': [], 'time_id':[], 'stock_id':[]}\n",
    "\n",
    ")\n",
    "extend_models = []\n",
    "extend_split_importance = []\n",
    "extend_gain_importance = []\n",
    "train_scores = []\n",
    "inner_k = 5\n",
    "outer_k = 5\n",
    "\n",
    "params =  {\n",
    "    'boosting_type': 'goss',\n",
    "    'learning_rate': 0.01,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8, \n",
    "    'bagging_fraction': 0.8,\n",
    "    'lambda_l1': 1.2,\n",
    "    'lambda_l2': 1.2,\n",
    "    'n_jobs': -1,\n",
    "    'force_col_wise': True,\n",
    "    'extra_trees': True,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "outer_kfold = KFold(n_splits=outer_k, random_state=42, shuffle=True)\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_kfold.split(extendX, extendY)):\n",
    "    print('*'*120)\n",
    "    print(\"Outer Fold :\", outer_fold + 1)\n",
    "    print('*'*120)\n",
    "\n",
    "    X_outer_train = extendX.iloc[outer_train_idx].reset_index(drop=True)  \n",
    "    X_outer_test = extendX.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    y_outer_train = extendY.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    y_outer_test = extendY.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    \n",
    "    target = np.zeros(len(y_outer_test))\n",
    "    inner_scores = 0.0\n",
    "    models = []\n",
    "    \n",
    "    inner_kfold = KFold(n_splits= inner_k, random_state=42, shuffle=True)\n",
    "    for inner_fold, (inner_train_idx, inner_valid_idx) in enumerate(inner_kfold.split(X_outer_train, y_outer_train)):\n",
    "        print(\"\\n\\t\"+\"*\"*20)\n",
    "        print(f\"\\t*  Inner Fold : {inner_fold + 1}  *\")\n",
    "        print(\"\\t\"+\"*\"*20+\"\\n\")\n",
    "    \n",
    "        # inner train data and valid data\n",
    "        X_inner_train = X_outer_train.iloc[inner_train_idx].reset_index(drop=True)\n",
    "        X_inner_valid = X_outer_train.iloc[inner_valid_idx].reset_index(drop=True)\n",
    "\n",
    "        y_inner_train = y_outer_train.iloc[inner_train_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "        y_inner_valid = y_outer_train.iloc[inner_valid_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "            \n",
    "        lgbm_train = lgbm.Dataset(X_inner_train,y_inner_train,weight=1/(np.square(y_inner_train.values)+EPSILON))\n",
    "        lgbm_valid = lgbm.Dataset(\n",
    "            X_inner_valid,y_inner_valid,reference=lgbm_train,weight=1/(np.square(y_inner_valid.values)+EPSILON))\n",
    "        \n",
    "        # model training\n",
    "        model = lgbm.train(\n",
    "            params=params, #tuner.best_params,\n",
    "            train_set=lgbm_train,\n",
    "            valid_sets=[lgbm_train, lgbm_valid],\n",
    "            num_boost_round=10000,       \n",
    "            feval=feval_RMSPE,\n",
    "            callbacks=[lgbm.log_evaluation(period=100), lgbm.early_stopping(100)]\n",
    "        )\n",
    "        # validation \n",
    "        y_inner_pred = model.predict(X_inner_valid, num_iteration=model.best_iteration)\n",
    "        RMSPE = rmspe(\n",
    "            y_true=(y_inner_valid.values.flatten()), \n",
    "            y_pred=(y_inner_pred), n=4\n",
    "        )\n",
    "        \n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "        print(f'\\tInner Validation RMSPE: \\t{RMSPE}')\n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "\n",
    "        # keep training validation score\n",
    "        inner_scores += RMSPE / inner_k\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "        # record feature importances by gain and split\n",
    "        features = list(X_inner_train.columns.values)\n",
    "        \n",
    "        extend_gain_importance.append(compute_importance(model, features, typ='gain'))\n",
    "        extend_split_importance.append(compute_importance(model, features, typ='split'))\n",
    "        \n",
    "    # store all models for prediction in oof evaluation\n",
    "    extend_models.append(models)\n",
    "    train_scores.append(inner_scores)\n",
    "    \n",
    "    # out of fold test set\n",
    "    for model in extend_models[outer_fold]:\n",
    "        y_outer_pred = model.predict(X_outer_test,num_iteration=model.best_iteration)\n",
    "        target += y_outer_pred / len(extend_models[outer_fold])\n",
    "    \n",
    "    y_outer_test = y_outer_test.assign(predicted_volatility_extend = target)\n",
    " \n",
    "    extend_test = pd.concat([extend_test, y_outer_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afb04ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.260842"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(extend_test['target_realized_volatility'], extend_test['predicted_volatility_extend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37251268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>predicted_volatility_extend</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>454.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>627.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213292</th>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>32361.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213293</th>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>32614.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213294</th>\n",
       "      <td>0.012813</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>32649.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213295</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213296</th>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>32748.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213297 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  predicted_volatility_extend  time_id  \\\n",
       "0                         0.001498                     0.002114    169.0   \n",
       "1                         0.001420                     0.001232    211.0   \n",
       "2                         0.003371                     0.002278    266.0   \n",
       "3                         0.001155                     0.001133    454.0   \n",
       "4                         0.001119                     0.001276    627.0   \n",
       "...                            ...                          ...      ...   \n",
       "213292                    0.002960                     0.002583  32361.0   \n",
       "213293                    0.001725                     0.001673  32614.0   \n",
       "213294                    0.012813                     0.009662  32649.0   \n",
       "213295                    0.003511                     0.004358  32724.0   \n",
       "213296                    0.003057                     0.002234  32748.0   \n",
       "\n",
       "        stock_id  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "...          ...  \n",
       "213292     126.0  \n",
       "213293     126.0  \n",
       "213294     126.0  \n",
       "213295     126.0  \n",
       "213296     126.0  \n",
       "\n",
       "[213297 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extend_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9372db",
   "metadata": {},
   "source": [
    "### Using Normalised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37162e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  5.3min finished\n"
     ]
    }
   ],
   "source": [
    "norm = process_stocks(beta.stock_id.unique(), extend=False, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62d2b82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>1.003701</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>1.003655</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>2.449679e-10</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>1.000025</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>5.966433e-13</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>1.000027</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>1.000102</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>4.260982e-11</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.999144</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.998773</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>3.424817e-11</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>1.021313e-11</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>1.182409e-10</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>1.001540</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>1.001618</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>3.711303e-11</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>1.000707</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>1.000623</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>8.922521e-12</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>1.001811</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>1.001805</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>1.357511e-10</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>1.000289</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>1.000429</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>8.055744e-12</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954      1.003701     0.000830   \n",
       "1            11                    0.000981      1.000025     0.000154   \n",
       "2            16                    0.001295      1.000027     0.000420   \n",
       "3            31                    0.001776      0.999144     0.000780   \n",
       "4            62                    0.001520      0.999752     0.000239   \n",
       "...         ...                         ...           ...          ...   \n",
       "428927    32751                    0.002899      0.999757     0.000538   \n",
       "428928    32753                    0.003454      1.001540     0.000556   \n",
       "428929    32758                    0.002792      1.000707     0.000298   \n",
       "428930    32763                    0.002379      1.001811     0.000441   \n",
       "428931    32767                    0.001414      1.000289     0.000227   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0            1.003655      0.000918                             0.003394   \n",
       "1            1.000016      0.000183                             0.000699   \n",
       "2            1.000102      0.000474                             0.001983   \n",
       "3            0.998773      0.000724                             0.001863   \n",
       "4            0.999638      0.000358                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428927       0.999857      0.000664                             0.002284   \n",
       "428928       1.001618      0.000632                             0.002217   \n",
       "428929       1.000623      0.000393                             0.001386   \n",
       "428930       1.001805      0.000497                             0.002783   \n",
       "428931       1.000429      0.000287                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000267                2.449679e-10   \n",
       "1                                  0.000076                5.966433e-13   \n",
       "2                                  0.000182                4.260982e-11   \n",
       "3                                  0.000229                3.424817e-11   \n",
       "4                                  0.000122                1.021313e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000177                1.182409e-10   \n",
       "428928                             0.000205                3.711303e-11   \n",
       "428929                             0.000148                8.922521e-12   \n",
       "428930                             0.000193                1.357511e-10   \n",
       "428931                             0.000141                8.055744e-12   \n",
       "\n",
       "        log_returns_mean_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0                   0.000013  ...            1              4   \n",
       "1                   0.000003  ...            1              4   \n",
       "2                   0.000004  ...            1              4   \n",
       "3                  -0.000025  ...            1              4   \n",
       "4                  -0.000008  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "428927             -0.000005  ...            1              2   \n",
       "428928              0.000009  ...            1              2   \n",
       "428929             -0.000008  ...            1              2   \n",
       "428930              0.000010  ...            1              2   \n",
       "428931             -0.000003  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428932 rows x 195 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "598dd6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.to_feather(\"norm.fth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d07aac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm[norm['target_realized_volatility']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a79c411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>bds_kmeans4</th>\n",
       "      <th>bds_agg_ward4</th>\n",
       "      <th>bds_dbscan_ward5</th>\n",
       "      <th>bds_gmm6</th>\n",
       "      <th>som_bmu1D</th>\n",
       "      <th>bmu2D_km5</th>\n",
       "      <th>bmu2D_agg3</th>\n",
       "      <th>bmu2D_gmm19</th>\n",
       "      <th>bmuW_km3</th>\n",
       "      <th>bmuW_agg2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>1.003701</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>1.003655</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>2.449679e-10</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>1.000025</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>5.966433e-13</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>1.000027</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>1.000102</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>4.260982e-11</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.999144</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.998773</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>3.424817e-11</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>1.021313e-11</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>1.182409e-10</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>1.001540</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>1.001618</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>3.711303e-11</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>1.000707</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>1.000623</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>8.922521e-12</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>1.001811</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>1.001805</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>1.357511e-10</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>1.000289</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>1.000429</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>8.055744e-12</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954      1.003701     0.000830   \n",
       "1            11                    0.000981      1.000025     0.000154   \n",
       "2            16                    0.001295      1.000027     0.000420   \n",
       "3            31                    0.001776      0.999144     0.000780   \n",
       "4            62                    0.001520      0.999752     0.000239   \n",
       "...         ...                         ...           ...          ...   \n",
       "428927    32751                    0.002899      0.999757     0.000538   \n",
       "428928    32753                    0.003454      1.001540     0.000556   \n",
       "428929    32758                    0.002792      1.000707     0.000298   \n",
       "428930    32763                    0.002379      1.001811     0.000441   \n",
       "428931    32767                    0.001414      1.000289     0.000227   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0            1.003655      0.000918                             0.003394   \n",
       "1            1.000016      0.000183                             0.000699   \n",
       "2            1.000102      0.000474                             0.001983   \n",
       "3            0.998773      0.000724                             0.001863   \n",
       "4            0.999638      0.000358                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428927       0.999857      0.000664                             0.002284   \n",
       "428928       1.001618      0.000632                             0.002217   \n",
       "428929       1.000623      0.000393                             0.001386   \n",
       "428930       1.001805      0.000497                             0.002783   \n",
       "428931       1.000429      0.000287                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000267                2.449679e-10   \n",
       "1                                  0.000076                5.966433e-13   \n",
       "2                                  0.000182                4.260982e-11   \n",
       "3                                  0.000229                3.424817e-11   \n",
       "4                                  0.000122                1.021313e-11   \n",
       "...                                     ...                         ...   \n",
       "428927                             0.000177                1.182409e-10   \n",
       "428928                             0.000205                3.711303e-11   \n",
       "428929                             0.000148                8.922521e-12   \n",
       "428930                             0.000193                1.357511e-10   \n",
       "428931                             0.000141                8.055744e-12   \n",
       "\n",
       "        log_returns_mean_300  ...  bds_kmeans4  bds_agg_ward4  \\\n",
       "0                   0.000013  ...            1              4   \n",
       "1                   0.000003  ...            1              4   \n",
       "2                   0.000004  ...            1              4   \n",
       "3                  -0.000025  ...            1              4   \n",
       "4                  -0.000008  ...            1              4   \n",
       "...                      ...  ...          ...            ...   \n",
       "428927             -0.000005  ...            1              2   \n",
       "428928              0.000009  ...            1              2   \n",
       "428929             -0.000008  ...            1              2   \n",
       "428930              0.000010  ...            1              2   \n",
       "428931             -0.000003  ...            1              2   \n",
       "\n",
       "        bds_dbscan_ward5  bds_gmm6  som_bmu1D  bmu2D_km5  bmu2D_agg3  \\\n",
       "0                      0         0          1          1           3   \n",
       "1                      0         0          1          1           3   \n",
       "2                      0         0          1          1           3   \n",
       "3                      0         0          1          1           3   \n",
       "4                      0         0          1          1           3   \n",
       "...                  ...       ...        ...        ...         ...   \n",
       "428927                -1         2          1          1           3   \n",
       "428928                -1         2          1          1           3   \n",
       "428929                -1         2          1          1           3   \n",
       "428930                -1         2          1          1           3   \n",
       "428931                -1         2          1          1           3   \n",
       "\n",
       "        bmu2D_gmm19  bmuW_km3  bmuW_agg2  \n",
       "0                14         1          1  \n",
       "1                14         1          1  \n",
       "2                14         1          1  \n",
       "3                14         1          1  \n",
       "4                14         1          1  \n",
       "...             ...       ...        ...  \n",
       "428927           12         1          1  \n",
       "428928           12         1          1  \n",
       "428929           12         1          1  \n",
       "428930           12         1          1  \n",
       "428931           12         1          1  \n",
       "\n",
       "[428931 rows x 195 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = norm[norm['target_realized_volatility'] != 0]#.sort_values(['stock_id','time_id']).reset_index(drop=True)\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40511723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm[norm.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11bee3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "normX = norm.drop(['time_id','target_realized_volatility'], axis=1)  # leave stock id as feature\n",
    "normY = norm[['target_realized_volatility', 'stock_id', 'time_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c273c4",
   "metadata": {},
   "source": [
    "### Can recover lost rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "011ad47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "Outer Fold : 1\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274515, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488689\ttraining's RMSPE: 0.335\tvalid_1's rmse: 0.000502472\tvalid_1's RMSPE: 0.3442\n",
      "[200]\ttraining's rmse: 0.000395031\ttraining's RMSPE: 0.2708\tvalid_1's rmse: 0.000420393\tvalid_1's RMSPE: 0.288\n",
      "[300]\ttraining's rmse: 0.000373085\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000402939\tvalid_1's RMSPE: 0.276\n",
      "[400]\ttraining's rmse: 0.000365596\ttraining's RMSPE: 0.2506\tvalid_1's rmse: 0.000396616\tvalid_1's RMSPE: 0.2717\n",
      "[500]\ttraining's rmse: 0.000361631\ttraining's RMSPE: 0.2479\tvalid_1's rmse: 0.000393612\tvalid_1's RMSPE: 0.2696\n",
      "[600]\ttraining's rmse: 0.000358684\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000391046\tvalid_1's RMSPE: 0.2679\n",
      "[700]\ttraining's rmse: 0.000356307\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000389678\tvalid_1's RMSPE: 0.2669\n",
      "[800]\ttraining's rmse: 0.000354326\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000388534\tvalid_1's RMSPE: 0.2662\n",
      "[900]\ttraining's rmse: 0.000352552\ttraining's RMSPE: 0.2417\tvalid_1's rmse: 0.000387318\tvalid_1's RMSPE: 0.2653\n",
      "[1000]\ttraining's rmse: 0.00035096\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000386096\tvalid_1's RMSPE: 0.2645\n",
      "[1100]\ttraining's rmse: 0.000349608\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000385677\tvalid_1's RMSPE: 0.2642\n",
      "[1200]\ttraining's rmse: 0.000348352\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000385671\tvalid_1's RMSPE: 0.2642\n",
      "Early stopping, best iteration is:\n",
      "[1110]\ttraining's rmse: 0.000349473\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000385601\tvalid_1's RMSPE: 0.2641\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2641\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274515, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001171\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489282\ttraining's RMSPE: 0.3363\tvalid_1's rmse: 0.000492284\tvalid_1's RMSPE: 0.3338\n",
      "[200]\ttraining's rmse: 0.000394561\ttraining's RMSPE: 0.2712\tvalid_1's rmse: 0.000403974\tvalid_1's RMSPE: 0.2739\n",
      "[300]\ttraining's rmse: 0.000372014\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.000383875\tvalid_1's RMSPE: 0.2603\n",
      "[400]\ttraining's rmse: 0.000364399\ttraining's RMSPE: 0.2504\tvalid_1's rmse: 0.000377839\tvalid_1's RMSPE: 0.2562\n",
      "[500]\ttraining's rmse: 0.000360317\ttraining's RMSPE: 0.2476\tvalid_1's rmse: 0.000375042\tvalid_1's RMSPE: 0.2543\n",
      "[600]\ttraining's rmse: 0.000357362\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.000373192\tvalid_1's RMSPE: 0.253\n",
      "[700]\ttraining's rmse: 0.000354951\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000371881\tvalid_1's RMSPE: 0.2521\n",
      "[800]\ttraining's rmse: 0.000352935\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000370869\tvalid_1's RMSPE: 0.2514\n",
      "[900]\ttraining's rmse: 0.00035115\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000370024\tvalid_1's RMSPE: 0.2509\n",
      "[1000]\ttraining's rmse: 0.000349613\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.00036924\tvalid_1's RMSPE: 0.2503\n",
      "[1100]\ttraining's rmse: 0.000348224\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000368673\tvalid_1's RMSPE: 0.2499\n",
      "[1200]\ttraining's rmse: 0.000346956\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000368191\tvalid_1's RMSPE: 0.2496\n",
      "[1300]\ttraining's rmse: 0.000345824\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.000367767\tvalid_1's RMSPE: 0.2493\n",
      "[1400]\ttraining's rmse: 0.000344761\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000367406\tvalid_1's RMSPE: 0.2491\n",
      "[1500]\ttraining's rmse: 0.00034377\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000367132\tvalid_1's RMSPE: 0.2489\n",
      "[1600]\ttraining's rmse: 0.000342859\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.00036679\tvalid_1's RMSPE: 0.2487\n",
      "[1700]\ttraining's rmse: 0.000342007\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000366478\tvalid_1's RMSPE: 0.2485\n",
      "[1800]\ttraining's rmse: 0.000341201\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000366238\tvalid_1's RMSPE: 0.2483\n",
      "[1900]\ttraining's rmse: 0.000340422\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000366039\tvalid_1's RMSPE: 0.2482\n",
      "[2000]\ttraining's rmse: 0.000339679\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000365871\tvalid_1's RMSPE: 0.248\n",
      "[2100]\ttraining's rmse: 0.000339012\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.00036571\tvalid_1's RMSPE: 0.2479\n",
      "[2200]\ttraining's rmse: 0.000338351\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000365517\tvalid_1's RMSPE: 0.2478\n",
      "[2300]\ttraining's rmse: 0.000337723\ttraining's RMSPE: 0.2321\tvalid_1's rmse: 0.0003654\tvalid_1's RMSPE: 0.2477\n",
      "Early stopping, best iteration is:\n",
      "[2247]\ttraining's rmse: 0.000338043\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000365431\tvalid_1's RMSPE: 0.2477\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2477\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274515, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001176\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489176\ttraining's RMSPE: 0.3357\tvalid_1's rmse: 0.000494745\tvalid_1's RMSPE: 0.3375\n",
      "[200]\ttraining's rmse: 0.000395146\ttraining's RMSPE: 0.2712\tvalid_1's rmse: 0.000403221\tvalid_1's RMSPE: 0.275\n",
      "[300]\ttraining's rmse: 0.000372725\ttraining's RMSPE: 0.2558\tvalid_1's rmse: 0.000382314\tvalid_1's RMSPE: 0.2608\n",
      "[400]\ttraining's rmse: 0.000364967\ttraining's RMSPE: 0.2505\tvalid_1's rmse: 0.000375911\tvalid_1's RMSPE: 0.2564\n",
      "[500]\ttraining's rmse: 0.000360793\ttraining's RMSPE: 0.2476\tvalid_1's rmse: 0.00037323\tvalid_1's RMSPE: 0.2546\n",
      "[600]\ttraining's rmse: 0.000357817\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.000371608\tvalid_1's RMSPE: 0.2535\n",
      "[700]\ttraining's rmse: 0.000355465\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000370524\tvalid_1's RMSPE: 0.2527\n",
      "[800]\ttraining's rmse: 0.000353412\ttraining's RMSPE: 0.2425\tvalid_1's rmse: 0.000369673\tvalid_1's RMSPE: 0.2522\n",
      "[900]\ttraining's rmse: 0.0003516\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000368895\tvalid_1's RMSPE: 0.2516\n",
      "[1000]\ttraining's rmse: 0.000350056\ttraining's RMSPE: 0.2402\tvalid_1's rmse: 0.000368213\tvalid_1's RMSPE: 0.2512\n",
      "[1100]\ttraining's rmse: 0.000348646\ttraining's RMSPE: 0.2393\tvalid_1's rmse: 0.000367605\tvalid_1's RMSPE: 0.2507\n",
      "[1200]\ttraining's rmse: 0.00034735\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000367242\tvalid_1's RMSPE: 0.2505\n",
      "[1300]\ttraining's rmse: 0.000346241\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000366914\tvalid_1's RMSPE: 0.2503\n",
      "[1400]\ttraining's rmse: 0.000345203\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000366574\tvalid_1's RMSPE: 0.25\n",
      "[1500]\ttraining's rmse: 0.000344282\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000366238\tvalid_1's RMSPE: 0.2498\n",
      "[1600]\ttraining's rmse: 0.000343333\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000365907\tvalid_1's RMSPE: 0.2496\n",
      "[1700]\ttraining's rmse: 0.00034244\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000365628\tvalid_1's RMSPE: 0.2494\n",
      "[1800]\ttraining's rmse: 0.000341649\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000365434\tvalid_1's RMSPE: 0.2493\n",
      "[1900]\ttraining's rmse: 0.000340852\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000365186\tvalid_1's RMSPE: 0.2491\n",
      "[2000]\ttraining's rmse: 0.000340113\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000364983\tvalid_1's RMSPE: 0.249\n",
      "[2100]\ttraining's rmse: 0.000339422\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000364857\tvalid_1's RMSPE: 0.2489\n",
      "Early stopping, best iteration is:\n",
      "[2005]\ttraining's rmse: 0.000340079\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000364972\tvalid_1's RMSPE: 0.2489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2489\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274515, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001174\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488206\ttraining's RMSPE: 0.3352\tvalid_1's rmse: 0.000501354\tvalid_1's RMSPE: 0.3414\n",
      "[200]\ttraining's rmse: 0.000394014\ttraining's RMSPE: 0.2705\tvalid_1's rmse: 0.000421962\tvalid_1's RMSPE: 0.2873\n",
      "[300]\ttraining's rmse: 0.000371991\ttraining's RMSPE: 0.2554\tvalid_1's rmse: 0.000407285\tvalid_1's RMSPE: 0.2773\n",
      "[400]\ttraining's rmse: 0.000364496\ttraining's RMSPE: 0.2502\tvalid_1's rmse: 0.000403626\tvalid_1's RMSPE: 0.2748\n",
      "[500]\ttraining's rmse: 0.000360469\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000401462\tvalid_1's RMSPE: 0.2734\n",
      "[600]\ttraining's rmse: 0.000357613\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000399824\tvalid_1's RMSPE: 0.2722\n",
      "[700]\ttraining's rmse: 0.000355303\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000398684\tvalid_1's RMSPE: 0.2715\n",
      "[800]\ttraining's rmse: 0.000353374\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000397534\tvalid_1's RMSPE: 0.2707\n",
      "[900]\ttraining's rmse: 0.00035161\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000396778\tvalid_1's RMSPE: 0.2702\n",
      "[1000]\ttraining's rmse: 0.000350066\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000395908\tvalid_1's RMSPE: 0.2696\n",
      "[1100]\ttraining's rmse: 0.000348695\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000395726\tvalid_1's RMSPE: 0.2695\n",
      "[1200]\ttraining's rmse: 0.000347471\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.00039507\tvalid_1's RMSPE: 0.269\n",
      "[1300]\ttraining's rmse: 0.000346306\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000394505\tvalid_1's RMSPE: 0.2686\n",
      "[1400]\ttraining's rmse: 0.000345248\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000393842\tvalid_1's RMSPE: 0.2682\n",
      "[1500]\ttraining's rmse: 0.000344265\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.00039352\tvalid_1's RMSPE: 0.268\n",
      "Early stopping, best iteration is:\n",
      "[1478]\ttraining's rmse: 0.000344481\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000393505\tvalid_1's RMSPE: 0.2679\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2679\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001192\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000490088\ttraining's RMSPE: 0.334\tvalid_1's rmse: 0.000515628\tvalid_1's RMSPE: 0.3614\n",
      "[200]\ttraining's rmse: 0.000397741\ttraining's RMSPE: 0.2711\tvalid_1's rmse: 0.000413677\tvalid_1's RMSPE: 0.2899\n",
      "[300]\ttraining's rmse: 0.000375767\ttraining's RMSPE: 0.2561\tvalid_1's rmse: 0.000388946\tvalid_1's RMSPE: 0.2726\n",
      "[400]\ttraining's rmse: 0.00036812\ttraining's RMSPE: 0.2509\tvalid_1's rmse: 0.000381335\tvalid_1's RMSPE: 0.2673\n",
      "[500]\ttraining's rmse: 0.000363917\ttraining's RMSPE: 0.248\tvalid_1's rmse: 0.000376264\tvalid_1's RMSPE: 0.2637\n",
      "[600]\ttraining's rmse: 0.000360888\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000374224\tvalid_1's RMSPE: 0.2623\n",
      "[700]\ttraining's rmse: 0.000358449\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000372707\tvalid_1's RMSPE: 0.2612\n",
      "[800]\ttraining's rmse: 0.000356292\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000370408\tvalid_1's RMSPE: 0.2596\n",
      "[900]\ttraining's rmse: 0.000354508\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.000369723\tvalid_1's RMSPE: 0.2591\n",
      "[1000]\ttraining's rmse: 0.00035289\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000370203\tvalid_1's RMSPE: 0.2595\n",
      "Early stopping, best iteration is:\n",
      "[988]\ttraining's rmse: 0.000353084\ttraining's RMSPE: 0.2406\tvalid_1's rmse: 0.000369011\tvalid_1's RMSPE: 0.2586\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2586\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 2\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001192\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488858\ttraining's RMSPE: 0.3331\tvalid_1's rmse: 0.000498136\tvalid_1's RMSPE: 0.3362\n",
      "[200]\ttraining's rmse: 0.000396087\ttraining's RMSPE: 0.2699\tvalid_1's rmse: 0.000413686\tvalid_1's RMSPE: 0.2792\n",
      "[300]\ttraining's rmse: 0.000374697\ttraining's RMSPE: 0.2553\tvalid_1's rmse: 0.000396451\tvalid_1's RMSPE: 0.2675\n",
      "[400]\ttraining's rmse: 0.000367383\ttraining's RMSPE: 0.2503\tvalid_1's rmse: 0.000391346\tvalid_1's RMSPE: 0.2641\n",
      "[500]\ttraining's rmse: 0.000363363\ttraining's RMSPE: 0.2476\tvalid_1's rmse: 0.000388756\tvalid_1's RMSPE: 0.2623\n",
      "[600]\ttraining's rmse: 0.000360492\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.000387101\tvalid_1's RMSPE: 0.2612\n",
      "[700]\ttraining's rmse: 0.000358083\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000385683\tvalid_1's RMSPE: 0.2603\n",
      "[800]\ttraining's rmse: 0.000356026\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000384348\tvalid_1's RMSPE: 0.2594\n",
      "[900]\ttraining's rmse: 0.000354227\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000383482\tvalid_1's RMSPE: 0.2588\n",
      "[1000]\ttraining's rmse: 0.000352698\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000382997\tvalid_1's RMSPE: 0.2585\n",
      "[1100]\ttraining's rmse: 0.000351332\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000382437\tvalid_1's RMSPE: 0.2581\n",
      "[1200]\ttraining's rmse: 0.000350117\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000382052\tvalid_1's RMSPE: 0.2578\n",
      "[1300]\ttraining's rmse: 0.000348979\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000381574\tvalid_1's RMSPE: 0.2575\n",
      "[1400]\ttraining's rmse: 0.000347927\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000381362\tvalid_1's RMSPE: 0.2574\n",
      "[1500]\ttraining's rmse: 0.000346918\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.000381004\tvalid_1's RMSPE: 0.2571\n",
      "[1600]\ttraining's rmse: 0.000346\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000380768\tvalid_1's RMSPE: 0.257\n",
      "[1700]\ttraining's rmse: 0.000345163\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000380699\tvalid_1's RMSPE: 0.2569\n",
      "Early stopping, best iteration is:\n",
      "[1669]\ttraining's rmse: 0.000345416\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000380616\tvalid_1's RMSPE: 0.2568\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2568\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001197\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000490546\ttraining's RMSPE: 0.3333\tvalid_1's rmse: 0.000491361\tvalid_1's RMSPE: 0.3353\n",
      "[200]\ttraining's rmse: 0.000398413\ttraining's RMSPE: 0.2707\tvalid_1's rmse: 0.000403271\tvalid_1's RMSPE: 0.2751\n",
      "[300]\ttraining's rmse: 0.000376661\ttraining's RMSPE: 0.2559\tvalid_1's rmse: 0.000384129\tvalid_1's RMSPE: 0.2621\n",
      "[400]\ttraining's rmse: 0.000369312\ttraining's RMSPE: 0.2509\tvalid_1's rmse: 0.000378329\tvalid_1's RMSPE: 0.2581\n",
      "[500]\ttraining's rmse: 0.000365239\ttraining's RMSPE: 0.2482\tvalid_1's rmse: 0.000375488\tvalid_1's RMSPE: 0.2562\n",
      "[600]\ttraining's rmse: 0.000362301\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000373838\tvalid_1's RMSPE: 0.2551\n",
      "[700]\ttraining's rmse: 0.000359867\ttraining's RMSPE: 0.2445\tvalid_1's rmse: 0.000372437\tvalid_1's RMSPE: 0.2541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttraining's rmse: 0.000357784\ttraining's RMSPE: 0.2431\tvalid_1's rmse: 0.000371314\tvalid_1's RMSPE: 0.2533\n",
      "[900]\ttraining's rmse: 0.000355976\ttraining's RMSPE: 0.2419\tvalid_1's rmse: 0.000370564\tvalid_1's RMSPE: 0.2528\n",
      "[1000]\ttraining's rmse: 0.000354355\ttraining's RMSPE: 0.2408\tvalid_1's rmse: 0.000369878\tvalid_1's RMSPE: 0.2524\n",
      "[1100]\ttraining's rmse: 0.000352931\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000369175\tvalid_1's RMSPE: 0.2519\n",
      "[1200]\ttraining's rmse: 0.000351631\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.00036864\tvalid_1's RMSPE: 0.2515\n",
      "[1300]\ttraining's rmse: 0.000350448\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000368004\tvalid_1's RMSPE: 0.2511\n",
      "[1400]\ttraining's rmse: 0.000349395\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000367582\tvalid_1's RMSPE: 0.2508\n",
      "[1500]\ttraining's rmse: 0.000348376\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000367184\tvalid_1's RMSPE: 0.2505\n",
      "[1600]\ttraining's rmse: 0.000347413\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000366728\tvalid_1's RMSPE: 0.2502\n",
      "[1700]\ttraining's rmse: 0.000346495\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000366369\tvalid_1's RMSPE: 0.25\n",
      "[1800]\ttraining's rmse: 0.000345678\ttraining's RMSPE: 0.2349\tvalid_1's rmse: 0.000366192\tvalid_1's RMSPE: 0.2498\n",
      "[1900]\ttraining's rmse: 0.00034493\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000365964\tvalid_1's RMSPE: 0.2497\n",
      "[2000]\ttraining's rmse: 0.000344188\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000365759\tvalid_1's RMSPE: 0.2496\n",
      "[2100]\ttraining's rmse: 0.000343473\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000365519\tvalid_1's RMSPE: 0.2494\n",
      "[2200]\ttraining's rmse: 0.000342788\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000365422\tvalid_1's RMSPE: 0.2493\n",
      "Early stopping, best iteration is:\n",
      "[2170]\ttraining's rmse: 0.000342989\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000365457\tvalid_1's RMSPE: 0.2493\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2493\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001195\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489493\ttraining's RMSPE: 0.333\tvalid_1's rmse: 0.00049439\tvalid_1's RMSPE: 0.3355\n",
      "[200]\ttraining's rmse: 0.000397002\ttraining's RMSPE: 0.2701\tvalid_1's rmse: 0.000405317\tvalid_1's RMSPE: 0.2751\n",
      "[300]\ttraining's rmse: 0.000375092\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000385294\tvalid_1's RMSPE: 0.2615\n",
      "[400]\ttraining's rmse: 0.00036762\ttraining's RMSPE: 0.2501\tvalid_1's rmse: 0.000379178\tvalid_1's RMSPE: 0.2573\n",
      "[500]\ttraining's rmse: 0.000363523\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.000376132\tvalid_1's RMSPE: 0.2553\n",
      "[600]\ttraining's rmse: 0.000360571\ttraining's RMSPE: 0.2453\tvalid_1's rmse: 0.000374282\tvalid_1's RMSPE: 0.254\n",
      "[700]\ttraining's rmse: 0.000358189\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000372802\tvalid_1's RMSPE: 0.253\n",
      "[800]\ttraining's rmse: 0.000356196\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000371731\tvalid_1's RMSPE: 0.2523\n",
      "[900]\ttraining's rmse: 0.000354374\ttraining's RMSPE: 0.2411\tvalid_1's rmse: 0.000370791\tvalid_1's RMSPE: 0.2516\n",
      "[1000]\ttraining's rmse: 0.000352779\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.00037009\tvalid_1's RMSPE: 0.2512\n",
      "[1100]\ttraining's rmse: 0.000351387\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.00036947\tvalid_1's RMSPE: 0.2507\n",
      "[1200]\ttraining's rmse: 0.000350152\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000368952\tvalid_1's RMSPE: 0.2504\n",
      "[1300]\ttraining's rmse: 0.000348964\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000368419\tvalid_1's RMSPE: 0.25\n",
      "[1400]\ttraining's rmse: 0.000347886\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000367983\tvalid_1's RMSPE: 0.2497\n",
      "[1500]\ttraining's rmse: 0.000346897\ttraining's RMSPE: 0.236\tvalid_1's rmse: 0.00036773\tvalid_1's RMSPE: 0.2496\n",
      "[1600]\ttraining's rmse: 0.000345992\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000367435\tvalid_1's RMSPE: 0.2494\n",
      "[1700]\ttraining's rmse: 0.000345123\ttraining's RMSPE: 0.2348\tvalid_1's rmse: 0.000367115\tvalid_1's RMSPE: 0.2491\n",
      "[1800]\ttraining's rmse: 0.000344306\ttraining's RMSPE: 0.2343\tvalid_1's rmse: 0.00036684\tvalid_1's RMSPE: 0.2489\n",
      "[1900]\ttraining's rmse: 0.000343569\ttraining's RMSPE: 0.2338\tvalid_1's rmse: 0.000366508\tvalid_1's RMSPE: 0.2487\n",
      "[2000]\ttraining's rmse: 0.000342849\ttraining's RMSPE: 0.2333\tvalid_1's rmse: 0.000366336\tvalid_1's RMSPE: 0.2486\n",
      "[2100]\ttraining's rmse: 0.000342143\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.0003661\tvalid_1's RMSPE: 0.2484\n",
      "Early stopping, best iteration is:\n",
      "[2096]\ttraining's rmse: 0.000342168\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000366105\tvalid_1's RMSPE: 0.2484\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2484\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001203\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488777\ttraining's RMSPE: 0.3312\tvalid_1's rmse: 0.000509943\tvalid_1's RMSPE: 0.3517\n",
      "[200]\ttraining's rmse: 0.000396858\ttraining's RMSPE: 0.2689\tvalid_1's rmse: 0.000428545\tvalid_1's RMSPE: 0.2955\n",
      "[300]\ttraining's rmse: 0.000375634\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000411276\tvalid_1's RMSPE: 0.2836\n",
      "[400]\ttraining's rmse: 0.000368505\ttraining's RMSPE: 0.2497\tvalid_1's rmse: 0.000405186\tvalid_1's RMSPE: 0.2794\n",
      "[500]\ttraining's rmse: 0.000364599\ttraining's RMSPE: 0.2471\tvalid_1's rmse: 0.000402147\tvalid_1's RMSPE: 0.2773\n",
      "[600]\ttraining's rmse: 0.000361763\ttraining's RMSPE: 0.2451\tvalid_1's rmse: 0.000400344\tvalid_1's RMSPE: 0.2761\n",
      "[700]\ttraining's rmse: 0.000359466\ttraining's RMSPE: 0.2436\tvalid_1's rmse: 0.000398453\tvalid_1's RMSPE: 0.2748\n",
      "[800]\ttraining's rmse: 0.000357424\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000396841\tvalid_1's RMSPE: 0.2737\n",
      "[900]\ttraining's rmse: 0.000355703\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000396102\tvalid_1's RMSPE: 0.2732\n",
      "[1000]\ttraining's rmse: 0.000354176\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000395028\tvalid_1's RMSPE: 0.2724\n",
      "[1100]\ttraining's rmse: 0.00035285\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000394506\tvalid_1's RMSPE: 0.2721\n",
      "Early stopping, best iteration is:\n",
      "[1069]\ttraining's rmse: 0.00035324\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000394497\tvalid_1's RMSPE: 0.272\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.272\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001192\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048991\ttraining's RMSPE: 0.3338\tvalid_1's rmse: 0.00049071\tvalid_1's RMSPE: 0.3311\n",
      "[200]\ttraining's rmse: 0.000397104\ttraining's RMSPE: 0.2706\tvalid_1's rmse: 0.000399804\tvalid_1's RMSPE: 0.2698\n",
      "[300]\ttraining's rmse: 0.000375274\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.00037983\tvalid_1's RMSPE: 0.2563\n",
      "[400]\ttraining's rmse: 0.00036779\ttraining's RMSPE: 0.2506\tvalid_1's rmse: 0.000373974\tvalid_1's RMSPE: 0.2523\n",
      "[500]\ttraining's rmse: 0.000363629\ttraining's RMSPE: 0.2478\tvalid_1's rmse: 0.000371248\tvalid_1's RMSPE: 0.2505\n",
      "[600]\ttraining's rmse: 0.000360679\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000369502\tvalid_1's RMSPE: 0.2493\n",
      "[700]\ttraining's rmse: 0.000358337\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000368228\tvalid_1's RMSPE: 0.2485\n",
      "[800]\ttraining's rmse: 0.000356291\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000367138\tvalid_1's RMSPE: 0.2477\n",
      "[900]\ttraining's rmse: 0.00035454\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.000366201\tvalid_1's RMSPE: 0.2471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's rmse: 0.000352961\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000365365\tvalid_1's RMSPE: 0.2465\n",
      "[1100]\ttraining's rmse: 0.000351551\ttraining's RMSPE: 0.2395\tvalid_1's rmse: 0.000364657\tvalid_1's RMSPE: 0.246\n",
      "[1200]\ttraining's rmse: 0.000350291\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000364076\tvalid_1's RMSPE: 0.2457\n",
      "[1300]\ttraining's rmse: 0.000349142\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.00036359\tvalid_1's RMSPE: 0.2453\n",
      "[1400]\ttraining's rmse: 0.000348086\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.00036322\tvalid_1's RMSPE: 0.2451\n",
      "[1500]\ttraining's rmse: 0.000347114\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000362855\tvalid_1's RMSPE: 0.2448\n",
      "[1600]\ttraining's rmse: 0.000346161\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000362505\tvalid_1's RMSPE: 0.2446\n",
      "[1700]\ttraining's rmse: 0.000345312\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000362162\tvalid_1's RMSPE: 0.2444\n",
      "[1800]\ttraining's rmse: 0.000344482\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000361857\tvalid_1's RMSPE: 0.2442\n",
      "[1900]\ttraining's rmse: 0.000343703\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000361618\tvalid_1's RMSPE: 0.244\n",
      "[2000]\ttraining's rmse: 0.00034297\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000361429\tvalid_1's RMSPE: 0.2439\n",
      "[2100]\ttraining's rmse: 0.000342296\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.00036127\tvalid_1's RMSPE: 0.2438\n",
      "[2200]\ttraining's rmse: 0.000341607\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000361058\tvalid_1's RMSPE: 0.2436\n",
      "Early stopping, best iteration is:\n",
      "[2168]\ttraining's rmse: 0.000341811\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000361103\tvalid_1's RMSPE: 0.2436\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2437\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 3\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001175\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000487849\ttraining's RMSPE: 0.3347\tvalid_1's rmse: 0.00049945\tvalid_1's RMSPE: 0.3406\n",
      "[200]\ttraining's rmse: 0.000393674\ttraining's RMSPE: 0.2701\tvalid_1's rmse: 0.000415243\tvalid_1's RMSPE: 0.2832\n",
      "[300]\ttraining's rmse: 0.000371926\ttraining's RMSPE: 0.2551\tvalid_1's rmse: 0.000397145\tvalid_1's RMSPE: 0.2708\n",
      "[400]\ttraining's rmse: 0.000364601\ttraining's RMSPE: 0.2501\tvalid_1's rmse: 0.000390684\tvalid_1's RMSPE: 0.2664\n",
      "[500]\ttraining's rmse: 0.00036075\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000387273\tvalid_1's RMSPE: 0.2641\n",
      "[600]\ttraining's rmse: 0.000357882\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000384654\tvalid_1's RMSPE: 0.2623\n",
      "[700]\ttraining's rmse: 0.000355567\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000382871\tvalid_1's RMSPE: 0.2611\n",
      "[800]\ttraining's rmse: 0.000353579\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000381693\tvalid_1's RMSPE: 0.2603\n",
      "[900]\ttraining's rmse: 0.000351839\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000380624\tvalid_1's RMSPE: 0.2596\n",
      "[1000]\ttraining's rmse: 0.000350335\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000379761\tvalid_1's RMSPE: 0.259\n",
      "[1100]\ttraining's rmse: 0.000348991\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000379259\tvalid_1's RMSPE: 0.2586\n",
      "[1200]\ttraining's rmse: 0.000347748\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000378872\tvalid_1's RMSPE: 0.2584\n",
      "[1300]\ttraining's rmse: 0.000346641\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000378471\tvalid_1's RMSPE: 0.2581\n",
      "[1400]\ttraining's rmse: 0.000345551\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000377704\tvalid_1's RMSPE: 0.2576\n",
      "[1500]\ttraining's rmse: 0.000344603\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.000377252\tvalid_1's RMSPE: 0.2573\n",
      "[1600]\ttraining's rmse: 0.000343691\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000376812\tvalid_1's RMSPE: 0.257\n",
      "Early stopping, best iteration is:\n",
      "[1534]\ttraining's rmse: 0.000344279\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000376878\tvalid_1's RMSPE: 0.257\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.257\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001173\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048806\ttraining's RMSPE: 0.335\tvalid_1's rmse: 0.000489328\tvalid_1's RMSPE: 0.3328\n",
      "[200]\ttraining's rmse: 0.000393726\ttraining's RMSPE: 0.2703\tvalid_1's rmse: 0.00040157\tvalid_1's RMSPE: 0.2731\n",
      "[300]\ttraining's rmse: 0.000371868\ttraining's RMSPE: 0.2553\tvalid_1's rmse: 0.000382083\tvalid_1's RMSPE: 0.2598\n",
      "[400]\ttraining's rmse: 0.000364704\ttraining's RMSPE: 0.2504\tvalid_1's rmse: 0.000376024\tvalid_1's RMSPE: 0.2557\n",
      "[500]\ttraining's rmse: 0.000360844\ttraining's RMSPE: 0.2477\tvalid_1's rmse: 0.000373105\tvalid_1's RMSPE: 0.2537\n",
      "[600]\ttraining's rmse: 0.000358076\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000371192\tvalid_1's RMSPE: 0.2524\n",
      "[700]\ttraining's rmse: 0.000355771\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000369636\tvalid_1's RMSPE: 0.2514\n",
      "[800]\ttraining's rmse: 0.000353908\ttraining's RMSPE: 0.243\tvalid_1's rmse: 0.000368533\tvalid_1's RMSPE: 0.2506\n",
      "[900]\ttraining's rmse: 0.000352166\ttraining's RMSPE: 0.2418\tvalid_1's rmse: 0.000367537\tvalid_1's RMSPE: 0.2499\n",
      "[1000]\ttraining's rmse: 0.000350638\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000366681\tvalid_1's RMSPE: 0.2494\n",
      "[1100]\ttraining's rmse: 0.000349309\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000366042\tvalid_1's RMSPE: 0.2489\n",
      "[1200]\ttraining's rmse: 0.000348039\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000365509\tvalid_1's RMSPE: 0.2486\n",
      "[1300]\ttraining's rmse: 0.000346945\ttraining's RMSPE: 0.2382\tvalid_1's rmse: 0.000364959\tvalid_1's RMSPE: 0.2482\n",
      "[1400]\ttraining's rmse: 0.000345885\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.00036461\tvalid_1's RMSPE: 0.2479\n",
      "[1500]\ttraining's rmse: 0.000344997\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000364261\tvalid_1's RMSPE: 0.2477\n",
      "[1600]\ttraining's rmse: 0.00034407\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000363953\tvalid_1's RMSPE: 0.2475\n",
      "[1700]\ttraining's rmse: 0.000343224\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000363527\tvalid_1's RMSPE: 0.2472\n",
      "[1800]\ttraining's rmse: 0.000342425\ttraining's RMSPE: 0.2351\tvalid_1's rmse: 0.000363283\tvalid_1's RMSPE: 0.247\n",
      "[1900]\ttraining's rmse: 0.000341656\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000363027\tvalid_1's RMSPE: 0.2469\n",
      "[2000]\ttraining's rmse: 0.000340968\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000362909\tvalid_1's RMSPE: 0.2468\n",
      "Early stopping, best iteration is:\n",
      "[1916]\ttraining's rmse: 0.000341539\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000363004\tvalid_1's RMSPE: 0.2468\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2468\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001190\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048789\ttraining's RMSPE: 0.3327\tvalid_1's rmse: 0.000518049\tvalid_1's RMSPE: 0.3617\n",
      "[200]\ttraining's rmse: 0.000395634\ttraining's RMSPE: 0.2698\tvalid_1's rmse: 0.000421501\tvalid_1's RMSPE: 0.2943\n",
      "[300]\ttraining's rmse: 0.000374157\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000397164\tvalid_1's RMSPE: 0.2773\n",
      "[400]\ttraining's rmse: 0.000366875\ttraining's RMSPE: 0.2502\tvalid_1's rmse: 0.000389116\tvalid_1's RMSPE: 0.2716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 0.000362983\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000386185\tvalid_1's RMSPE: 0.2696\n",
      "[600]\ttraining's rmse: 0.000360154\ttraining's RMSPE: 0.2456\tvalid_1's rmse: 0.000383698\tvalid_1's RMSPE: 0.2679\n",
      "[700]\ttraining's rmse: 0.000357733\ttraining's RMSPE: 0.244\tvalid_1's rmse: 0.000381859\tvalid_1's RMSPE: 0.2666\n",
      "[800]\ttraining's rmse: 0.000355733\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000380628\tvalid_1's RMSPE: 0.2657\n",
      "[900]\ttraining's rmse: 0.000353955\ttraining's RMSPE: 0.2414\tvalid_1's rmse: 0.000380443\tvalid_1's RMSPE: 0.2656\n",
      "[1000]\ttraining's rmse: 0.000352405\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000378957\tvalid_1's RMSPE: 0.2646\n",
      "[1100]\ttraining's rmse: 0.000351072\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000378405\tvalid_1's RMSPE: 0.2642\n",
      "[1200]\ttraining's rmse: 0.000349799\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000377779\tvalid_1's RMSPE: 0.2637\n",
      "[1300]\ttraining's rmse: 0.000348648\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000377313\tvalid_1's RMSPE: 0.2634\n",
      "[1400]\ttraining's rmse: 0.000347535\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000377597\tvalid_1's RMSPE: 0.2636\n",
      "Early stopping, best iteration is:\n",
      "[1305]\ttraining's rmse: 0.00034858\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.000377219\tvalid_1's RMSPE: 0.2633\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2633\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001179\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489075\ttraining's RMSPE: 0.3349\tvalid_1's rmse: 0.000495213\tvalid_1's RMSPE: 0.3402\n",
      "[200]\ttraining's rmse: 0.000395116\ttraining's RMSPE: 0.2706\tvalid_1's rmse: 0.000406502\tvalid_1's RMSPE: 0.2792\n",
      "[300]\ttraining's rmse: 0.000373083\ttraining's RMSPE: 0.2555\tvalid_1's rmse: 0.000387653\tvalid_1's RMSPE: 0.2663\n",
      "[400]\ttraining's rmse: 0.000365746\ttraining's RMSPE: 0.2505\tvalid_1's rmse: 0.000382182\tvalid_1's RMSPE: 0.2625\n",
      "[500]\ttraining's rmse: 0.000361727\ttraining's RMSPE: 0.2477\tvalid_1's rmse: 0.000379917\tvalid_1's RMSPE: 0.261\n",
      "[600]\ttraining's rmse: 0.000358874\ttraining's RMSPE: 0.2458\tvalid_1's rmse: 0.000378219\tvalid_1's RMSPE: 0.2598\n",
      "[700]\ttraining's rmse: 0.000356545\ttraining's RMSPE: 0.2442\tvalid_1's rmse: 0.000377096\tvalid_1's RMSPE: 0.259\n",
      "[800]\ttraining's rmse: 0.00035453\ttraining's RMSPE: 0.2428\tvalid_1's rmse: 0.000376346\tvalid_1's RMSPE: 0.2585\n",
      "[900]\ttraining's rmse: 0.000352754\ttraining's RMSPE: 0.2416\tvalid_1's rmse: 0.00037545\tvalid_1's RMSPE: 0.2579\n",
      "[1000]\ttraining's rmse: 0.000351237\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000374973\tvalid_1's RMSPE: 0.2576\n",
      "[1100]\ttraining's rmse: 0.000349867\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.000374461\tvalid_1's RMSPE: 0.2572\n",
      "[1200]\ttraining's rmse: 0.000348621\ttraining's RMSPE: 0.2387\tvalid_1's rmse: 0.000374072\tvalid_1's RMSPE: 0.2569\n",
      "[1300]\ttraining's rmse: 0.000347444\ttraining's RMSPE: 0.2379\tvalid_1's rmse: 0.000373742\tvalid_1's RMSPE: 0.2567\n",
      "[1400]\ttraining's rmse: 0.00034639\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000373472\tvalid_1's RMSPE: 0.2565\n",
      "[1500]\ttraining's rmse: 0.000345431\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000373265\tvalid_1's RMSPE: 0.2564\n",
      "[1600]\ttraining's rmse: 0.000344488\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000373115\tvalid_1's RMSPE: 0.2563\n",
      "[1700]\ttraining's rmse: 0.000343603\ttraining's RMSPE: 0.2353\tvalid_1's rmse: 0.000372868\tvalid_1's RMSPE: 0.2561\n",
      "[1800]\ttraining's rmse: 0.00034278\ttraining's RMSPE: 0.2347\tvalid_1's rmse: 0.000372684\tvalid_1's RMSPE: 0.256\n",
      "[1900]\ttraining's rmse: 0.000342009\ttraining's RMSPE: 0.2342\tvalid_1's rmse: 0.000372383\tvalid_1's RMSPE: 0.2558\n",
      "Early stopping, best iteration is:\n",
      "[1840]\ttraining's rmse: 0.000342487\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000372474\tvalid_1's RMSPE: 0.2558\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2558\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001173\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489113\ttraining's RMSPE: 0.3359\tvalid_1's rmse: 0.00049175\tvalid_1's RMSPE: 0.3338\n",
      "[200]\ttraining's rmse: 0.000394766\ttraining's RMSPE: 0.2711\tvalid_1's rmse: 0.00040081\tvalid_1's RMSPE: 0.2721\n",
      "[300]\ttraining's rmse: 0.000372652\ttraining's RMSPE: 0.2559\tvalid_1's rmse: 0.00038036\tvalid_1's RMSPE: 0.2582\n",
      "[400]\ttraining's rmse: 0.000365287\ttraining's RMSPE: 0.2509\tvalid_1's rmse: 0.000374283\tvalid_1's RMSPE: 0.2541\n",
      "[500]\ttraining's rmse: 0.00036132\ttraining's RMSPE: 0.2482\tvalid_1's rmse: 0.000371615\tvalid_1's RMSPE: 0.2523\n",
      "[600]\ttraining's rmse: 0.000358424\ttraining's RMSPE: 0.2462\tvalid_1's rmse: 0.000369951\tvalid_1's RMSPE: 0.2511\n",
      "[700]\ttraining's rmse: 0.000356062\ttraining's RMSPE: 0.2445\tvalid_1's rmse: 0.000368742\tvalid_1's RMSPE: 0.2503\n",
      "[800]\ttraining's rmse: 0.000354056\ttraining's RMSPE: 0.2432\tvalid_1's rmse: 0.000367665\tvalid_1's RMSPE: 0.2496\n",
      "[900]\ttraining's rmse: 0.000352308\ttraining's RMSPE: 0.242\tvalid_1's rmse: 0.000366808\tvalid_1's RMSPE: 0.249\n",
      "[1000]\ttraining's rmse: 0.00035073\ttraining's RMSPE: 0.2409\tvalid_1's rmse: 0.000366103\tvalid_1's RMSPE: 0.2485\n",
      "[1100]\ttraining's rmse: 0.00034938\ttraining's RMSPE: 0.2399\tvalid_1's rmse: 0.000365508\tvalid_1's RMSPE: 0.2481\n",
      "[1200]\ttraining's rmse: 0.000348128\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000364866\tvalid_1's RMSPE: 0.2477\n",
      "[1300]\ttraining's rmse: 0.000346954\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000364407\tvalid_1's RMSPE: 0.2474\n",
      "[1400]\ttraining's rmse: 0.000345918\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000364045\tvalid_1's RMSPE: 0.2471\n",
      "[1500]\ttraining's rmse: 0.000344938\ttraining's RMSPE: 0.2369\tvalid_1's rmse: 0.000363799\tvalid_1's RMSPE: 0.247\n",
      "[1600]\ttraining's rmse: 0.00034406\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000363505\tvalid_1's RMSPE: 0.2468\n",
      "[1700]\ttraining's rmse: 0.000343195\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000363142\tvalid_1's RMSPE: 0.2465\n",
      "[1800]\ttraining's rmse: 0.000342426\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000362889\tvalid_1's RMSPE: 0.2463\n",
      "[1900]\ttraining's rmse: 0.000341655\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000362642\tvalid_1's RMSPE: 0.2462\n",
      "[2000]\ttraining's rmse: 0.000340919\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000362393\tvalid_1's RMSPE: 0.246\n",
      "[2100]\ttraining's rmse: 0.000340228\ttraining's RMSPE: 0.2337\tvalid_1's rmse: 0.000362234\tvalid_1's RMSPE: 0.2459\n",
      "[2200]\ttraining's rmse: 0.000339554\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000362052\tvalid_1's RMSPE: 0.2458\n",
      "[2300]\ttraining's rmse: 0.00033892\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000361878\tvalid_1's RMSPE: 0.2457\n",
      "[2400]\ttraining's rmse: 0.00033833\ttraining's RMSPE: 0.2324\tvalid_1's rmse: 0.000361786\tvalid_1's RMSPE: 0.2456\n",
      "Early stopping, best iteration is:\n",
      "[2301]\ttraining's rmse: 0.000338914\ttraining's RMSPE: 0.2328\tvalid_1's rmse: 0.000361873\tvalid_1's RMSPE: 0.2456\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2456\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 4\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001172\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 0.000489929\ttraining's RMSPE: 0.337\tvalid_1's rmse: 0.000490469\tvalid_1's RMSPE: 0.3309\n",
      "[200]\ttraining's rmse: 0.000395197\ttraining's RMSPE: 0.2718\tvalid_1's rmse: 0.000399202\tvalid_1's RMSPE: 0.2693\n",
      "[300]\ttraining's rmse: 0.000372858\ttraining's RMSPE: 0.2565\tvalid_1's rmse: 0.000378732\tvalid_1's RMSPE: 0.2555\n",
      "[400]\ttraining's rmse: 0.000365195\ttraining's RMSPE: 0.2512\tvalid_1's rmse: 0.000372263\tvalid_1's RMSPE: 0.2511\n",
      "[500]\ttraining's rmse: 0.000361105\ttraining's RMSPE: 0.2484\tvalid_1's rmse: 0.000369132\tvalid_1's RMSPE: 0.249\n",
      "[600]\ttraining's rmse: 0.000358136\ttraining's RMSPE: 0.2464\tvalid_1's rmse: 0.00036705\tvalid_1's RMSPE: 0.2476\n",
      "[700]\ttraining's rmse: 0.000355741\ttraining's RMSPE: 0.2447\tvalid_1's rmse: 0.00036544\tvalid_1's RMSPE: 0.2465\n",
      "[800]\ttraining's rmse: 0.000353701\ttraining's RMSPE: 0.2433\tvalid_1's rmse: 0.000364164\tvalid_1's RMSPE: 0.2457\n",
      "[900]\ttraining's rmse: 0.000351884\ttraining's RMSPE: 0.2421\tvalid_1's rmse: 0.00036312\tvalid_1's RMSPE: 0.2449\n",
      "[1000]\ttraining's rmse: 0.00035034\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000362233\tvalid_1's RMSPE: 0.2443\n",
      "[1100]\ttraining's rmse: 0.000348962\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000361474\tvalid_1's RMSPE: 0.2438\n",
      "[1200]\ttraining's rmse: 0.000347707\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000360829\tvalid_1's RMSPE: 0.2434\n",
      "[1300]\ttraining's rmse: 0.000346528\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000360215\tvalid_1's RMSPE: 0.243\n",
      "[1400]\ttraining's rmse: 0.000345454\ttraining's RMSPE: 0.2376\tvalid_1's rmse: 0.000359716\tvalid_1's RMSPE: 0.2426\n",
      "[1500]\ttraining's rmse: 0.000344482\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000359297\tvalid_1's RMSPE: 0.2424\n",
      "[1600]\ttraining's rmse: 0.000343521\ttraining's RMSPE: 0.2363\tvalid_1's rmse: 0.000358828\tvalid_1's RMSPE: 0.2421\n",
      "[1700]\ttraining's rmse: 0.000342664\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.00035849\tvalid_1's RMSPE: 0.2418\n",
      "[1800]\ttraining's rmse: 0.00034186\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000358184\tvalid_1's RMSPE: 0.2416\n",
      "[1900]\ttraining's rmse: 0.000341065\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000357902\tvalid_1's RMSPE: 0.2414\n",
      "[2000]\ttraining's rmse: 0.000340334\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000357682\tvalid_1's RMSPE: 0.2413\n",
      "[2100]\ttraining's rmse: 0.000339608\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000357443\tvalid_1's RMSPE: 0.2411\n",
      "[2200]\ttraining's rmse: 0.000338941\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000357234\tvalid_1's RMSPE: 0.241\n",
      "[2300]\ttraining's rmse: 0.000338306\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000357055\tvalid_1's RMSPE: 0.2409\n",
      "[2400]\ttraining's rmse: 0.00033768\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000356891\tvalid_1's RMSPE: 0.2407\n",
      "Early stopping, best iteration is:\n",
      "[2396]\ttraining's rmse: 0.000337701\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000356894\tvalid_1's RMSPE: 0.2407\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2407\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048898\ttraining's RMSPE: 0.3352\tvalid_1's rmse: 0.000491898\tvalid_1's RMSPE: 0.3364\n",
      "[200]\ttraining's rmse: 0.000394043\ttraining's RMSPE: 0.2702\tvalid_1's rmse: 0.000403008\tvalid_1's RMSPE: 0.2756\n",
      "[300]\ttraining's rmse: 0.000371798\ttraining's RMSPE: 0.2549\tvalid_1's rmse: 0.00038348\tvalid_1's RMSPE: 0.2622\n",
      "[400]\ttraining's rmse: 0.000364453\ttraining's RMSPE: 0.2499\tvalid_1's rmse: 0.000377782\tvalid_1's RMSPE: 0.2583\n",
      "[500]\ttraining's rmse: 0.00036055\ttraining's RMSPE: 0.2472\tvalid_1's rmse: 0.000375166\tvalid_1's RMSPE: 0.2565\n",
      "[600]\ttraining's rmse: 0.000357641\ttraining's RMSPE: 0.2452\tvalid_1's rmse: 0.000373443\tvalid_1's RMSPE: 0.2554\n",
      "[700]\ttraining's rmse: 0.00035536\ttraining's RMSPE: 0.2436\tvalid_1's rmse: 0.000372141\tvalid_1's RMSPE: 0.2545\n",
      "[800]\ttraining's rmse: 0.000353333\ttraining's RMSPE: 0.2422\tvalid_1's rmse: 0.000370995\tvalid_1's RMSPE: 0.2537\n",
      "[900]\ttraining's rmse: 0.000351598\ttraining's RMSPE: 0.2411\tvalid_1's rmse: 0.000370167\tvalid_1's RMSPE: 0.2531\n",
      "[1000]\ttraining's rmse: 0.000350076\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000369361\tvalid_1's RMSPE: 0.2526\n",
      "[1100]\ttraining's rmse: 0.00034876\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000368745\tvalid_1's RMSPE: 0.2521\n",
      "[1200]\ttraining's rmse: 0.000347508\ttraining's RMSPE: 0.2383\tvalid_1's rmse: 0.000368213\tvalid_1's RMSPE: 0.2518\n",
      "[1300]\ttraining's rmse: 0.000346445\ttraining's RMSPE: 0.2375\tvalid_1's rmse: 0.000367721\tvalid_1's RMSPE: 0.2514\n",
      "[1400]\ttraining's rmse: 0.00034539\ttraining's RMSPE: 0.2368\tvalid_1's rmse: 0.000367415\tvalid_1's RMSPE: 0.2512\n",
      "[1500]\ttraining's rmse: 0.000344465\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.00036696\tvalid_1's RMSPE: 0.2509\n",
      "[1600]\ttraining's rmse: 0.00034358\ttraining's RMSPE: 0.2356\tvalid_1's rmse: 0.000366628\tvalid_1's RMSPE: 0.2507\n",
      "[1700]\ttraining's rmse: 0.00034272\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.00036623\tvalid_1's RMSPE: 0.2504\n",
      "[1800]\ttraining's rmse: 0.000341912\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000365994\tvalid_1's RMSPE: 0.2503\n",
      "[1900]\ttraining's rmse: 0.000341164\ttraining's RMSPE: 0.2339\tvalid_1's rmse: 0.000365719\tvalid_1's RMSPE: 0.2501\n",
      "[2000]\ttraining's rmse: 0.000340439\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000365475\tvalid_1's RMSPE: 0.2499\n",
      "[2100]\ttraining's rmse: 0.000339742\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000365245\tvalid_1's RMSPE: 0.2497\n",
      "[2200]\ttraining's rmse: 0.000339082\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000365103\tvalid_1's RMSPE: 0.2497\n",
      "Early stopping, best iteration is:\n",
      "[2100]\ttraining's rmse: 0.000339742\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.000365245\tvalid_1's RMSPE: 0.2497\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2497\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001178\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488127\ttraining's RMSPE: 0.3347\tvalid_1's rmse: 0.000505563\tvalid_1's RMSPE: 0.3455\n",
      "[200]\ttraining's rmse: 0.000394222\ttraining's RMSPE: 0.2703\tvalid_1's rmse: 0.00042433\tvalid_1's RMSPE: 0.29\n",
      "[300]\ttraining's rmse: 0.000372103\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000407177\tvalid_1's RMSPE: 0.2783\n",
      "[400]\ttraining's rmse: 0.000364684\ttraining's RMSPE: 0.2501\tvalid_1's rmse: 0.000401541\tvalid_1's RMSPE: 0.2744\n",
      "[500]\ttraining's rmse: 0.000360632\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.000398113\tvalid_1's RMSPE: 0.2721\n",
      "[600]\ttraining's rmse: 0.000357779\ttraining's RMSPE: 0.2453\tvalid_1's rmse: 0.000396017\tvalid_1's RMSPE: 0.2706\n",
      "[700]\ttraining's rmse: 0.000355406\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000394765\tvalid_1's RMSPE: 0.2698\n",
      "[800]\ttraining's rmse: 0.000353485\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000393668\tvalid_1's RMSPE: 0.269\n",
      "[900]\ttraining's rmse: 0.0003517\ttraining's RMSPE: 0.2412\tvalid_1's rmse: 0.000392886\tvalid_1's RMSPE: 0.2685\n",
      "[1000]\ttraining's rmse: 0.000350196\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000391824\tvalid_1's RMSPE: 0.2678\n",
      "[1100]\ttraining's rmse: 0.000348848\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000391759\tvalid_1's RMSPE: 0.2677\n",
      "Early stopping, best iteration is:\n",
      "[1048]\ttraining's rmse: 0.00034951\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000391482\tvalid_1's RMSPE: 0.2675\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2675\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.001193\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000489353\ttraining's RMSPE: 0.3333\tvalid_1's rmse: 0.000510631\tvalid_1's RMSPE: 0.3582\n",
      "[200]\ttraining's rmse: 0.000396578\ttraining's RMSPE: 0.2701\tvalid_1's rmse: 0.000415241\tvalid_1's RMSPE: 0.2913\n",
      "[300]\ttraining's rmse: 0.000374683\ttraining's RMSPE: 0.2552\tvalid_1's rmse: 0.000392286\tvalid_1's RMSPE: 0.2752\n",
      "[400]\ttraining's rmse: 0.000367198\ttraining's RMSPE: 0.2501\tvalid_1's rmse: 0.000385705\tvalid_1's RMSPE: 0.2706\n",
      "[500]\ttraining's rmse: 0.000363056\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.000381388\tvalid_1's RMSPE: 0.2675\n",
      "[600]\ttraining's rmse: 0.000360137\ttraining's RMSPE: 0.2453\tvalid_1's rmse: 0.000379369\tvalid_1's RMSPE: 0.2661\n",
      "[700]\ttraining's rmse: 0.000357713\ttraining's RMSPE: 0.2436\tvalid_1's rmse: 0.000377039\tvalid_1's RMSPE: 0.2645\n",
      "[800]\ttraining's rmse: 0.000355712\ttraining's RMSPE: 0.2423\tvalid_1's rmse: 0.000375073\tvalid_1's RMSPE: 0.2631\n",
      "[900]\ttraining's rmse: 0.000353879\ttraining's RMSPE: 0.241\tvalid_1's rmse: 0.000374061\tvalid_1's RMSPE: 0.2624\n",
      "[1000]\ttraining's rmse: 0.000352357\ttraining's RMSPE: 0.24\tvalid_1's rmse: 0.000373693\tvalid_1's RMSPE: 0.2621\n",
      "Early stopping, best iteration is:\n",
      "[986]\ttraining's rmse: 0.000352562\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000373428\tvalid_1's RMSPE: 0.2619\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2619\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001177\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488392\ttraining's RMSPE: 0.335\tvalid_1's rmse: 0.00049988\tvalid_1's RMSPE: 0.3413\n",
      "[200]\ttraining's rmse: 0.000394443\ttraining's RMSPE: 0.2705\tvalid_1's rmse: 0.000416053\tvalid_1's RMSPE: 0.284\n",
      "[300]\ttraining's rmse: 0.000372281\ttraining's RMSPE: 0.2553\tvalid_1's rmse: 0.000398376\tvalid_1's RMSPE: 0.272\n",
      "[400]\ttraining's rmse: 0.000364825\ttraining's RMSPE: 0.2502\tvalid_1's rmse: 0.00039327\tvalid_1's RMSPE: 0.2685\n",
      "[500]\ttraining's rmse: 0.000360791\ttraining's RMSPE: 0.2475\tvalid_1's rmse: 0.000391061\tvalid_1's RMSPE: 0.267\n",
      "[600]\ttraining's rmse: 0.000357978\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000389806\tvalid_1's RMSPE: 0.2661\n",
      "[700]\ttraining's rmse: 0.000355611\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000388524\tvalid_1's RMSPE: 0.2652\n",
      "[800]\ttraining's rmse: 0.000353655\ttraining's RMSPE: 0.2426\tvalid_1's rmse: 0.000387645\tvalid_1's RMSPE: 0.2646\n",
      "[900]\ttraining's rmse: 0.000351878\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000386885\tvalid_1's RMSPE: 0.2641\n",
      "[1000]\ttraining's rmse: 0.000350365\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000386294\tvalid_1's RMSPE: 0.2637\n",
      "[1100]\ttraining's rmse: 0.000349013\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000385998\tvalid_1's RMSPE: 0.2635\n",
      "[1200]\ttraining's rmse: 0.000347767\ttraining's RMSPE: 0.2385\tvalid_1's rmse: 0.000385348\tvalid_1's RMSPE: 0.2631\n",
      "[1300]\ttraining's rmse: 0.000346616\ttraining's RMSPE: 0.2377\tvalid_1's rmse: 0.000384941\tvalid_1's RMSPE: 0.2628\n",
      "[1400]\ttraining's rmse: 0.000345603\ttraining's RMSPE: 0.237\tvalid_1's rmse: 0.000384568\tvalid_1's RMSPE: 0.2625\n",
      "[1500]\ttraining's rmse: 0.000344638\ttraining's RMSPE: 0.2364\tvalid_1's rmse: 0.000384355\tvalid_1's RMSPE: 0.2624\n",
      "[1600]\ttraining's rmse: 0.000343712\ttraining's RMSPE: 0.2357\tvalid_1's rmse: 0.000384137\tvalid_1's RMSPE: 0.2622\n",
      "Early stopping, best iteration is:\n",
      "[1580]\ttraining's rmse: 0.000343898\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000384131\tvalid_1's RMSPE: 0.2622\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2622\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 5\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001175\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488391\ttraining's RMSPE: 0.3354\tvalid_1's rmse: 0.000489065\tvalid_1's RMSPE: 0.331\n",
      "[200]\ttraining's rmse: 0.000394295\ttraining's RMSPE: 0.2708\tvalid_1's rmse: 0.000397446\tvalid_1's RMSPE: 0.269\n",
      "[300]\ttraining's rmse: 0.000372374\ttraining's RMSPE: 0.2557\tvalid_1's rmse: 0.000377024\tvalid_1's RMSPE: 0.2551\n",
      "[400]\ttraining's rmse: 0.000364875\ttraining's RMSPE: 0.2506\tvalid_1's rmse: 0.000370842\tvalid_1's RMSPE: 0.251\n",
      "[500]\ttraining's rmse: 0.000360915\ttraining's RMSPE: 0.2479\tvalid_1's rmse: 0.000367882\tvalid_1's RMSPE: 0.249\n",
      "[600]\ttraining's rmse: 0.000358041\ttraining's RMSPE: 0.2459\tvalid_1's rmse: 0.000366002\tvalid_1's RMSPE: 0.2477\n",
      "[700]\ttraining's rmse: 0.000355702\ttraining's RMSPE: 0.2443\tvalid_1's rmse: 0.000364548\tvalid_1's RMSPE: 0.2467\n",
      "[800]\ttraining's rmse: 0.000353746\ttraining's RMSPE: 0.2429\tvalid_1's rmse: 0.000363404\tvalid_1's RMSPE: 0.2459\n",
      "[900]\ttraining's rmse: 0.000352005\ttraining's RMSPE: 0.2417\tvalid_1's rmse: 0.000362447\tvalid_1's RMSPE: 0.2453\n",
      "[1000]\ttraining's rmse: 0.000350446\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000361667\tvalid_1's RMSPE: 0.2448\n",
      "[1100]\ttraining's rmse: 0.000349098\ttraining's RMSPE: 0.2398\tvalid_1's rmse: 0.000361\tvalid_1's RMSPE: 0.2443\n",
      "[1200]\ttraining's rmse: 0.000347862\ttraining's RMSPE: 0.2389\tvalid_1's rmse: 0.000360372\tvalid_1's RMSPE: 0.2439\n",
      "[1300]\ttraining's rmse: 0.000346741\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000359884\tvalid_1's RMSPE: 0.2435\n",
      "[1400]\ttraining's rmse: 0.000345666\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.00035941\tvalid_1's RMSPE: 0.2432\n",
      "[1500]\ttraining's rmse: 0.000344723\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000359013\tvalid_1's RMSPE: 0.243\n",
      "[1600]\ttraining's rmse: 0.000343855\ttraining's RMSPE: 0.2362\tvalid_1's rmse: 0.000358689\tvalid_1's RMSPE: 0.2427\n",
      "[1700]\ttraining's rmse: 0.000342973\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000358318\tvalid_1's RMSPE: 0.2425\n",
      "[1800]\ttraining's rmse: 0.000342164\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000357985\tvalid_1's RMSPE: 0.2423\n",
      "[1900]\ttraining's rmse: 0.000341424\ttraining's RMSPE: 0.2345\tvalid_1's rmse: 0.000357786\tvalid_1's RMSPE: 0.2421\n",
      "[2000]\ttraining's rmse: 0.000340661\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000357605\tvalid_1's RMSPE: 0.242\n",
      "[2100]\ttraining's rmse: 0.000339949\ttraining's RMSPE: 0.2335\tvalid_1's rmse: 0.000357367\tvalid_1's RMSPE: 0.2418\n",
      "[2200]\ttraining's rmse: 0.000339267\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000357127\tvalid_1's RMSPE: 0.2417\n",
      "[2300]\ttraining's rmse: 0.000338623\ttraining's RMSPE: 0.2326\tvalid_1's rmse: 0.000356978\tvalid_1's RMSPE: 0.2416\n",
      "Early stopping, best iteration is:\n",
      "[2230]\ttraining's rmse: 0.000339067\ttraining's RMSPE: 0.2329\tvalid_1's rmse: 0.00035708\tvalid_1's RMSPE: 0.2416\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2417\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001176\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000486795\ttraining's RMSPE: 0.334\tvalid_1's rmse: 0.000495321\tvalid_1's RMSPE: 0.3364\n",
      "[200]\ttraining's rmse: 0.00039296\ttraining's RMSPE: 0.2696\tvalid_1's rmse: 0.000410606\tvalid_1's RMSPE: 0.2788\n",
      "[300]\ttraining's rmse: 0.000371332\ttraining's RMSPE: 0.2548\tvalid_1's rmse: 0.000392724\tvalid_1's RMSPE: 0.2667\n",
      "[400]\ttraining's rmse: 0.000364297\ttraining's RMSPE: 0.25\tvalid_1's rmse: 0.000387395\tvalid_1's RMSPE: 0.2631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 0.00036047\ttraining's RMSPE: 0.2474\tvalid_1's rmse: 0.000384955\tvalid_1's RMSPE: 0.2614\n",
      "[600]\ttraining's rmse: 0.000357756\ttraining's RMSPE: 0.2455\tvalid_1's rmse: 0.000383427\tvalid_1's RMSPE: 0.2604\n",
      "[700]\ttraining's rmse: 0.00035551\ttraining's RMSPE: 0.2439\tvalid_1's rmse: 0.000382096\tvalid_1's RMSPE: 0.2595\n",
      "[800]\ttraining's rmse: 0.000353631\ttraining's RMSPE: 0.2427\tvalid_1's rmse: 0.000381028\tvalid_1's RMSPE: 0.2588\n",
      "[900]\ttraining's rmse: 0.000351978\ttraining's RMSPE: 0.2415\tvalid_1's rmse: 0.000380114\tvalid_1's RMSPE: 0.2581\n",
      "[1000]\ttraining's rmse: 0.000350534\ttraining's RMSPE: 0.2405\tvalid_1's rmse: 0.000379296\tvalid_1's RMSPE: 0.2576\n",
      "[1100]\ttraining's rmse: 0.000349224\ttraining's RMSPE: 0.2396\tvalid_1's rmse: 0.00037868\tvalid_1's RMSPE: 0.2572\n",
      "[1200]\ttraining's rmse: 0.000348067\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.00037818\tvalid_1's RMSPE: 0.2568\n",
      "[1300]\ttraining's rmse: 0.000346972\ttraining's RMSPE: 0.2381\tvalid_1's rmse: 0.000377806\tvalid_1's RMSPE: 0.2566\n",
      "[1400]\ttraining's rmse: 0.000345958\ttraining's RMSPE: 0.2374\tvalid_1's rmse: 0.000377353\tvalid_1's RMSPE: 0.2563\n",
      "[1500]\ttraining's rmse: 0.000344971\ttraining's RMSPE: 0.2367\tvalid_1's rmse: 0.000376831\tvalid_1's RMSPE: 0.2559\n",
      "[1600]\ttraining's rmse: 0.00034408\ttraining's RMSPE: 0.2361\tvalid_1's rmse: 0.000376516\tvalid_1's RMSPE: 0.2557\n",
      "[1700]\ttraining's rmse: 0.000343212\ttraining's RMSPE: 0.2355\tvalid_1's rmse: 0.000376061\tvalid_1's RMSPE: 0.2554\n",
      "[1800]\ttraining's rmse: 0.000342406\ttraining's RMSPE: 0.235\tvalid_1's rmse: 0.000375782\tvalid_1's RMSPE: 0.2552\n",
      "[1900]\ttraining's rmse: 0.000341653\ttraining's RMSPE: 0.2344\tvalid_1's rmse: 0.000375603\tvalid_1's RMSPE: 0.2551\n",
      "[2000]\ttraining's rmse: 0.000340944\ttraining's RMSPE: 0.234\tvalid_1's rmse: 0.000375353\tvalid_1's RMSPE: 0.2549\n",
      "[2100]\ttraining's rmse: 0.000340202\ttraining's RMSPE: 0.2334\tvalid_1's rmse: 0.000375152\tvalid_1's RMSPE: 0.2548\n",
      "[2200]\ttraining's rmse: 0.000339541\ttraining's RMSPE: 0.233\tvalid_1's rmse: 0.000375012\tvalid_1's RMSPE: 0.2547\n",
      "Early stopping, best iteration is:\n",
      "[2187]\ttraining's rmse: 0.000339635\ttraining's RMSPE: 0.2331\tvalid_1's rmse: 0.000374984\tvalid_1's RMSPE: 0.2546\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2546\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001181\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488168\ttraining's RMSPE: 0.3341\tvalid_1's rmse: 0.000493057\tvalid_1's RMSPE: 0.3383\n",
      "[200]\ttraining's rmse: 0.000394065\ttraining's RMSPE: 0.2697\tvalid_1's rmse: 0.000403031\tvalid_1's RMSPE: 0.2766\n",
      "[300]\ttraining's rmse: 0.00037187\ttraining's RMSPE: 0.2545\tvalid_1's rmse: 0.000382934\tvalid_1's RMSPE: 0.2628\n",
      "[400]\ttraining's rmse: 0.000364356\ttraining's RMSPE: 0.2494\tvalid_1's rmse: 0.000376641\tvalid_1's RMSPE: 0.2584\n",
      "[500]\ttraining's rmse: 0.000360372\ttraining's RMSPE: 0.2466\tvalid_1's rmse: 0.000373813\tvalid_1's RMSPE: 0.2565\n",
      "[600]\ttraining's rmse: 0.000357541\ttraining's RMSPE: 0.2447\tvalid_1's rmse: 0.000372137\tvalid_1's RMSPE: 0.2554\n",
      "[700]\ttraining's rmse: 0.000355268\ttraining's RMSPE: 0.2432\tvalid_1's rmse: 0.000370917\tvalid_1's RMSPE: 0.2545\n",
      "[800]\ttraining's rmse: 0.000353314\ttraining's RMSPE: 0.2418\tvalid_1's rmse: 0.000369822\tvalid_1's RMSPE: 0.2538\n",
      "[900]\ttraining's rmse: 0.000351625\ttraining's RMSPE: 0.2407\tvalid_1's rmse: 0.000368841\tvalid_1's RMSPE: 0.2531\n",
      "[1000]\ttraining's rmse: 0.000350155\ttraining's RMSPE: 0.2397\tvalid_1's rmse: 0.000368126\tvalid_1's RMSPE: 0.2526\n",
      "[1100]\ttraining's rmse: 0.000348874\ttraining's RMSPE: 0.2388\tvalid_1's rmse: 0.000367513\tvalid_1's RMSPE: 0.2522\n",
      "[1200]\ttraining's rmse: 0.000347689\ttraining's RMSPE: 0.238\tvalid_1's rmse: 0.000366986\tvalid_1's RMSPE: 0.2518\n",
      "[1300]\ttraining's rmse: 0.000346601\ttraining's RMSPE: 0.2372\tvalid_1's rmse: 0.000366561\tvalid_1's RMSPE: 0.2515\n",
      "[1400]\ttraining's rmse: 0.000345546\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000366101\tvalid_1's RMSPE: 0.2512\n",
      "[1500]\ttraining's rmse: 0.000344578\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.000365735\tvalid_1's RMSPE: 0.251\n",
      "[1600]\ttraining's rmse: 0.000343686\ttraining's RMSPE: 0.2352\tvalid_1's rmse: 0.000365435\tvalid_1's RMSPE: 0.2508\n",
      "[1700]\ttraining's rmse: 0.000342815\ttraining's RMSPE: 0.2346\tvalid_1's rmse: 0.000365155\tvalid_1's RMSPE: 0.2506\n",
      "[1800]\ttraining's rmse: 0.000342051\ttraining's RMSPE: 0.2341\tvalid_1's rmse: 0.000364873\tvalid_1's RMSPE: 0.2504\n",
      "[1900]\ttraining's rmse: 0.000341345\ttraining's RMSPE: 0.2336\tvalid_1's rmse: 0.000364575\tvalid_1's RMSPE: 0.2502\n",
      "[2000]\ttraining's rmse: 0.00034067\ttraining's RMSPE: 0.2332\tvalid_1's rmse: 0.000364371\tvalid_1's RMSPE: 0.25\n",
      "[2100]\ttraining's rmse: 0.000339995\ttraining's RMSPE: 0.2327\tvalid_1's rmse: 0.000364209\tvalid_1's RMSPE: 0.2499\n",
      "[2200]\ttraining's rmse: 0.000339353\ttraining's RMSPE: 0.2323\tvalid_1's rmse: 0.000364082\tvalid_1's RMSPE: 0.2498\n",
      "Early stopping, best iteration is:\n",
      "[2136]\ttraining's rmse: 0.000339755\ttraining's RMSPE: 0.2325\tvalid_1's rmse: 0.000364115\tvalid_1's RMSPE: 0.2498\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2498\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001195\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488776\ttraining's RMSPE: 0.3326\tvalid_1's rmse: 0.000515142\tvalid_1's RMSPE: 0.3616\n",
      "[200]\ttraining's rmse: 0.000396106\ttraining's RMSPE: 0.2695\tvalid_1's rmse: 0.000420428\tvalid_1's RMSPE: 0.2951\n",
      "[300]\ttraining's rmse: 0.000374564\ttraining's RMSPE: 0.2549\tvalid_1's rmse: 0.000395883\tvalid_1's RMSPE: 0.2779\n",
      "[400]\ttraining's rmse: 0.000367304\ttraining's RMSPE: 0.2499\tvalid_1's rmse: 0.000387372\tvalid_1's RMSPE: 0.2719\n",
      "[500]\ttraining's rmse: 0.000363354\ttraining's RMSPE: 0.2472\tvalid_1's rmse: 0.000384029\tvalid_1's RMSPE: 0.2696\n",
      "[600]\ttraining's rmse: 0.00036047\ttraining's RMSPE: 0.2453\tvalid_1's rmse: 0.0003819\tvalid_1's RMSPE: 0.2681\n",
      "[700]\ttraining's rmse: 0.000358159\ttraining's RMSPE: 0.2437\tvalid_1's rmse: 0.000380819\tvalid_1's RMSPE: 0.2673\n",
      "[800]\ttraining's rmse: 0.000356177\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000379499\tvalid_1's RMSPE: 0.2664\n",
      "[900]\ttraining's rmse: 0.000354427\ttraining's RMSPE: 0.2412\tvalid_1's rmse: 0.000378503\tvalid_1's RMSPE: 0.2657\n",
      "[1000]\ttraining's rmse: 0.00035293\ttraining's RMSPE: 0.2401\tvalid_1's rmse: 0.000377375\tvalid_1's RMSPE: 0.2649\n",
      "[1100]\ttraining's rmse: 0.000351595\ttraining's RMSPE: 0.2392\tvalid_1's rmse: 0.000376301\tvalid_1's RMSPE: 0.2641\n",
      "[1200]\ttraining's rmse: 0.000350382\ttraining's RMSPE: 0.2384\tvalid_1's rmse: 0.000377015\tvalid_1's RMSPE: 0.2646\n",
      "Early stopping, best iteration is:\n",
      "[1119]\ttraining's rmse: 0.000351348\ttraining's RMSPE: 0.2391\tvalid_1's rmse: 0.000376013\tvalid_1's RMSPE: 0.2639\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2639\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "[LightGBM] [Info] Total Bins 45649\n",
      "[LightGBM] [Info] Number of data points in the train set: 274516, number of used features: 193\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 0.001176\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000488225\ttraining's RMSPE: 0.335\tvalid_1's rmse: 0.000494503\tvalid_1's RMSPE: 0.3361\n",
      "[200]\ttraining's rmse: 0.000393746\ttraining's RMSPE: 0.2701\tvalid_1's rmse: 0.000406211\tvalid_1's RMSPE: 0.2761\n",
      "[300]\ttraining's rmse: 0.000371737\ttraining's RMSPE: 0.255\tvalid_1's rmse: 0.000386834\tvalid_1's RMSPE: 0.2629\n",
      "[400]\ttraining's rmse: 0.00036436\ttraining's RMSPE: 0.25\tvalid_1's rmse: 0.000381466\tvalid_1's RMSPE: 0.2593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 0.000360424\ttraining's RMSPE: 0.2473\tvalid_1's rmse: 0.000378979\tvalid_1's RMSPE: 0.2576\n",
      "[600]\ttraining's rmse: 0.000357669\ttraining's RMSPE: 0.2454\tvalid_1's rmse: 0.000377159\tvalid_1's RMSPE: 0.2563\n",
      "[700]\ttraining's rmse: 0.000355325\ttraining's RMSPE: 0.2438\tvalid_1's rmse: 0.000375809\tvalid_1's RMSPE: 0.2554\n",
      "[800]\ttraining's rmse: 0.000353392\ttraining's RMSPE: 0.2424\tvalid_1's rmse: 0.000374738\tvalid_1's RMSPE: 0.2547\n",
      "[900]\ttraining's rmse: 0.000351699\ttraining's RMSPE: 0.2413\tvalid_1's rmse: 0.000373792\tvalid_1's RMSPE: 0.254\n",
      "[1000]\ttraining's rmse: 0.00035021\ttraining's RMSPE: 0.2403\tvalid_1's rmse: 0.000372945\tvalid_1's RMSPE: 0.2535\n",
      "[1100]\ttraining's rmse: 0.000348891\ttraining's RMSPE: 0.2394\tvalid_1's rmse: 0.000372415\tvalid_1's RMSPE: 0.2531\n",
      "[1200]\ttraining's rmse: 0.000347721\ttraining's RMSPE: 0.2386\tvalid_1's rmse: 0.000372019\tvalid_1's RMSPE: 0.2528\n",
      "[1300]\ttraining's rmse: 0.000346635\ttraining's RMSPE: 0.2378\tvalid_1's rmse: 0.000371649\tvalid_1's RMSPE: 0.2526\n",
      "[1400]\ttraining's rmse: 0.000345651\ttraining's RMSPE: 0.2371\tvalid_1's rmse: 0.000371175\tvalid_1's RMSPE: 0.2523\n",
      "[1500]\ttraining's rmse: 0.000344745\ttraining's RMSPE: 0.2365\tvalid_1's rmse: 0.000370812\tvalid_1's RMSPE: 0.252\n",
      "[1600]\ttraining's rmse: 0.000343885\ttraining's RMSPE: 0.2359\tvalid_1's rmse: 0.000370631\tvalid_1's RMSPE: 0.2519\n",
      "[1700]\ttraining's rmse: 0.000343067\ttraining's RMSPE: 0.2354\tvalid_1's rmse: 0.000370528\tvalid_1's RMSPE: 0.2518\n",
      "Early stopping, best iteration is:\n",
      "[1626]\ttraining's rmse: 0.000343657\ttraining's RMSPE: 0.2358\tvalid_1's rmse: 0.00037056\tvalid_1's RMSPE: 0.2518\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2518\n",
      "\t**********************************************************************\n",
      "Wall time: 21min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPSILON = 0\n",
    "norm_test = pd.DataFrame(\n",
    "    {'target_realized_volatility':[],'predicted_volatility_norm': [], 'time_id':[], 'stock_id':[]}\n",
    "\n",
    ")\n",
    "norm_models = []\n",
    "norm_split_importance = []\n",
    "norm_gain_importance = []\n",
    "train_scores = []\n",
    "inner_k = 5\n",
    "outer_k = 5\n",
    "\n",
    "params =  {\n",
    "    'boosting_type': 'goss',\n",
    "    'learning_rate': 0.01,\n",
    "    'metric': 'rmse',\n",
    "    'feature_fraction': 0.8, \n",
    "    'bagging_fraction': 0.8,\n",
    "    'lambda_l1': 1.2,\n",
    "    'lambda_l2': 1.2,\n",
    "    'n_jobs': -1,\n",
    "    'force_col_wise': True,\n",
    "    'extra_trees': True,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "outer_kfold = KFold(n_splits=outer_k, random_state=42, shuffle=True)\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_kfold.split(normX, normY)):\n",
    "    print('*'*120)\n",
    "    print(\"Outer Fold :\", outer_fold + 1)\n",
    "    print('*'*120)\n",
    "\n",
    "    X_outer_train = normX.iloc[outer_train_idx].reset_index(drop=True)  \n",
    "    X_outer_test = normX.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    y_outer_train = normY.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    y_outer_test = normY.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    \n",
    "    target = np.zeros(len(y_outer_test))\n",
    "    inner_scores = 0.0\n",
    "    models = []\n",
    "    \n",
    "    inner_kfold = KFold(n_splits= inner_k, random_state=42, shuffle=True)\n",
    "    for inner_fold, (inner_train_idx, inner_valid_idx) in enumerate(inner_kfold.split(X_outer_train, y_outer_train)):\n",
    "        print(\"\\n\\t\"+\"*\"*20)\n",
    "        print(f\"\\t*  Inner Fold : {inner_fold + 1}  *\")\n",
    "        print(\"\\t\"+\"*\"*20+\"\\n\")\n",
    "    \n",
    "        # inner train data and valid data\n",
    "        X_inner_train = X_outer_train.iloc[inner_train_idx].reset_index(drop=True)\n",
    "        X_inner_valid = X_outer_train.iloc[inner_valid_idx].reset_index(drop=True)\n",
    "\n",
    "        y_inner_train = y_outer_train.iloc[inner_train_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "        y_inner_valid = y_outer_train.iloc[inner_valid_idx].reset_index(drop=True)['target_realized_volatility']\n",
    "            \n",
    "        lgbm_train = lgbm.Dataset(X_inner_train,y_inner_train,weight=1/(np.square(y_inner_train.values)+EPSILON))\n",
    "        lgbm_valid = lgbm.Dataset(\n",
    "            X_inner_valid,y_inner_valid,reference=lgbm_train,weight=1/(np.square(y_inner_valid.values)+EPSILON))\n",
    "        \n",
    "        # model training\n",
    "        model = lgbm.train(\n",
    "            params=params, #tuner.best_params,\n",
    "            train_set=lgbm_train,\n",
    "            valid_sets=[lgbm_train, lgbm_valid],\n",
    "            num_boost_round=10000,       \n",
    "            feval=feval_RMSPE,\n",
    "            callbacks=[lgbm.log_evaluation(period=100), lgbm.early_stopping(100)]\n",
    "        )\n",
    "        # validation \n",
    "        y_inner_pred = model.predict(X_inner_valid, num_iteration=model.best_iteration)\n",
    "        RMSPE = rmspe(\n",
    "            y_true=(y_inner_valid.values.flatten()), \n",
    "            y_pred=(y_inner_pred), n=4\n",
    "        )\n",
    "        \n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "        print(f'\\tInner Validation RMSPE: \\t{RMSPE}')\n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "\n",
    "        # keep training validation score\n",
    "        inner_scores += RMSPE / inner_k\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "        # record feature importances by gain and split\n",
    "        features = list(X_inner_train.columns.values)\n",
    "        \n",
    "        norm_gain_importance.append(compute_importance(model, features, typ='gain'))\n",
    "        norm_split_importance.append(compute_importance(model, features, typ='split'))\n",
    "        \n",
    "    # store all models for prediction in oof evaluation\n",
    "    norm_models.append(models)\n",
    "    train_scores.append(inner_scores)\n",
    "    \n",
    "    # out of fold test set\n",
    "    for model in norm_models[outer_fold]:\n",
    "        y_outer_pred = model.predict(X_outer_test,num_iteration=model.best_iteration)\n",
    "        target += y_outer_pred / len(norm_models[outer_fold])\n",
    "    \n",
    "    y_outer_test = y_outer_test.assign(predicted_volatility_norm = target)\n",
    " \n",
    "    norm_test = pd.concat([norm_test, y_outer_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4692292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.255564"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(norm_test['target_realized_volatility'], norm_test['predicted_volatility_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a299e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>predicted_volatility_norm</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428926</th>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>32712.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  predicted_volatility_norm  time_id  \\\n",
       "0                         0.003397                   0.002773    103.0   \n",
       "1                         0.002993                   0.004439    146.0   \n",
       "2                         0.001094                   0.001236    250.0   \n",
       "3                         0.001711                   0.001649    297.0   \n",
       "4                         0.001197                   0.002248    319.0   \n",
       "...                            ...                        ...      ...   \n",
       "428926                    0.004120                   0.002958  32712.0   \n",
       "428927                    0.003511                   0.004066  32724.0   \n",
       "428928                    0.010431                   0.010297  32746.0   \n",
       "428929                    0.001827                   0.002151  32750.0   \n",
       "428930                    0.003454                   0.002089  32753.0   \n",
       "\n",
       "        stock_id  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "...          ...  \n",
       "428926     126.0  \n",
       "428927     126.0  \n",
       "428928     126.0  \n",
       "428929     126.0  \n",
       "428930     126.0  \n",
       "\n",
       "[428931 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05030582",
   "metadata": {},
   "source": [
    "## Poly Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4118d05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks = pd.read_feather(\"simple.fth\")\n",
    "stocks = remove_clusters(stocks)\n",
    "\n",
    "len(stocks[stocks['target_realized_volatility'] == 0].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4c5717e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>...</th>\n",
       "      <th>beta2-300</th>\n",
       "      <th>target_mean_enc</th>\n",
       "      <th>spread_mean_enc</th>\n",
       "      <th>dom_mean_enc</th>\n",
       "      <th>target_std_enc</th>\n",
       "      <th>spread_std_enc</th>\n",
       "      <th>dom_std_enc</th>\n",
       "      <th>encode_mean_beta</th>\n",
       "      <th>encode_mean_beta2</th>\n",
       "      <th>kmeans5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778640</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>391.300538</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>104.550467</td>\n",
       "      <td>0.635215</td>\n",
       "      <td>0.633070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>391.419207</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>104.729473</td>\n",
       "      <td>0.634261</td>\n",
       "      <td>0.632797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.519335</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>390.605604</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>103.699696</td>\n",
       "      <td>0.632499</td>\n",
       "      <td>0.630289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.378868</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>391.181654</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>104.727931</td>\n",
       "      <td>0.636051</td>\n",
       "      <td>0.633793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973303</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>391.062991</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>104.675748</td>\n",
       "      <td>0.633840</td>\n",
       "      <td>0.633266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049953</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.917628</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.568403</td>\n",
       "      <td>0.961481</td>\n",
       "      <td>0.958937</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160123</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.850800</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>99.572674</td>\n",
       "      <td>0.962304</td>\n",
       "      <td>0.960865</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046489</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>383.053325</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>99.715361</td>\n",
       "      <td>0.959879</td>\n",
       "      <td>0.957927</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.315896</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.605401</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.632305</td>\n",
       "      <td>0.961479</td>\n",
       "      <td>0.958523</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.130582</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>382.545423</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.228970</td>\n",
       "      <td>0.961903</td>\n",
       "      <td>0.960295</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  target_realized_volatility  wap_mean_300  wap_std_300  \\\n",
       "0             5                    0.002954    194.495455     0.164908   \n",
       "1            11                    0.000981    199.598260     0.031047   \n",
       "2            16                    0.001295    209.021831     0.092861   \n",
       "3            31                    0.001776    216.281256     0.183025   \n",
       "4            62                    0.001520    214.542788     0.051133   \n",
       "...         ...                         ...           ...          ...   \n",
       "428786    32751                    0.002899    306.672174     0.163919   \n",
       "428787    32753                    0.003454    291.124934     0.147318   \n",
       "428788    32758                    0.002792    202.972820     0.064758   \n",
       "428789    32763                    0.002379    152.478929     0.068413   \n",
       "428790    32767                    0.001414    194.982143     0.040268   \n",
       "\n",
       "        wap2_mean_300  wap2_std_300  log_returns_realized_volatility_300  \\\n",
       "0          194.479014      0.196558                             0.003394   \n",
       "1          199.597336      0.036259                             0.000699   \n",
       "2          209.053034      0.098250                             0.001983   \n",
       "3          216.198136      0.164985                             0.001863   \n",
       "4          214.524415      0.071793                             0.001131   \n",
       "...               ...           ...                                  ...   \n",
       "428786     306.730549      0.206509                             0.002284   \n",
       "428787     291.137380      0.164838                             0.002217   \n",
       "428788     202.958539      0.080424                             0.001386   \n",
       "428789     152.480008      0.075165                             0.002783   \n",
       "428790     195.012846      0.055999                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428786                             0.000132                2.117007e-10   \n",
       "428787                             0.000128                9.484441e-11   \n",
       "428788                             0.000080                3.031629e-11   \n",
       "428789                             0.000161                1.960849e-10   \n",
       "428790                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  ...  beta2-300  target_mean_enc  \\\n",
       "0               7.138250e-06  ...   0.778640         0.002964   \n",
       "1               8.823633e-07  ...   0.450700         0.002976   \n",
       "2               1.729093e-06  ...   1.519335         0.002974   \n",
       "3              -5.516464e-06  ...   2.378868         0.002975   \n",
       "4              -2.164288e-06  ...   0.973303         0.002974   \n",
       "...                      ...  ...        ...              ...   \n",
       "428786         -2.858852e-06  ...   1.049953         0.003899   \n",
       "428787          3.674218e-06  ...   2.160123         0.003897   \n",
       "428788         -2.437732e-06  ...  -0.046489         0.003903   \n",
       "428789          6.737309e-06  ...   1.315896         0.003896   \n",
       "428790         -1.059089e-06  ...  -2.130582         0.003894   \n",
       "\n",
       "        spread_mean_enc  dom_mean_enc  target_std_enc  spread_std_enc  \\\n",
       "0              0.001023    391.300538        0.002289        0.000925   \n",
       "1              0.001027    391.419207        0.002414        0.000949   \n",
       "2              0.001025    390.605604        0.002411        0.000946   \n",
       "3              0.001026    391.181654        0.002411        0.000947   \n",
       "4              0.001027    391.062991        0.002412        0.000948   \n",
       "...                 ...           ...             ...             ...   \n",
       "428786         0.001108    382.917628        0.002595        0.000784   \n",
       "428787         0.001108    382.850800        0.002590        0.000783   \n",
       "428788         0.001109    383.053325        0.002601        0.000786   \n",
       "428789         0.001108    382.605401        0.002593        0.000784   \n",
       "428790         0.001107    382.545423        0.002589        0.000784   \n",
       "\n",
       "        dom_std_enc  encode_mean_beta  encode_mean_beta2  kmeans5  \n",
       "0        104.550467          0.635215           0.633070        0  \n",
       "1        104.729473          0.634261           0.632797        0  \n",
       "2        103.699696          0.632499           0.630289        0  \n",
       "3        104.727931          0.636051           0.633793        0  \n",
       "4        104.675748          0.633840           0.633266        0  \n",
       "...             ...               ...                ...      ...  \n",
       "428786    99.568403          0.961481           0.958937        4  \n",
       "428787    99.572674          0.962304           0.960865        4  \n",
       "428788    99.715361          0.959879           0.957927        4  \n",
       "428789    99.632305          0.961479           0.958523        4  \n",
       "428790    99.228970          0.961903           0.960295        4  \n",
       "\n",
       "[428791 rows x 182 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks = stocks[stocks['target_realized_volatility'] != 0].reset_index(drop=True)\n",
    "stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e07464cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stocks[stocks.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "68d1c9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>log_returns2_realized_volatility_300</th>\n",
       "      <th>log_returns2_weighted_volatility_300</th>\n",
       "      <th>...</th>\n",
       "      <th>beta2-300</th>\n",
       "      <th>target_mean_enc</th>\n",
       "      <th>spread_mean_enc</th>\n",
       "      <th>dom_mean_enc</th>\n",
       "      <th>target_std_enc</th>\n",
       "      <th>spread_std_enc</th>\n",
       "      <th>dom_std_enc</th>\n",
       "      <th>encode_mean_beta</th>\n",
       "      <th>encode_mean_beta2</th>\n",
       "      <th>kmeans5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194.495455</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>194.479014</td>\n",
       "      <td>0.196558</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>4.521321e-10</td>\n",
       "      <td>7.138250e-06</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778640</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>391.300538</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>104.550467</td>\n",
       "      <td>0.635215</td>\n",
       "      <td>0.633070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199.598260</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>199.597336</td>\n",
       "      <td>0.036259</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.123766e-12</td>\n",
       "      <td>8.823633e-07</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>391.419207</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>104.729473</td>\n",
       "      <td>0.634261</td>\n",
       "      <td>0.632797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209.021831</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>209.053034</td>\n",
       "      <td>0.098250</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.070617e-10</td>\n",
       "      <td>1.729093e-06</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>...</td>\n",
       "      <td>1.519335</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>390.605604</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>103.699696</td>\n",
       "      <td>0.632499</td>\n",
       "      <td>0.630289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216.281256</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>216.198136</td>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>1.551546e-10</td>\n",
       "      <td>-5.516464e-06</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>2.378868</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>391.181654</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>104.727931</td>\n",
       "      <td>0.636051</td>\n",
       "      <td>0.633793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214.542788</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>214.524415</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>3.550845e-11</td>\n",
       "      <td>-2.164288e-06</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973303</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>391.062991</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>104.675748</td>\n",
       "      <td>0.633840</td>\n",
       "      <td>0.633266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>306.672174</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>306.730549</td>\n",
       "      <td>0.206509</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>2.117007e-10</td>\n",
       "      <td>-2.858852e-06</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049953</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.917628</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.568403</td>\n",
       "      <td>0.961481</td>\n",
       "      <td>0.958937</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>291.124934</td>\n",
       "      <td>0.147318</td>\n",
       "      <td>291.137380</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>9.484441e-11</td>\n",
       "      <td>3.674218e-06</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160123</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.850800</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>99.572674</td>\n",
       "      <td>0.962304</td>\n",
       "      <td>0.960865</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>202.972820</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>202.958539</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3.031629e-11</td>\n",
       "      <td>-2.437732e-06</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046489</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>383.053325</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>99.715361</td>\n",
       "      <td>0.959879</td>\n",
       "      <td>0.957927</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>152.478929</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>152.480008</td>\n",
       "      <td>0.075165</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.960849e-10</td>\n",
       "      <td>6.737309e-06</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.315896</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>382.605401</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.632305</td>\n",
       "      <td>0.961479</td>\n",
       "      <td>0.958523</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>194.982143</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>195.012846</td>\n",
       "      <td>0.055999</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.007223e-11</td>\n",
       "      <td>-1.059089e-06</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.130582</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>382.545423</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>99.228970</td>\n",
       "      <td>0.961903</td>\n",
       "      <td>0.960295</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wap_mean_300  wap_std_300  wap2_mean_300  wap2_std_300  \\\n",
       "0         194.495455     0.164908     194.479014      0.196558   \n",
       "1         199.598260     0.031047     199.597336      0.036259   \n",
       "2         209.021831     0.092861     209.053034      0.098250   \n",
       "3         216.281256     0.183025     216.198136      0.164985   \n",
       "4         214.542788     0.051133     214.524415      0.071793   \n",
       "...              ...          ...            ...           ...   \n",
       "428786    306.672174     0.163919     306.730549      0.206509   \n",
       "428787    291.124934     0.147318     291.137380      0.164838   \n",
       "428788    202.972820     0.064758     202.958539      0.080424   \n",
       "428789    152.478929     0.068413     152.480008      0.075165   \n",
       "428790    194.982143     0.040268     195.012846      0.055999   \n",
       "\n",
       "        log_returns_realized_volatility_300  \\\n",
       "0                                  0.003394   \n",
       "1                                  0.000699   \n",
       "2                                  0.001983   \n",
       "3                                  0.001863   \n",
       "4                                  0.001131   \n",
       "...                                     ...   \n",
       "428786                             0.002284   \n",
       "428787                             0.002217   \n",
       "428788                             0.001386   \n",
       "428789                             0.002783   \n",
       "428790                             0.001541   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.000196                4.521321e-10   \n",
       "1                                  0.000040                2.123766e-12   \n",
       "2                                  0.000115                1.070617e-10   \n",
       "3                                  0.000108                1.551546e-10   \n",
       "4                                  0.000065                3.550845e-11   \n",
       "...                                     ...                         ...   \n",
       "428786                             0.000132                2.117007e-10   \n",
       "428787                             0.000128                9.484441e-11   \n",
       "428788                             0.000080                3.031629e-11   \n",
       "428789                             0.000161                1.960849e-10   \n",
       "428790                             0.000089                2.007223e-11   \n",
       "\n",
       "        log_returns_mean_300  log_returns2_realized_volatility_300  \\\n",
       "0               7.138250e-06                              0.005032   \n",
       "1               8.823633e-07                              0.001448   \n",
       "2               1.729093e-06                              0.003583   \n",
       "3              -5.516464e-06                              0.002422   \n",
       "4              -2.164288e-06                              0.002412   \n",
       "...                      ...                                   ...   \n",
       "428786         -2.858852e-06                              0.004503   \n",
       "428787          3.674218e-06                              0.003652   \n",
       "428788         -2.437732e-06                              0.002686   \n",
       "428789          6.737309e-06                              0.004316   \n",
       "428790         -1.059089e-06                              0.001784   \n",
       "\n",
       "        log_returns2_weighted_volatility_300  ...  beta2-300  target_mean_enc  \\\n",
       "0                                   0.000291  ...   0.778640         0.002964   \n",
       "1                                   0.000084  ...   0.450700         0.002976   \n",
       "2                                   0.000207  ...   1.519335         0.002974   \n",
       "3                                   0.000140  ...   2.378868         0.002975   \n",
       "4                                   0.000140  ...   0.973303         0.002974   \n",
       "...                                      ...  ...        ...              ...   \n",
       "428786                              0.000260  ...   1.049953         0.003899   \n",
       "428787                              0.000211  ...   2.160123         0.003897   \n",
       "428788                              0.000155  ...  -0.046489         0.003903   \n",
       "428789                              0.000250  ...   1.315896         0.003896   \n",
       "428790                              0.000103  ...  -2.130582         0.003894   \n",
       "\n",
       "        spread_mean_enc  dom_mean_enc  target_std_enc  spread_std_enc  \\\n",
       "0              0.001023    391.300538        0.002289        0.000925   \n",
       "1              0.001027    391.419207        0.002414        0.000949   \n",
       "2              0.001025    390.605604        0.002411        0.000946   \n",
       "3              0.001026    391.181654        0.002411        0.000947   \n",
       "4              0.001027    391.062991        0.002412        0.000948   \n",
       "...                 ...           ...             ...             ...   \n",
       "428786         0.001108    382.917628        0.002595        0.000784   \n",
       "428787         0.001108    382.850800        0.002590        0.000783   \n",
       "428788         0.001109    383.053325        0.002601        0.000786   \n",
       "428789         0.001108    382.605401        0.002593        0.000784   \n",
       "428790         0.001107    382.545423        0.002589        0.000784   \n",
       "\n",
       "        dom_std_enc  encode_mean_beta  encode_mean_beta2  kmeans5  \n",
       "0        104.550467          0.635215           0.633070        0  \n",
       "1        104.729473          0.634261           0.632797        0  \n",
       "2        103.699696          0.632499           0.630289        0  \n",
       "3        104.727931          0.636051           0.633793        0  \n",
       "4        104.675748          0.633840           0.633266        0  \n",
       "...             ...               ...                ...      ...  \n",
       "428786    99.568403          0.961481           0.958937        4  \n",
       "428787    99.572674          0.962304           0.960865        4  \n",
       "428788    99.715361          0.959879           0.957927        4  \n",
       "428789    99.632305          0.961479           0.958523        4  \n",
       "428790    99.228970          0.961903           0.960295        4  \n",
       "\n",
       "[428791 rows x 180 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocksX = stocks.drop(['time_id','target_realized_volatility'], axis=1)\n",
    "stocksX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3ab5236b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap_mean_300</th>\n",
       "      <th>wap_std_300</th>\n",
       "      <th>wap2_mean_300</th>\n",
       "      <th>wap2_std_300</th>\n",
       "      <th>log_returns_realized_volatility_300</th>\n",
       "      <th>log_returns_weighted_volatility_300</th>\n",
       "      <th>log_returns_quarticity_300</th>\n",
       "      <th>log_returns_mean_300</th>\n",
       "      <th>log_returns2_realized_volatility_300</th>\n",
       "      <th>log_returns2_weighted_volatility_300</th>\n",
       "      <th>...</th>\n",
       "      <th>beta2-300</th>\n",
       "      <th>target_mean_enc</th>\n",
       "      <th>spread_mean_enc</th>\n",
       "      <th>dom_mean_enc</th>\n",
       "      <th>target_std_enc</th>\n",
       "      <th>spread_std_enc</th>\n",
       "      <th>dom_std_enc</th>\n",
       "      <th>encode_mean_beta</th>\n",
       "      <th>encode_mean_beta2</th>\n",
       "      <th>kmeans5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.116622</td>\n",
       "      <td>-0.073991</td>\n",
       "      <td>-0.116666</td>\n",
       "      <td>-0.029679</td>\n",
       "      <td>0.115178</td>\n",
       "      <td>0.115178</td>\n",
       "      <td>-0.025665</td>\n",
       "      <td>0.777948</td>\n",
       "      <td>0.209614</td>\n",
       "      <td>0.209614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045470</td>\n",
       "      <td>0.114352</td>\n",
       "      <td>0.921646</td>\n",
       "      <td>-0.183144</td>\n",
       "      <td>0.361328</td>\n",
       "      <td>1.626625</td>\n",
       "      <td>-0.196633</td>\n",
       "      <td>-0.157124</td>\n",
       "      <td>-0.157451</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.102802</td>\n",
       "      <td>-0.393314</td>\n",
       "      <td>-0.102803</td>\n",
       "      <td>-0.390362</td>\n",
       "      <td>-0.861672</td>\n",
       "      <td>-0.861672</td>\n",
       "      <td>-0.026308</td>\n",
       "      <td>0.104284</td>\n",
       "      <td>-0.714063</td>\n",
       "      <td>-0.714063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124573</td>\n",
       "      <td>0.127112</td>\n",
       "      <td>0.931338</td>\n",
       "      <td>-0.183137</td>\n",
       "      <td>0.562387</td>\n",
       "      <td>1.709131</td>\n",
       "      <td>-0.196617</td>\n",
       "      <td>-0.157617</td>\n",
       "      <td>-0.157589</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.077279</td>\n",
       "      <td>-0.245858</td>\n",
       "      <td>-0.077194</td>\n",
       "      <td>-0.250878</td>\n",
       "      <td>-0.396223</td>\n",
       "      <td>-0.396223</td>\n",
       "      <td>-0.026158</td>\n",
       "      <td>0.195464</td>\n",
       "      <td>-0.163976</td>\n",
       "      <td>-0.163976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133196</td>\n",
       "      <td>0.125187</td>\n",
       "      <td>0.926293</td>\n",
       "      <td>-0.183186</td>\n",
       "      <td>0.557938</td>\n",
       "      <td>1.699614</td>\n",
       "      <td>-0.196709</td>\n",
       "      <td>-0.158527</td>\n",
       "      <td>-0.158860</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.057618</td>\n",
       "      <td>-0.030774</td>\n",
       "      <td>-0.057842</td>\n",
       "      <td>-0.100720</td>\n",
       "      <td>-0.439829</td>\n",
       "      <td>-0.439829</td>\n",
       "      <td>-0.026089</td>\n",
       "      <td>-0.584773</td>\n",
       "      <td>-0.463032</td>\n",
       "      <td>-0.463032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340528</td>\n",
       "      <td>0.126183</td>\n",
       "      <td>0.928879</td>\n",
       "      <td>-0.183151</td>\n",
       "      <td>0.558200</td>\n",
       "      <td>1.703427</td>\n",
       "      <td>-0.196617</td>\n",
       "      <td>-0.156693</td>\n",
       "      <td>-0.157084</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.062326</td>\n",
       "      <td>-0.345399</td>\n",
       "      <td>-0.062375</td>\n",
       "      <td>-0.310409</td>\n",
       "      <td>-0.705091</td>\n",
       "      <td>-0.705091</td>\n",
       "      <td>-0.026260</td>\n",
       "      <td>-0.223794</td>\n",
       "      <td>-0.465613</td>\n",
       "      <td>-0.465613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.125147</td>\n",
       "      <td>0.932751</td>\n",
       "      <td>-0.183158</td>\n",
       "      <td>0.558976</td>\n",
       "      <td>1.707109</td>\n",
       "      <td>-0.196621</td>\n",
       "      <td>-0.157834</td>\n",
       "      <td>-0.157351</td>\n",
       "      <td>-1.287891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>0.187196</td>\n",
       "      <td>-0.076352</td>\n",
       "      <td>0.187356</td>\n",
       "      <td>-0.007289</td>\n",
       "      <td>-0.287284</td>\n",
       "      <td>-0.287284</td>\n",
       "      <td>-0.026008</td>\n",
       "      <td>-0.298588</td>\n",
       "      <td>0.073070</td>\n",
       "      <td>0.073070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019975</td>\n",
       "      <td>1.090489</td>\n",
       "      <td>1.137244</td>\n",
       "      <td>-0.183658</td>\n",
       "      <td>0.852915</td>\n",
       "      <td>1.137068</td>\n",
       "      <td>-0.197082</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>0.145088</td>\n",
       "      <td>-0.115953</td>\n",
       "      <td>0.145123</td>\n",
       "      <td>-0.101051</td>\n",
       "      <td>-0.311628</td>\n",
       "      <td>-0.311628</td>\n",
       "      <td>-0.026176</td>\n",
       "      <td>0.404924</td>\n",
       "      <td>-0.146166</td>\n",
       "      <td>-0.146166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287763</td>\n",
       "      <td>1.088662</td>\n",
       "      <td>1.136687</td>\n",
       "      <td>-0.183662</td>\n",
       "      <td>0.844337</td>\n",
       "      <td>1.133919</td>\n",
       "      <td>-0.197081</td>\n",
       "      <td>0.011807</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>-0.093662</td>\n",
       "      <td>-0.312896</td>\n",
       "      <td>-0.093700</td>\n",
       "      <td>-0.290989</td>\n",
       "      <td>-0.612571</td>\n",
       "      <td>-0.612571</td>\n",
       "      <td>-0.026268</td>\n",
       "      <td>-0.253240</td>\n",
       "      <td>-0.394997</td>\n",
       "      <td>-0.394997</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244502</td>\n",
       "      <td>1.094256</td>\n",
       "      <td>1.139253</td>\n",
       "      <td>-0.183650</td>\n",
       "      <td>0.862119</td>\n",
       "      <td>1.141944</td>\n",
       "      <td>-0.197069</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.007146</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>-0.230420</td>\n",
       "      <td>-0.304178</td>\n",
       "      <td>-0.230416</td>\n",
       "      <td>-0.302822</td>\n",
       "      <td>-0.106322</td>\n",
       "      <td>-0.106322</td>\n",
       "      <td>-0.026031</td>\n",
       "      <td>0.734773</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084124</td>\n",
       "      <td>1.086904</td>\n",
       "      <td>1.135989</td>\n",
       "      <td>-0.183677</td>\n",
       "      <td>0.849011</td>\n",
       "      <td>1.135628</td>\n",
       "      <td>-0.197076</td>\n",
       "      <td>0.011380</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>-0.115304</td>\n",
       "      <td>-0.371317</td>\n",
       "      <td>-0.115220</td>\n",
       "      <td>-0.345946</td>\n",
       "      <td>-0.556588</td>\n",
       "      <td>-0.556588</td>\n",
       "      <td>-0.026282</td>\n",
       "      <td>-0.104781</td>\n",
       "      <td>-0.627569</td>\n",
       "      <td>-0.627569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.747214</td>\n",
       "      <td>1.084767</td>\n",
       "      <td>1.134965</td>\n",
       "      <td>-0.183681</td>\n",
       "      <td>0.842662</td>\n",
       "      <td>1.134689</td>\n",
       "      <td>-0.197112</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>1.359310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wap_mean_300  wap_std_300  wap2_mean_300  wap2_std_300  \\\n",
       "0          -0.116622    -0.073991      -0.116666     -0.029679   \n",
       "1          -0.102802    -0.393314      -0.102803     -0.390362   \n",
       "2          -0.077279    -0.245858      -0.077194     -0.250878   \n",
       "3          -0.057618    -0.030774      -0.057842     -0.100720   \n",
       "4          -0.062326    -0.345399      -0.062375     -0.310409   \n",
       "...              ...          ...            ...           ...   \n",
       "428786      0.187196    -0.076352       0.187356     -0.007289   \n",
       "428787      0.145088    -0.115953       0.145123     -0.101051   \n",
       "428788     -0.093662    -0.312896      -0.093700     -0.290989   \n",
       "428789     -0.230420    -0.304178      -0.230416     -0.302822   \n",
       "428790     -0.115304    -0.371317      -0.115220     -0.345946   \n",
       "\n",
       "        log_returns_realized_volatility_300  \\\n",
       "0                                  0.115178   \n",
       "1                                 -0.861672   \n",
       "2                                 -0.396223   \n",
       "3                                 -0.439829   \n",
       "4                                 -0.705091   \n",
       "...                                     ...   \n",
       "428786                            -0.287284   \n",
       "428787                            -0.311628   \n",
       "428788                            -0.612571   \n",
       "428789                            -0.106322   \n",
       "428790                            -0.556588   \n",
       "\n",
       "        log_returns_weighted_volatility_300  log_returns_quarticity_300  \\\n",
       "0                                  0.115178                   -0.025665   \n",
       "1                                 -0.861672                   -0.026308   \n",
       "2                                 -0.396223                   -0.026158   \n",
       "3                                 -0.439829                   -0.026089   \n",
       "4                                 -0.705091                   -0.026260   \n",
       "...                                     ...                         ...   \n",
       "428786                            -0.287284                   -0.026008   \n",
       "428787                            -0.311628                   -0.026176   \n",
       "428788                            -0.612571                   -0.026268   \n",
       "428789                            -0.106322                   -0.026031   \n",
       "428790                            -0.556588                   -0.026282   \n",
       "\n",
       "        log_returns_mean_300  log_returns2_realized_volatility_300  \\\n",
       "0                   0.777948                              0.209614   \n",
       "1                   0.104284                             -0.714063   \n",
       "2                   0.195464                             -0.163976   \n",
       "3                  -0.584773                             -0.463032   \n",
       "4                  -0.223794                             -0.465613   \n",
       "...                      ...                                   ...   \n",
       "428786             -0.298588                              0.073070   \n",
       "428787              0.404924                             -0.146166   \n",
       "428788             -0.253240                             -0.394997   \n",
       "428789              0.734773                              0.025002   \n",
       "428790             -0.104781                             -0.627569   \n",
       "\n",
       "        log_returns2_weighted_volatility_300  ...  beta2-300  target_mean_enc  \\\n",
       "0                                   0.209614  ...  -0.045470         0.114352   \n",
       "1                                  -0.714063  ...  -0.124573         0.127112   \n",
       "2                                  -0.163976  ...   0.133196         0.125187   \n",
       "3                                  -0.463032  ...   0.340528         0.126183   \n",
       "4                                  -0.465613  ...   0.001486         0.125147   \n",
       "...                                      ...  ...        ...              ...   \n",
       "428786                              0.073070  ...   0.019975         1.090489   \n",
       "428787                             -0.146166  ...   0.287763         1.088662   \n",
       "428788                             -0.394997  ...  -0.244502         1.094256   \n",
       "428789                              0.025002  ...   0.084124         1.086904   \n",
       "428790                             -0.627569  ...  -0.747214         1.084767   \n",
       "\n",
       "        spread_mean_enc  dom_mean_enc  target_std_enc  spread_std_enc  \\\n",
       "0              0.921646     -0.183144        0.361328        1.626625   \n",
       "1              0.931338     -0.183137        0.562387        1.709131   \n",
       "2              0.926293     -0.183186        0.557938        1.699614   \n",
       "3              0.928879     -0.183151        0.558200        1.703427   \n",
       "4              0.932751     -0.183158        0.558976        1.707109   \n",
       "...                 ...           ...             ...             ...   \n",
       "428786         1.137244     -0.183658        0.852915        1.137068   \n",
       "428787         1.136687     -0.183662        0.844337        1.133919   \n",
       "428788         1.139253     -0.183650        0.862119        1.141944   \n",
       "428789         1.135989     -0.183677        0.849011        1.135628   \n",
       "428790         1.134965     -0.183681        0.842662        1.134689   \n",
       "\n",
       "        dom_std_enc  encode_mean_beta  encode_mean_beta2   kmeans5  \n",
       "0         -0.196633         -0.157124          -0.157451 -1.287891  \n",
       "1         -0.196617         -0.157617          -0.157589 -1.287891  \n",
       "2         -0.196709         -0.158527          -0.158860 -1.287891  \n",
       "3         -0.196617         -0.156693          -0.157084 -1.287891  \n",
       "4         -0.196621         -0.157834          -0.157351 -1.287891  \n",
       "...             ...               ...                ...       ...  \n",
       "428786    -0.197082          0.011382           0.007657  1.359310  \n",
       "428787    -0.197081          0.011807           0.008634  1.359310  \n",
       "428788    -0.197069          0.010554           0.007146  1.359310  \n",
       "428789    -0.197076          0.011380           0.007447  1.359310  \n",
       "428790    -0.197112          0.011600           0.008345  1.359310  \n",
       "\n",
       "[428791 rows x 180 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocksX[:] = StandardScaler().fit(stocksX).transform(stocksX)\n",
    "stocksX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ca516b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3830"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocksY = stocks[['target_realized_volatility', 'stock_id', 'time_id']]\n",
    "len(stocksY.time_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "201b258a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_realized_volatility</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002954</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000981</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001295</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001776</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001520</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428786</th>\n",
       "      <td>0.002899</td>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428787</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428788</th>\n",
       "      <td>0.002792</td>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428789</th>\n",
       "      <td>0.002379</td>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428790</th>\n",
       "      <td>0.001414</td>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428791 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target_realized_volatility  stock_id  time_id\n",
       "0                         0.002954         0        5\n",
       "1                         0.000981         0       11\n",
       "2                         0.001295         0       16\n",
       "3                         0.001776         0       31\n",
       "4                         0.001520         0       62\n",
       "...                            ...       ...      ...\n",
       "428786                    0.002899       126    32751\n",
       "428787                    0.003454       126    32753\n",
       "428788                    0.002792       126    32758\n",
       "428789                    0.002379       126    32763\n",
       "428790                    0.001414       126    32767\n",
       "\n",
       "[428791 rows x 3 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocksY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ea99df2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_realized_volatility              1.000000\n",
       "log_returns_weighted_volatility_300     0.904700\n",
       "log_returns_realized_volatility_300     0.904700\n",
       "log_returns_weighted_volatility_150     0.896073\n",
       "log_returns_realized_volatility_150     0.896073\n",
       "log_returns2_weighted_volatility_300    0.889992\n",
       "log_returns2_realized_volatility_300    0.889992\n",
       "log_returns2_weighted_volatility_150    0.882288\n",
       "log_returns2_realized_volatility_150    0.882288\n",
       "log_returns_weighted_volatility_75      0.864377\n",
       "log_returns_realized_volatility_75      0.864377\n",
       "log_returns2_weighted_volatility_75     0.851997\n",
       "log_returns2_realized_volatility_75     0.851997\n",
       "price_diff_amax_150                     0.826341\n",
       "price_diff_amax_75                      0.822470\n",
       "price_diff2_amax_150                    0.812327\n",
       "price_diff_amax_300                     0.808808\n",
       "price_diff2_amax_75                     0.808346\n",
       "price_diff2_amax_300                    0.791596\n",
       "bid_ask_spread1_mean_300                0.786301\n",
       "Name: target_realized_volatility, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks.corr()['target_realized_volatility'].nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854343e",
   "metadata": {},
   "source": [
    "### PolyRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3c0ef4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  1.15178472e-01,  1.15178472e-01, ...,\n",
       "        -3.39332584e-04, -1.80268760e-04, -9.57668889e-05],\n",
       "       [ 1.00000000e+00, -8.61672011e-01, -8.61672011e-01, ...,\n",
       "        -8.73022721e-04, -2.91421509e-04, -9.72786780e-05],\n",
       "       [ 1.00000000e+00, -3.96223010e-01, -3.96223010e-01, ...,\n",
       "        -2.78156807e-04, -1.66880027e-04, -1.00119583e-04],\n",
       "       ...,\n",
       "       [ 1.00000000e+00, -6.12570671e-01, -6.12570671e-01, ...,\n",
       "         2.90446383e-07, -6.16735692e-09,  1.30958048e-10],\n",
       "       [ 1.00000000e+00, -1.06322387e-01, -1.06322387e-01, ...,\n",
       "         2.07317784e-07, -6.29089831e-09,  1.90892459e-10],\n",
       "       [ 1.00000000e+00, -5.56587551e-01, -5.56587551e-01, ...,\n",
       "         6.19762228e-08, -3.60767623e-09,  2.10005179e-10]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "EPSILON = 0\n",
    "feats = ['log_returns_realized_volatility_300','log_returns_weighted_volatility_300',\n",
    "         'beta-300','ofi_mean_300', 'dom_imbalance_mean_150', 'encode_mean_beta']\n",
    "\n",
    "x = stocksX[feats]\n",
    "deg = 5\n",
    "\n",
    "poly = PolynomialFeatures(degree=deg)\n",
    "poly_stocks_X = poly.fit_transform(x)\n",
    "poly_stocks_y = stocksY.reset_index(drop=True)['target_realized_volatility'].values.flatten()\n",
    "\n",
    "weights = 1/np.square(poly_stocks_y+EPSILON)\n",
    "poly_stocks_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7753ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "Outer Fold : 1\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2676\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2745\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.289\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2862\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2821\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 2\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2777\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2715\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2666\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2786\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2646\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 3\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2778\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2845\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2674\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2741\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2851\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 4\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.3358\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2745\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2717\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.281\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2941\n",
      "\t**********************************************************************\n",
      "************************************************************************************************************************\n",
      "Outer Fold : 5\n",
      "************************************************************************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 1  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2766\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 2  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2845\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 3  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2743\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 4  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2717\n",
      "\t**********************************************************************\n",
      "\n",
      "\t********************\n",
      "\t*  Inner Fold : 5  *\n",
      "\t********************\n",
      "\n",
      "\t**********************************************************************\n",
      "\tInner Validation RMSPE: \t0.2657\n",
      "\t**********************************************************************\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "poly_test = pd.DataFrame(\n",
    "    {'target_realized_volatility':[],'predicted_volatility_poly': [],'time_id':[], 'stock_id':[]}\n",
    ")\n",
    "\n",
    "poly_models = []\n",
    "train_scores = []\n",
    "\n",
    "inner_k = 5\n",
    "outer_k = 5\n",
    "\n",
    "#params.update(search.best_trial.params)\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "outer_kfold = KFold(n_splits=outer_k, random_state=42, shuffle=True)\n",
    "for outer_fold, (outer_train_idx, outer_test_idx) in enumerate(outer_kfold.split(x, stocksY)):\n",
    "    print('*'*120)\n",
    "    print(\"Outer Fold :\", outer_fold + 1)\n",
    "    print('*'*120)\n",
    "\n",
    "    X_outer_train = x.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    X_outer_test = x.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    y_outer_train = stocksY.iloc[outer_train_idx].reset_index(drop=True)\n",
    "    y_outer_test = stocksY.iloc[outer_test_idx].reset_index(drop=True)\n",
    "    \n",
    "    target = np.zeros(len(y_outer_test))\n",
    "\n",
    "    inner_scores = 0.0\n",
    "\n",
    "    models = []\n",
    "    \n",
    "    inner_kfold = KFold(n_splits= inner_k, random_state=42, shuffle=True)\n",
    "    for inner_fold, (inner_train_idx, inner_valid_idx) in enumerate(inner_kfold.split(X_outer_train, y_outer_train)):\n",
    "        print(\"\\n\\t\"+\"*\"*20)\n",
    "        print(f\"\\t*  Inner Fold : {inner_fold + 1}  *\")\n",
    "        print(\"\\t\"+\"*\"*20+\"\\n\")\n",
    "    \n",
    "        # inner train data and valid data\n",
    "        X_inner_train = X_outer_train.iloc[inner_train_idx].reset_index(drop=True)\n",
    "        X_inner_valid = X_outer_train.iloc[inner_valid_idx].reset_index(drop=True)\n",
    "\n",
    "        y_inner_train = y_outer_train.iloc[\n",
    "            inner_train_idx].reset_index(drop=True)['target_realized_volatility'].values.flatten()\n",
    "        y_inner_valid = y_outer_train.iloc[\n",
    "            inner_valid_idx].reset_index(drop=True)['target_realized_volatility'].values.flatten()\n",
    "            \n",
    "        weights_train = 1/np.square(y_inner_train+EPSILON)\n",
    "\n",
    "        poly = PolynomialFeatures(degree=deg)\n",
    "        poly_train_X = poly.fit_transform(X_inner_train)\n",
    "        poly_valid_X = poly.fit_transform(X_inner_valid)\n",
    "        \n",
    "        model.fit(poly_train_X, y_inner_train, sample_weight=weights_train)\n",
    "       \n",
    "        # validation \n",
    "        y_inner_pred = model.predict(poly_valid_X)\n",
    "        RMSPE = rmspe(\n",
    "            y_true=(y_inner_valid), \n",
    "            y_pred=(y_inner_pred), n=4\n",
    "        )\n",
    "        \n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "        print(f'\\tInner Validation RMSPE: \\t{RMSPE}')\n",
    "        print(\"\\t\"+\"*\" * 70)\n",
    "\n",
    "        # keep training validation score\n",
    "        inner_scores += RMSPE / inner_k\n",
    "        \n",
    "        models.append(model)\n",
    "    \n",
    "    # store all models for prediction in oof evaluation\n",
    "    poly_models.append(models)\n",
    "    train_scores.append(inner_scores)\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=deg)\n",
    "    poly_test_X = poly.fit_transform(X_outer_test)\n",
    "    \n",
    "    # out of fold test set\n",
    "    for model in poly_models[outer_fold]:\n",
    "        y_outer_pred = model.predict(poly_test_X)\n",
    "        target += y_outer_pred / len(poly_models[outer_fold])\n",
    "   \n",
    "    # out of fold test set 600\n",
    "    y_outer_test = y_outer_test.assign(predicted_volatility_poly = target)\n",
    "    \n",
    "    poly_test = pd.concat([poly_test, y_outer_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bafbb",
   "metadata": {},
   "source": [
    "### Training Validation Result with 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7aaa77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result RMSPE-poly: 0.279088\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Result RMSPE-poly: {np.mean(train_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e455162f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27988, 0.27180000000000004, 0.27778, 0.29142, 0.27455999999999997]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "687eff66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.278368"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(poly_test['target_realized_volatility'], poly_test['predicted_volatility_poly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2cfddd",
   "metadata": {},
   "source": [
    "# Merging Prediction Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "05d03a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving test tables\n",
    "norm_test.to_csv(\"norm_test.csv\")\n",
    "simple_test.to_csv(\"simple_test.csv\")\n",
    "extend_test.to_csv(\"extend_test.csv\")\n",
    "poly_test.to_csv(\"poly_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d2ca417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle dump to save model lists\n",
    "import pickle\n",
    "\n",
    "pickle.dump(norm_models, open(\"norm_models.pickle\",\"wb\"))\n",
    "pickle.dump(simple_models, open(\"simple_models.pickle\",\"wb\"))\n",
    "pickle.dump(extend_models, open(\"extend_models.pickle\",\"wb\"))\n",
    "pickle.dump(poly_models, open(\"poly_models.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "55fed27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realized_volatility</th>\n",
       "      <th>norm_predict</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>simple_predict</th>\n",
       "      <th>extend_predict</th>\n",
       "      <th>poly_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.004512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428926</th>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>32712.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.003848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>0.010190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.002305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        realized_volatility  norm_predict  time_id  stock_id  simple_predict  \\\n",
       "0                  0.003397      0.002773    103.0       0.0        0.002806   \n",
       "1                  0.002993      0.004439    146.0       0.0        0.004295   \n",
       "2                  0.001094      0.001236    250.0       0.0        0.001244   \n",
       "3                  0.001711      0.001649    297.0       0.0        0.001679   \n",
       "4                  0.001197      0.002248    319.0       0.0        0.002181   \n",
       "...                     ...           ...      ...       ...             ...   \n",
       "428926             0.004120      0.002958  32712.0     126.0        0.002781   \n",
       "428927             0.003511      0.004066  32724.0     126.0        0.004173   \n",
       "428928             0.010431      0.010297  32746.0     126.0        0.010131   \n",
       "428929             0.001827      0.002151  32750.0     126.0        0.002191   \n",
       "428930             0.003454      0.002089  32753.0     126.0        0.002073   \n",
       "\n",
       "        extend_predict  poly_predict  \n",
       "0                  NaN      0.003289  \n",
       "1             0.004247      0.004512  \n",
       "2                  NaN      0.001410  \n",
       "3                  NaN      0.001702  \n",
       "4                  NaN      0.003186  \n",
       "...                ...           ...  \n",
       "428926             NaN      0.002591  \n",
       "428927        0.004358      0.003848  \n",
       "428928        0.009286      0.010190  \n",
       "428929        0.002251      0.002305  \n",
       "428930             NaN      0.001831  \n",
       "\n",
       "[428931 rows x 7 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make master table\n",
    "master = pd.merge(norm_test, simple_test, how=\"left\", on=[\"time_id\",\"stock_id\"])\n",
    "master = master.drop(\"target_realized_volatility_y\",axis=1)\n",
    "master = pd.merge(master, extend_test, how=\"left\", on=[\"time_id\",\"stock_id\"])\n",
    "master = master.drop([\"target_realized_volatility\"], axis=1)\n",
    "master = pd.merge(master, poly_test, how=\"left\", on=[\"time_id\",\"stock_id\"])\n",
    "master = master.drop(['target_realized_volatility'], axis=1)\n",
    "master = master.rename(columns={\n",
    "    \"target_realized_volatility_x\": \"realized_volatility\",\n",
    "    \"predicted_volatility_norm\": \"norm_predict\",\n",
    "    \"predicted_volatility_simple\": \"simple_predict\",\n",
    "    \"predicted_volatility_extend\": \"extend_predict\",\n",
    "    \"predicted_volatility_poly\": \"poly_predict\"}, errors=\"raise\")\n",
    "master.to_csv(\"master.csv\")\n",
    "master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5392b1",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "608a50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.read_csv(\"master.csv\").drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "10b1edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [\"norm_predict\", \"simple_predict\", \"extend_predict\", \"poly_predict\"]\n",
    "master['average'] = master[subset].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "ea933d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.255601"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(master['realized_volatility'], master['average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f917383",
   "metadata": {},
   "source": [
    "### Weighted Average\n",
    "\n",
    "* Merge and cover gaps with best performing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "6daa0814",
   "metadata": {},
   "outputs": [],
   "source": [
    "master['extend_predict'] = np.where(master['extend_predict'].isna(), \n",
    "                                    master['simple_predict'], master['extend_predict'])\n",
    "master['extend_predict'] = np.where(master['extend_predict'].isna(), \n",
    "                                    master['norm_predict'], master['extend_predict'])\n",
    "master['simple_predict'] = np.where(master['simple_predict'].isna(), \n",
    "                                    master['norm_predict'], master['simple_predict'])\n",
    "master['poly_predict'] = np.where(master['poly_predict'].isna(), \n",
    "                                  master['norm_predict'], master['poly_predict'])\n",
    "\n",
    "#master[\"waverage\"] = norm * + simp * + ext * + poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "7ae9c63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realized_volatility</th>\n",
       "      <th>norm_predict</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>simple_predict</th>\n",
       "      <th>extend_predict</th>\n",
       "      <th>poly_predict</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.002956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.004373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.001677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.002538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428926</th>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>32712.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.002777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.004111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.009976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.002224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.001998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        realized_volatility  norm_predict  time_id  stock_id  simple_predict  \\\n",
       "0                  0.003397      0.002773    103.0       0.0        0.002806   \n",
       "1                  0.002993      0.004439    146.0       0.0        0.004295   \n",
       "2                  0.001094      0.001236    250.0       0.0        0.001244   \n",
       "3                  0.001711      0.001649    297.0       0.0        0.001679   \n",
       "4                  0.001197      0.002248    319.0       0.0        0.002181   \n",
       "...                     ...           ...      ...       ...             ...   \n",
       "428926             0.004120      0.002958  32712.0     126.0        0.002781   \n",
       "428927             0.003511      0.004066  32724.0     126.0        0.004173   \n",
       "428928             0.010431      0.010297  32746.0     126.0        0.010131   \n",
       "428929             0.001827      0.002151  32750.0     126.0        0.002191   \n",
       "428930             0.003454      0.002089  32753.0     126.0        0.002073   \n",
       "\n",
       "        extend_predict  poly_predict   average  \n",
       "0             0.002806      0.003289  0.002956  \n",
       "1             0.004247      0.004512  0.004373  \n",
       "2             0.001244      0.001410  0.001297  \n",
       "3             0.001679      0.001702  0.001677  \n",
       "4             0.002181      0.003186  0.002538  \n",
       "...                ...           ...       ...  \n",
       "428926        0.002781      0.002591  0.002777  \n",
       "428927        0.004358      0.003848  0.004111  \n",
       "428928        0.009286      0.010190  0.009976  \n",
       "428929        0.002251      0.002305  0.002224  \n",
       "428930        0.002073      0.001831  0.001998  \n",
       "\n",
       "[428931 rows x 8 columns]"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "6893c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_minimize(args):\n",
    "    norm = master['norm_predict']\n",
    "    simp = master['simple_predict']\n",
    "    ext = master['extend_predict']\n",
    "    poly = master['poly_predict']\n",
    "    #avg = master['average']\n",
    "    \n",
    "    w_norm, w_simp, w_ext, w_poly = args\n",
    "    \n",
    "    out = (w_norm * norm) + (w_simp * simp) + (w_ext * ext) + (w_poly * poly) #+ (w_avg * avg)\n",
    "    \n",
    "    return rmspe(master['realized_volatility'], out)\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "start_weights = [.25, .25, .25, .25]#, .2]\n",
    "\n",
    "res = minimize(rmspe_minimize, start_weights, method='Nelder-Mead', tol=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "a2597b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0541321 ,  0.9553657 ,  0.00432063, -0.02161786])"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "cb3a34d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.252502"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = master['norm_predict']\n",
    "simp = master['simple_predict']\n",
    "ext = master['extend_predict']\n",
    "poly = master['poly_predict']\n",
    "avg = master['average']\n",
    "\n",
    "master['wav'] = norm*res.x[0] + simp*res.x[1] + ext*res.x[2] + poly*res.x[3] #+ avg*res.x[4]\n",
    "\n",
    "rmspe(master['realized_volatility'], master['wav'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "10900452",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master.rename(columns={'wav': 'ensemble-weighted', \"average\": \"ensemble-average\"})\n",
    "master.to_csv(\"final-ensemble.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a338400",
   "metadata": {},
   "source": [
    "## RMSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "d378f3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result RMSPE-ensemble: 0.252502\n"
     ]
    }
   ],
   "source": [
    "RMSPE = rmspe(y_true=master['realized_volatility'], y_pred=master['ensemble-weighted'])\n",
    "\n",
    "print(f'Test Result RMSPE-ensemble: {RMSPE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6406ea",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "fad9d2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result MAPE-ensemble: 0.18400463210667212\n"
     ]
    }
   ],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true+1e-10)))\n",
    "\n",
    "print(f\"Test Result MAPE-ensemble: {mape(master['realized_volatility'], master['ensemble-weighted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "1cf7dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_ens(test):\n",
    "    return rmspe(test['realized_volatility'], test['ensemble-weighted'])\n",
    "def rmspe_avg(test):\n",
    "    return rmspe(test['realized_volatility'], test['ensemble-average'])\n",
    "def rmspe_poly(test):\n",
    "    return rmspe(test['realized_volatility'], test['poly_predict'])\n",
    "def rmspe_ext(test):\n",
    "    return rmspe(test['realized_volatility'], test['extend_predict'])\n",
    "def rmspe_simp(test):\n",
    "    return rmspe(test['realized_volatility'], test['simple_predict'])\n",
    "def rmspe_norm(test):\n",
    "    return rmspe(test['realized_volatility'], test['norm_predict'])\n",
    "\n",
    "master['rmspe-weighted'] = master.apply(rmspe_ens, axis=1)\n",
    "master['rmspe-average'] = master.apply(rmspe_avg, axis=1)\n",
    "master['rmspe-poly'] = master.apply(rmspe_poly, axis=1)\n",
    "master['rmspe-ext'] = master.apply(rmspe_ext, axis=1)\n",
    "master['rmspe-simp'] = master.apply(rmspe_simp, axis=1)\n",
    "master['rmspe-norm'] = master.apply(rmspe_norm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "d7fbf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master = master.rename(columns={\n",
    "#     \"norm_predict\": \"lgbm-norm\",\n",
    "#     \"simple_predict\": \"lgbm-simp\",\n",
    "#     \"extend_predict\": \"lgbm-ext\",\n",
    "#     \"poly_predict\": \"poly-reg\"}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "ed3812c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realized_volatility</th>\n",
       "      <th>lgbm-norm</th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>lgbm-simp</th>\n",
       "      <th>lgbm-ext</th>\n",
       "      <th>poly-reg</th>\n",
       "      <th>ensemble-average</th>\n",
       "      <th>ensemble-weighted</th>\n",
       "      <th>rmspe-weighted</th>\n",
       "      <th>rmspe-average</th>\n",
       "      <th>rmspe-poly</th>\n",
       "      <th>rmspe-ext</th>\n",
       "      <th>rmspe-simp</th>\n",
       "      <th>rmspe-norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.183877</td>\n",
       "      <td>0.129677</td>\n",
       "      <td>0.031670</td>\n",
       "      <td>0.173835</td>\n",
       "      <td>0.173835</td>\n",
       "      <td>0.183526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.424850</td>\n",
       "      <td>0.461346</td>\n",
       "      <td>0.507842</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.435070</td>\n",
       "      <td>0.483372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.124998</td>\n",
       "      <td>0.185491</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.137539</td>\n",
       "      <td>0.137539</td>\n",
       "      <td>0.130152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.027278</td>\n",
       "      <td>0.019846</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.018368</td>\n",
       "      <td>0.018368</td>\n",
       "      <td>0.036184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.792124</td>\n",
       "      <td>1.120070</td>\n",
       "      <td>1.661073</td>\n",
       "      <td>0.821435</td>\n",
       "      <td>0.821435</td>\n",
       "      <td>0.877701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428926</th>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>32712.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.326850</td>\n",
       "      <td>0.326010</td>\n",
       "      <td>0.371161</td>\n",
       "      <td>0.324909</td>\n",
       "      <td>0.324909</td>\n",
       "      <td>0.281960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>32724.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.179623</td>\n",
       "      <td>0.170816</td>\n",
       "      <td>0.095997</td>\n",
       "      <td>0.241002</td>\n",
       "      <td>0.188311</td>\n",
       "      <td>0.157955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>32746.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.009976</td>\n",
       "      <td>0.010056</td>\n",
       "      <td>0.035947</td>\n",
       "      <td>0.043606</td>\n",
       "      <td>0.023060</td>\n",
       "      <td>0.109770</td>\n",
       "      <td>0.028760</td>\n",
       "      <td>0.012832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>32750.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.187058</td>\n",
       "      <td>0.217298</td>\n",
       "      <td>0.261440</td>\n",
       "      <td>0.232021</td>\n",
       "      <td>0.198802</td>\n",
       "      <td>0.176930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>32753.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.402598</td>\n",
       "      <td>0.421597</td>\n",
       "      <td>0.469833</td>\n",
       "      <td>0.399670</td>\n",
       "      <td>0.399670</td>\n",
       "      <td>0.395287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428931 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        realized_volatility  lgbm-norm  time_id  stock_id  lgbm-simp  \\\n",
       "0                  0.003397   0.002773    103.0       0.0   0.002806   \n",
       "1                  0.002993   0.004439    146.0       0.0   0.004295   \n",
       "2                  0.001094   0.001236    250.0       0.0   0.001244   \n",
       "3                  0.001711   0.001649    297.0       0.0   0.001679   \n",
       "4                  0.001197   0.002248    319.0       0.0   0.002181   \n",
       "...                     ...        ...      ...       ...        ...   \n",
       "428926             0.004120   0.002958  32712.0     126.0   0.002781   \n",
       "428927             0.003511   0.004066  32724.0     126.0   0.004173   \n",
       "428928             0.010431   0.010297  32746.0     126.0   0.010131   \n",
       "428929             0.001827   0.002151  32750.0     126.0   0.002191   \n",
       "428930             0.003454   0.002089  32753.0     126.0   0.002073   \n",
       "\n",
       "        lgbm-ext  poly-reg  ensemble-average  ensemble-weighted  \\\n",
       "0       0.002806  0.003289          0.002956           0.002772   \n",
       "1       0.004247  0.004512          0.004373           0.004264   \n",
       "2       0.001244  0.001410          0.001297           0.001230   \n",
       "3       0.001679  0.001702          0.001677           0.001664   \n",
       "4       0.002181  0.003186          0.002538           0.002146   \n",
       "...          ...       ...               ...                ...   \n",
       "428926  0.002781  0.002591          0.002777           0.002773   \n",
       "428927  0.004358  0.003848          0.004111           0.004142   \n",
       "428928  0.009286  0.010190          0.009976           0.010056   \n",
       "428929  0.002251  0.002305          0.002224           0.002169   \n",
       "428930  0.002073  0.001831          0.001998           0.002063   \n",
       "\n",
       "        rmspe-weighted  rmspe-average  rmspe-poly  rmspe-ext  rmspe-simp  \\\n",
       "0             0.183877       0.129677    0.031670   0.173835    0.173835   \n",
       "1             0.424850       0.461346    0.507842   0.419100    0.435070   \n",
       "2             0.124998       0.185491    0.288782   0.137539    0.137539   \n",
       "3             0.027278       0.019846    0.004986   0.018368    0.018368   \n",
       "4             0.792124       1.120070    1.661073   0.821435    0.821435   \n",
       "...                ...            ...         ...        ...         ...   \n",
       "428926        0.326850       0.326010    0.371161   0.324909    0.324909   \n",
       "428927        0.179623       0.170816    0.095997   0.241002    0.188311   \n",
       "428928        0.035947       0.043606    0.023060   0.109770    0.028760   \n",
       "428929        0.187058       0.217298    0.261440   0.232021    0.198802   \n",
       "428930        0.402598       0.421597    0.469833   0.399670    0.399670   \n",
       "\n",
       "        rmspe-norm  \n",
       "0         0.183526  \n",
       "1         0.483372  \n",
       "2         0.130152  \n",
       "3         0.036184  \n",
       "4         0.877701  \n",
       "...            ...  \n",
       "428926    0.281960  \n",
       "428927    0.157955  \n",
       "428928    0.012832  \n",
       "428929    0.176930  \n",
       "428930    0.395287  \n",
       "\n",
       "[428931 rows x 15 columns]"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "ee769b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "master.to_csv(\"final-ensemble.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b5464",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "1a0d5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "597f757f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=master[[\"rmspe-weighted\"]], orient=\"h\", fliersize=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "ee1ef875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.violinplot(data=master[[\"rmspe-weighted\"]], orient=\"h\", fliersize=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "f66a1f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=master[[\"rmspe-weighted\"]], orient=\"h\", showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a423ee9",
   "metadata": {},
   "source": [
    "### Write Predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8968d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.astype({'time_id': int, 'stock_id': int})\n",
    "test.to_csv(\"extend3.csv\", index=False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe34fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"extend3.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d23168",
   "metadata": {},
   "source": [
    "## Model Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_importance(importance, title='', save_to_file=None, top=None):    \n",
    "    importance = importance.sort_values(\n",
    "        ['Importance'], ascending=False\n",
    "    )[:top]#.sort_values(['Importance'])\n",
    "    sns.set(font_scale=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    #importance.plot.barh(ax=ax)\n",
    "    sns.barplot(x=\"Importance\", y=\"Features\", data=importance.sort_values(by=\"Importance\",ascending=False))\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_to_file:\n",
    "        plt.savefig(save_to_file)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def compute_mean_importance(importance):\n",
    "    res = importance[0].copy()\n",
    "    res['Importance'] = np.mean(np.array(\n",
    "        [df['importance'].values for df in importance]\n",
    "    ), axis=0)\n",
    "    \n",
    "    res = res.drop(['importance'], axis=1)\n",
    "    \n",
    "    # reformat for plot\n",
    "    return pd.DataFrame(\n",
    "        {'Features':[f for f in res.index], \n",
    "         'Importance': res['Importance']}\n",
    "    ).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a3528",
   "metadata": {},
   "source": [
    "### Plotting Importance by Gain & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c47ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_gain = compute_mean_importance(gain_importance)\n",
    "\n",
    "plot_importance(mean_gain, title='Extend-3 Model: Top 40 Features by Gain', top=40,\n",
    "               save_to_file=\"importance_by_gain_extend3.png\")\n",
    "\n",
    "mean_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41abbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gain = compute_mean_importance(split_importance)\n",
    "\n",
    "plot_importance(mean_gain, title='Extend-3 Model: Top 40 Features by Split', top=40,\n",
    "               save_to_file=\"importance_by_split_extend3.png\")\n",
    "\n",
    "mean_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.plot_tree(models[0], tree_index = 0, figsize=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e31bf7",
   "metadata": {},
   "source": [
    "## Post-Hoc ANOVA tests\n",
    "https://statisticsbyjim.com/anova/post-hoc-tests-anova/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c655c89",
   "metadata": {},
   "source": [
    "Every time we t-test the same data there is chance for a Type I error (i.e. reject null hypothesis when it is true), which is often the set significance level of 0.05. If we conduct multiple t-tests, Type I errors are accumulative. For two t-tests the chance for Type I error ~15% as seen in table below. Thus, we use ANOVA and post-hoc tests to control Type I error to always be 0.05 even with multiple comparisons."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAFuCAYAAABqeRZxAAAgAElEQVR4Xu2dPULduNvFxV5CCl5WQFYwpElFm46UoaGjpEvDlEmXlmqaMCsIK7h/Cpi95LUtydbnI8nWtWT5UM3k2vo4z6NjSffq55M/3R/r/v73v/+x//u//+v/s4m/1vpTU1CgbU3RmN8WNY4nMIL5Qu71ThhBG5HXjOBwOAwzgvPz8zZ6h15AASgQrUA3/odrxxnByckJE6uE6EJwIRSAAttVQB3zMILtxhEthwKLFIARLJIPN0OBNhSAEbQRR/QCCixSAEawSD7cDAXaUGAlI/iP/f3hlN08X7OnP9/ZX8z8/1xi/su+nFyyH0zWs7Tc3OUtbU8N929Nk9RcE/27eGBvv7+ydzVIvkIbMhqBFLxvtTkQU4xAJpre++unP+z7XyFFcidp7vKU9v/7hZ1c/pj+YTOJd0RNQuH1fp6Se6FKChjBf3+zD6c37Fk07eLhjf3+6rEg77WucRP/QMxnBKKB7OKCPT8/M33gzjECc/bQ2UuUGYQCXf7z//7+wE5vurBfP7E/wt36f/vMfvoToHyz621BUu6FurGyEciBLXLh3y8nrH8+OHOdvFa0W8mpUE/Vz7MZAU/uc/b0dsbue3fTGrTECBiTA2d0SuNpOjmoUY9MkIcHdn5z0y0ZLjqfeu6M6oI9vP1mg+mKssYypNjDE/oj+6UtafrLeaCGP2cfpbxKHVpEYp6oprsrZY19emJXj5fdcks25Q/79M/UtimReFmHB/v6cYbl09PSrzfnW/YarQnRDxZul19rPcXTci9U72QED+c37EbEWhuY3vxLGXr8WpnbY/mybMeApq9lfFlc1ghUNzo19gOG7ibsEdgDRTOCj7/Yh8+M/RzWb4FylWmUFNoUc3w6y6m5ZgxMb7dhGv/9/YX9+vi9MxR7HcqT2DE1IwLNU8PsvyxbmAGT00jz/xnjZvZm7JNMg5F/Lvuk3O/T06GfFUuvJoF+vJvaxWNjXO8t1xxsqbkX0CPUrl4Tb/4pbTOXfuPzQd97sGYAxlNf7S19bQ1GYAWNT329T2pys9BMIPn/7icsF0d+5p4RPKvrb01oblov59fsR+f8/Ubm6TCzYWLGYJQng2uu513BE9eaUzzXskCfMPC9A3WdaBlhP+Ma2+AzIamJPdW1nixa/ip6StPR+hupiTnTUp5+mmEpZWuxfBN7KKG9k+TcC+lhf67nmG5E1Gcx84PsRqBWmjA7yLI0sMTQptcRT27n1Fn9R90EtCnjcFnACFxTeNY783v27eSenT1dscfLR3bVLRfef+sGwkG6tm+QybaJJ77P/YfVg7HJGZgRWMugYZKgmEM/I8pkBNJsvHpKIyCXecZySWwUc0NVHwZGP+TMxWcE3bJNb5d74ys99/xGEGVQVrvU/IsZ+i4jUfJk9oxA3UmfNk9j99UyGIF7l5931zNAk2YEunDm0yxqRmA4Iy+je+oPBvDC7sS69/FK7CWM1/u/etIG7Ptv1lPcmxLjdNuzo0s8SYegnoqlQaYZwe0rH7AyYTQ9I41A9jWkiR67+Cev0xyHSufk3rIZwcdfhF7qRn/k0sC37nd9c5ByrTRR8hsIJUmXG4Ejcfvy9eAZa+3FRiCn7uaywbM0MKdI6i7z+d2wcz8Id+g2E7tvPM7Hp7heXv+UG3f2tX6b63L6yTA+6dR2deV9eL2d1vi+gR6cbcknqbE06G3ZsYegL4UMPSOMIKiJ17BoI+gHnFtrZbTNyj1zj8Czp+KZqXAj8OVf+oygGyh8hqd9a2DGTjw0qGu7pRTPn2EHXOwT+Tas7XYuNAJjI0t1RO3JZ+40Uz/yCO2q60+B6+t+fX9wr+m90ywzGYb5nvheX31S+zYjhZCawbieTv7vcce9AhkT1z7GGC+lnLlGcHHdTdp/sB/Ktwz8WwNCzwgjmH4c5tDE+M5b/31JaEag/jZgWGONX7Xymubmnqw3oId3yULl3wwjGEKg/6bE/MZH+4Gc99rp4avPxuPatNAI4irBVSUVsAdcydaUrxt6uGIAIyifmUduARJfFxh6wAiOPOTqLB6JDyMIZyZmBGGNcAUUaF4BzQjALGw+3uggFPAq0DyzEKTd42U/tD2etmuWvIulAZL1eCkFbY+n7ZolwwjWVLvBumAEbQQVRtBGHIv1AkZQTPqsFcMIssq5v8JgBG3EHEawuTiGfoK9bofqNYK6dEqPyrrtTzAC47ffQ8/iDzWkC5HvDmeyghmYRWBdW89pwIRz8VkaNRSy7kCy250KTTVLWLf9cUbgPJDTNXw4VNIf4+3JxPX+mUYAZmC+WDmNIAQTyVd9xSUtNYJ1uxZhBJHO5OTb9QZhPiVs/t7ENzRPlPF7Kd5eDNfO/dSiCK/hNncwAzADu1yNNQIfd1I9Gk1yFcfThnKAuDiOkk3pYCsmsx77esxZsDoDpnLTMXseT3+W5GX6zSVsBEHGnijcybczTcTD37NAIL6z9AZvT+CsJHxhYgjqHdaSNdifyDbLZdHIENwnMzDWCPSp+if2Tw/aHGcO5rFwI84hJmQMW3G8Jpb1GOJQTm12MhcdzI3ivExikhFtBBbldyzUgCao08IQt05it0JGoJSpicniuHZqsoIZKPFhAsixkBkY3iOwn4Aydaxz9744SyqTAXLp2Y5uapP6NPflZ4D16GIxaBzKWJ6CgyUx9GN9Xia12Ig2AhsMYTw5HRCQaP7eDCNw8/bc0/2UGUF0mzOgwlpgBsbPCNQNvH6fWaX52oPKhT1zJbJmBBRbMRXoIuGpjkq1GYAXYOLaIxD/VoqXuWhG4N19DRuB9c6AYX9RYb5ZTu/eI5imkA4O/LgyccAyxWdasoIZKPiEoSfalDV+ZmD8HsHwjJYvdhFFTzw9vxEMg07M/Lz8PSeJKkS0DswIjGWnPYZC+rk3C4vyMpcZQXe3E+MdYQTSRELcOvmKtPHbiXjeXpBrZ21oKXRcMAOVdbrOOoxiBiZsFo5sviEXenq0+o5Kc4/A924GzwbvMYxgfL+Bb1M5ZAQmO1LfSxveCFaAl+nzgvDSYLwz8B2xjw9IcusUk+nr6ZLk6eqRXZpwSC9vL8S14413/Y4AzMBQIs/R1p8jT+xSe5WXvlcjXs7hjbOxrPDtT+VcGgz8TVd/pDGE9BumQNM7Db0bo+vzMl1mkGAExLziaB/loeuU/fVbnj4cTeKFBefRtm2NFkq8yu0wgqPL3HaSwwiOnkCrVAAjOLrMMIKwxG1rFO5/+SsqN4I8AuV5auVpS2ulQNs2IgojaCOOxXoBIygmfdaKYQRZ5dxfYTCCNmIOI2gjjsV6ASMoJn3WimEEWeXcX2EwgjZiDiNoI47FegEjKCZ91ophBFnl3F9hMII2Yg4jaCOOxXoBIygmfdaKYQRZ5dxfYWEeQafJqsxC93mHiX1AxWhbeLGc2QYjyKnmDstK4xGsIZD5K0XzaDuMwKUAjGCN3Gy4jmgjcDEt387Y/Wl/1FRlDcZxLu17pMi+U4H9xKRjG/SUXYNgzTkHEo+mBMs6Pi8/2wa9OyXtYAQpauFaS4FUI3geVgpiQC7hXKrlaK0yjcDBzfjM2M9u5L+zuIIEVeh54hJwYC4Fv91eosAIthezqloc3iMwYKEqoszCh01Pa5VGpBGSJOfSi0x37BEQeHU+qOUT3mEELs6Gxi6sKhyzGwMjmC0dbuwVSJ4RqBuHWTiXZhz0GQETeDTVWDQE/nA7YQTmi3CU6uI2ILeRJzCCbcSp2lbmNoJ0ziVtBO+MWYc0BjmIgzMCB4m72mAsaBiMYIF4uDX/jGDEg/k4lz4k3hgMerPw9rWH3Hb7k2+/2deRSzht/unG0BdqvueizajDCNqM62q9Cu8RDLt67M/tK+f3BZYGQ8MpzuUMIxi/JRjqFnxEodD19TX78eMgjMGo28EZnITFZuFqSZarIvz6LZeSdjnQ9njarlkyZgRrqt1gXTCCNoIKI2gjjsV6ASMoJn3WimEEWeXcX2EwgjZirhnB4XD403fr/Pycdf/dRg/RCygABYIKqGP+5E/319+hukOwhA1cgKfW8YIEbY+n7ZolY2mwptoN1gUjaCOoMII24lisFzCCYtJnrRhGkFXO/RUGI2gj5jCCNuJYrBcwgmLSZ60YRpBVzv0VBiNoI+ZNGIE8ry5DYh4PRbIuS1aNB/D1nVYYtF2mbfBu49wFefTZe63kLCi1GRzJ7RtB3/mRONOfV+lPl52zp+50SU+l6v+QrMF081zAT94dHp7Y1eMle7zqkV4wgrlqpt8n9Re6D0eilQNSWoHUtdwIXu4EGcrRkO0bgdmpwRUf2dVwzBRGkJ58rjt4IsEI8qgZXcow8JnyUCMGNHntHo3AEgQzgujE814II1iuYXoJw+z28Yq9DXzF/s8fB/paE8xqw1cbmxG4hcLSID0J9TtgBEsVnHO/Pbh7APMJuz+zl2jJ1xrL56aMYKDLHB4UB8XSYE4C2vfACPLomFZKvhmBvq8jqUvdmoPj3bu/ZozAZwJ9JzEjSEtAGMFSvTLdn22PwGiPYx+tASMQX40weyYguw8jWJqYmBEsVXDe/fybgPHJrRmDyPvzHr82vLXFey378oG93k6b566H5vaNwIebVr4nhRHMS8MJ3Knfr6LBoe1cbSPv034boG7ymUbQ7yX235h1XMihaOVac4w43vOwfSOI0BPJGiHSzEug7UzhKrsNRlBZQLbWHBjB1iLmbi+MoI04FusFjKCY9FkrhhFklXN/hcEI2og5mIVtxBG9gAKLFACzcJF8uBkzgjZyAEuDNuJYrBcwgmLSZ60YRpBVzv0VBiNoI+YwgjbiWKwXMIJi0metGEaQVc79FQYjaCPmMII24lisFzCCYtJnrbgJI9CZhfZ765Gs83MG2hrapTAEu1sp3mNf8nAA6Ed3MuBhYgyEGJz659OZAv99e2AW9qeuPryyW0FxGYRl8kQWDyKMYK4RQFtduTkMQT/vsXMBdnLP2DX7wQ6SBxlgcLoAJEMbyft2iCpzCQUjmGsE+n271zaFDzBK5zvCLQfnGzu7d/MgxwE+Mjh7I7pnZwqP0xtZjTmwOyMwHFuoBCPIYQTQNoUYNCnuNoKhrJe7jiVw6gXDDmWo5iMGN7t4Zs/8rHE3ndBnv2O9LnaBvEc9oixuaGKPgIvVLbQ8wsAIFhgBtJ2e7RZM1M8QJI1gGNAv7G5A7vuhLxas1MSZi/2KcwU5xuulynQj/9swAiXP8V6DBYM+cOvetc0zIzCn6f5Ba5GEPEsTEzNPYft4iA2aUfcvzRkBp7TgvQZHsYO9a5tlj4APQjF/1cOkkIOcg9nS3zaRsAn0EwZ7jGzfCPpOfXvPfgsc696fWlkNANoacqYwBOWt9DTdnsZTDE6jLG2p4L/v310wC8V66IbYCMEewVx7ML9/tl+MsTttoxmC7ie/+nsB53o+xOD0/Y6Buu/TP9MeWl8pmIVzBwTu8ymwOyNoNBW2vzSICAySNUKkmZdA25nCVXYbjKCygGytOTCCrUXM3V4YQRtxLNYLGEEx6bNWDGZhVjlRGBTYpgJgFm4zbtW0GjOCakKxqCFYGiySDzfDCNrIARhBG3Es1gsYQTHps1YMI8gq5/4KgxG0EXMYQRtxLNYLGEEx6bNWDCPIKuf+CoMRtBHzpozAxX/rw4RkXZasPkZe9dom8AXncgKlss7c037/b5zToNpmnBswzyd4OYjRZdpnRtoxAhf/TUQJRjDfCLyMvOq1TeALzuUETi5gswdNiIh2hJlqm4Ej044My/tcHESizLce3HNgDxJxZh2nboZHQPPfYARzjSDMyKtW21nsAKGTNfgoTqA792wDVbQcBiZjTwOhqP9TYCWnKr2o/8wVA8fRZqK/t68dFu3xir0JwK+rzCZmBCH+W7XJOnd8rnVfBCOvVm3n0YSEsAmcQG/uWQNzYhnYA1Mf2HyZ0WP5b9nrBxfY1DYCsr/vvxnG0yKhKIL/VmuyrjWeZ9cTwcirVVvXkqYfYPdn0/sD3LpQ8I/+4d0/sW/YwAnUnt4eWtCIIrpgFx109PzuD7ONwGYfyj0HFzvAxSQM9Xcsb+j01BbB89k6qiyO/1Zrss4eoGvdGMHIq1XbuTOCeE4gX6e/dAObD6YQiSh2RvA2oMy66cBQLt8YZNP6XllKqKzCtP42NyOI47/VmqxrjefZ9UQw8qrVdsYeQRon8IGd39wE2YOj9mp7ktbzLoNJ2yOQT31nW8Q/NrFHMCW625WrTdbZI3StGwPT5K4Z9WqbwhecywlU40DNCMx3QhBtM03CiSx31UX1V22n+/0UMIK1xtRW6wl8F1+vEUzreY6zVL87FwP/XLwcZC4nUIupMTgN3SxWoZd9OL0PURY/3RvgIPrKDLWlq6gxI3CPtqqTdasGIdoNbTcewDaXBjCCtdMSRrC24sepDzOC4+i6m1JhBG2EGkbQRhyL9QJGUEz6rBWDWZhVThQGBbapAJiF24xbNa3GjKCaUCxqCJYGi+TDzTCCNnIARtBGHIv1AkZQTPqsFcMIssq5v8JgBG3EHEbQRhyL9QJGUEz6rBXDCLLKub/CYARtxLwBIxC/G+c/KOd/1+I35OJ/kazhZJ3FweuKrVrbBGahVMjJHgyU49NuLgdRv68HlEiSUdfKyLbw/uhsQm+MuyubMYLpXLid9FUna3iMHvmKmRy8d7xZ9WqbwCycXMBmDw6osEt2eBBAEw3WQmg3m4PYlfnhld0KrNhgTEw+2Og++fmSVIx552EERx5m2yl+3hn3ao0gmUfg4V5GlROCksgn+SO7GgCiYRakzBttcJNtiSnT385mjOBmXBrYqOZqk7Uql0jk4H3lU4JatU0j9ggS0Msd+/P9lP2tcALjyokwggQO4pQW+gyAbMvHXx1C7bFbDTyzZzkWjCUyRVFqwAj00cTXQefauqrWZK3KBxyorRAHb1tGYHMBR/0J7mWMBmFMWQIHsceeqXwEZTCTbRkApQqyPBpo0szSwBxONo8NRhBjOXudEdDcyxwzgngOog1WVR9sp/1DTsOSKzGzSMWRiDORGs3NCPiuqlyL1b6hFTNA17pmr3sEAe7l3Qs79b2DgL+UoN8A0JYTasTSOIgOwrKaz8H3Iah5vzMj+PfLB/Z622/AcPldwmNGEGMm8zh49WqbwixU9TF1iGEB+gfdDXtQXiziMQ71mwjWPci+vWe/BXFUX+qG+zSSjU0UfcCwtj8jMHlzF7bw9SZrzAA99jUzOXiiWVVr6+UCGsxCTWLHoPaWQ2g3TNXHlxpMNcg1v/f3AObvYqh3Jsa+TzEQ46512zeCiHFSdbJGtL/mS6BtzdGJbxuMIF4rXOlQAEbQRlrACNqIY7FewAiKSZ+1YhhBVjn3VxiMoI2Yg1nYRhzRCyiwSAEwCxfJh5sxI2gjB7A0aCOOxXoBIygmfdaKYQRZ5dxfYTCCNmIOI2gjjsV6ASMoJn3WimEEWeXcX2EwgjZiDiNoI47FegEjKCZ91oqbMQKd86b/BhvJGs4ZN8+uYR5kNM+Q1oDkC3ayO/NyOEXoOIfABJ+QaptxtuZCQ6h5ynxi7JKqr2tnE0bgZ7XxAQAjoIyA4tmZZ/XtcrapbQrPkNKA4gsKE9D4Af44TGxCFmAk3rOzAXc2uIx15F6tQecd6nWbnzVgBGFW2zaTNfwUz3uF/yhtc2DYKA6hVDdshuOVGjgknJdj/JKYAy/sbqQaE3VQJuH4bPtGIDpFsdpgBDGWQZypb4wHGUcd0o2AYmLyK41ZRkReyhrUp3OobRy33i8hbtmrwlY0I5wyG+jv3b4RmAAGB6sNRjDXCPT7WuFBxnEI3ZpZGnj4gpw7GGIITgbSjWzWs0hi2ibfvcAc7A3VlGSZxqJgWHqYnzViBH2/5Esg7CcbjCCPEcinnppEW9Q29NSl1bKZmNrSQIJzPcuPkSAkbjLbQrftTRvE3JQYe5B7Bp4y1f749tO2bwTWegdGEDPs7WsikNyt8CCT9ggMpWLX3j1yTGNnRqDghglCP5OwH2z9Ps3tKwEvlaw+sURJmQ20sTQwwZEOVtsWn1rzBvOSu+xEbZcHGWb/3ZzztwuRGvSm4OULBhDmXahcSy1r1qUag2kSjmWwu0yeF9Rn258R8B527nvDpvc68PWW/IMRUAaRwN1riQcZyzMkmZgpfMH+lZxqXhqbi/r8Xcln/Tcx4/6AuH78HcHw/0SZ5GctbBZGPARhBBEizbwE2s4UrrLb2pgRBERFsh4v66Dt8bRds2QYwZpqN1gXjKCNoMII2ohjsV7ACIpJn7ViMAuzyonCoMA2FQCzcJtxq6bVmBFUE4pFDcHSYJF8uBlG0EYOwAjaiGOxXsAIikmftWIYQVY591cYjKCNmMMI2ohjsV7ACIpJn7ViGEFWOfdXGIygjZhv3wjM34KPcRH8t+7/kax0supcPfM38RN3T/9dOy+zam2juYS8L27uZYDbqOWffi5Aqi7PB6j6kaxDH5fQauOU4/zUIsFBFI1xtaX/aPtG4Mhxk85SdbKWfqD0g+UzYz9/f2Ucg9efcT8XfAeKZ1i7EaRwCSm+IIEqM0+6WkeIO436f7vvzJX9YIerN/Z7OC5MsQ4N/Jh27JlmJJqpZFGKnG3hd7VnBK2cmS9lEM7z9n5WQbUmm8QcoPiCfiOwIR9mOfLeN3Z2f8pMKIkMsVbOoH8cl5CE9no4HS937rY0ZwQuVlu1yVpqsFP1up5qJvNBub9WbZMoRCRfkDhqbGmlcw6GNrzcdVyDU/a3ly9oHx2O4xJSR477iUjHNmScqTAue4i2NGYEboxUrclanw/4nvzbmxHEsP9G/aP5gjbcQ+cDXLCLi2d23tGEvp+qT3aHfj7WobGWt7iEgfv47cY40GYZ7lg2ZQS+qRKMIM5yhqQ+PLA3sV8w3dWCERAotki+oHOQadLKAcin3xMGnsbAufZlNJipg0s4PuXH/RwlWhpW3VzaNG8EfqgkjCBsBH4TGFLOO7WtVtuUPYII7uWoIMUsHOv8xP7pwIOuPXwneVgp8+OvGC6haI2zLeY4cBOohhIU4lQzMwKKx1ZtsobH5wpXiDUwc80Exu2s7RmBOT3WjEH0WXAJLaNTlgrvv31gr7fizUL9pNs7a6LW7IaRUqxDikvYLze8jEQeK2ociCucsWzECOiNExgB4Se+75+v+40m/vot88mmfh9etbaxXEI+gtzcS4pZaNzj+p2Fe/DRrEM/lzDASAxwCXdgBPSDs+pkXeGZf8wqoO0x1V2v7EZmBDCC9VJGrwlGUEr5vPXCCPLqubvSYARthBxG0EYci/UCRlBM+qwVg1mYVU4UBgW2qQCYhduMWzWtxoygmlAsagiWBovkw80wgjZyAEbQRhyL9QJGUEz6rBXDCLLKub/CYARtxBxG0EYci/UCRlBM+qwVwwiyyrm/wmAEbcS8DSMgGG99mJCsdLKSzMItaxvLLIzg/bl5hibnMJL3GKjvdMDFPYugKVxC8S+yLa6zDe7PAtzFrtwGjIBivHHlYASEEQSZhffs7E2cvtsUBi6NWWgqpBJ+vEiwoHaX7PDwxK4eL72YMlnvVF930OvDK7sVTAidNEQxJKnPCO6iaMD2jSCC8QYjSJi+qoOdhfl51WqbwiMw5TGBoR19dDRDSspE3uNYFME4cJsQBTpxfbYHI+jUDDHeqk3WhPG52qXGANqqtknMQkNc7SlM8gytGzukOBMEaPkZTSfqr3JxNvndvuP184xgXG0wG7u+/RmB0Hs8w61QV2QoYASxNuJOsC1qm8Qs1OQxCD/RPMN03qM62CWajI9/5R0FAxeCA0inv1QjMO7WkPX8swaMwEGONRhvMII4I7DpO9vVdu6MwLovkmc4h/fYR4VEksvPLS7hMiOw4KYtGEFMwGEEYSNwJfKmtZ21R+DgXkbwDOfyHl0D0opU8r5DeCnCaUyP7EpuArdgBHwapazLxFdG508dUlrMqGAElBEQzMJNa+ua4ss8MZmFXB83788YWNpSYRnv0VkfxTMcw5g2I/j3S5i72MDSQG4WTslufr8KIyCMgGQW/iU2YjeqbQqzkOL9xfIMpUxRvEd6I9C9secmEvN8f/PzJd9/09+L6NhHa8IIQhNfGEFIofmfQ9v52tV0J4ygpmhssC0wgg0GzdFkGEEbcSzWCxhBMemzVgwjyCrn/gqDEbQRczAL24gjegEFFikAZuEi+XAzZgRt5ACWBm3EsVgvYATFpM9aMYwgq5z7KwxG0EbMYQRtxLFYL2AExaTPWjGMIKuc+ysMRtBGzGEEbcSxWC9gBMWkz1pxG0ag/V7ehi4gWemc0Xl8fkYeL0XXt2ptY5mFfbcCbEZ+yQm7/NEpMPy2/92gBsV7nPsZPx14w0ZqoTxAF2IrevuwB2ahCY6wjp+CWUjbQHeQxcvIC5+Xr9cIUpiFYe7lYBT3HZyU/WCHK2EEFLNw7mfm4Sczv41gaqxDFammHTXeAarMPjNvBLUTrt5kzTq7y1KYrqetpVlJtdqm8AiC3Es5kN7Y2f2pH0RKsAddDIBRS/W+N+NYPSMGcTRfcgdGYPEIhKOq6KdqkzXL0M1ZiPEUjeD11aptDFRFVY5iMw5lvdx1yLBT9vcHwggcs9GxjsjPBpT54xV7ExTjbk7mrdNkHfr7YC4NGmUWjkw9sYa9uHhm53cAk0RbhI+RF8Hr244R8DX+/dm0vjf1cbIZtdlCGhBkKj/+vnjWooOmpOxjdBsZipnoPXUBUdrYLNT6aQtUa7JGD9QVL9SSJILXV6u2aTMCH5vxJ2OfT9nL+FAJPJ0P7sFHoczMz2Lb7V4SD6iugczF48jYg4Ijm9LIHiPtGQE2C5fZhrXuVNl29kCo1QjsJaN/newffA/s/Oam2x50/ClP3JSBbi1HTPOI2tuwB3KsgYivOhpkFjpmAwfl653+42qTddmQzXM3ycijeH28+nq1TWAWRrAZeZoCaX0AABVXSURBVG9NI6SYhXM/o9otWuHAkVPszk//7IFZaHzn6nofXL3JmmcsLyslsJEU+C6+am0TmIX6PpP+WwHvWp/iPX76R+cEykJ6niH1GZ/XK78jMDf2fKxDgt1pthPMwmVDBnfbClRtBAhYtALt7RE4uo5kjc6H5AuhbbJkVd4AI6gyLNtpFIxgO7GiWgojaCOOxXoBIygmfdaKwSzMKicKgwLbVADMwm3GrZpWY0ZQTSgWNQRLg0Xy4WYYQRs5ACNoI47FegEjKCZ91ophBFnl3F9hMII2Yg4jaCOOxXoBIygmfdaKYQRZ5dxfYTCCNmK+KSOQDDjrPMGWfw9fUR7ZTL4w665qI5jNLDR+30+V4/2M0i6gq4/BGWIWilwKsxVtLuVGjEAetHhiV4+XBioqzKarOllrMQIXk4/CZIl216ttOC9G6UnuJVUO9RmFByM+i2BwqiljUoqcbMWe2kVwKfvyNmIEsusOMETE+e16k7UWF/Ax+cKsu2q1jciLMassPJjCaqQYgoziC84zghgG55g1FiMxjq3ooiBt3ghigAzVJmslPuBn8oVZd7VqG5MX+oygh/t8Z90h4O5vYgLcvvoZgj/ZZ4IvyAbW4I1kkmsYeEJXy8DcSLKhlT1enXXHmvujy91fHFvRfYy5QSOw2XS1JmsVPhDN5JP4q3NlwNQLJoln//Eo+LiXthFM+WUbgZ+L6OIEajOSm0nXEINTtJh9OZnQZJxh8MLuBjPzzZwFa6lnIgjzkG1o0Ag2hNMq7gTm9JWCbOpPSplHtZps0ozAikOOGQF/Acr053+yqzMQY3xqsxP1M71/aXFsAF6KPYK83sGTM8Tk869J650RpDALbR9Q1v7UXgO5R2CUOvedB04Mumkqy+O4+RmB5aaAly7wCt1o//0SZt3VOiOg80Ks0c/tKbK8b+JeUgxB/2fdNj17vf3NxJvR+NJDgErfInRVp/8mg5NaZvD7jAcmyaXkd2zECNyON/6egGS8VfzUWjBkj3OrC1aqzBccrLt6jaAfDz72n2EEIe4llV++zyhOIPVZqC3mK9GcieCBrDo3LjdlBMvSvupkXda14ndD2+IhyNKAjcwIlvUVybpMP+puaHs8bdcsGUawptoN1gUjaCOoMII24lisFzCCYtJnrRjMwqxyojAosE0FwCzcZtyqaTVmBNWEYlFDsDRYJB9uhhG0kQMwgjbiWKwXMIJi0metGEaQVc79FQYjaCPmMII24lisFzCCYtJnrRhGkFXO/RUGI2gj5psyAi+zsIsF9RmSlUrWBWy9rtiqtc3FLBTyuViA+nkGxq6f/jDzKLHzPiVnefE6J5HK5/5qX5nuz8LsyY0YQQyz0MUz5BJXnazFHyjzkFqy2fVqm4tZOLoAO7nvBnp3aPtw9cZ+D8cKI+pwsiDFg+vxir39/sp0cgGV61RbqM/CyLmNGIFMOwqc4f+s3mQt7gL9c2lAar3c2U8y+rPKTTYXs3AYpR4WYLAOH0NQYSKa/JIxJXz5THEJ57MnYQQ1jMWibaC4hGAW9tN8HwswREHyMgQFpIRdPLNneTTYwoe5jYDiEi5hT8IIig7C+ipPYevVvOzKxSz8fupnAZJ1fPzlZwiayHKxl3Gu7S84jIDiEoaYhUqqtYkqC06lsEeQZjdpbL1al12hpzWtidTgjZ3dq8smfXD66+D7VdNyywV8UanJrqe/Gy7iLpMTk731WZ21Y4wZQdooaf/qRLZerUaQh1n4if1DMR3vXtjpABKWGHS5Rn9i7JJgQf5k7PPpI7t6kyizGCOguIQX3SqjW2a4stNBleLfdKj1bwZVhs3CYzkQxSWMYetVawTKuwmGr/O0jb0UZqGqvDlgKZ4hdZ9rhnBgD6MxTBuUj+M3FGYGxG+cx7AnNzIjoJiFb04Sr/p+xHqT9VjDO6HcuWw9UUXV2uZiFlLLzwAvk9/qW+/fjE/x6fcHAT5nxFLYqo+KsShvI0aQkNiOS6tO1mVdK343tC0egiwNgBFkkXG/hcAI2og9jKCNOBbrBYygmPRZK4YRZJVzf4XBCNqIOZiFbcQRvYACixQAs3CRfLgZM4I2cgBLgzbiWKwXMIJi0metGEaQVc79FQYjaCPmMII24lisFzCCYtJnrRhGkFXO/RUGI2gj5jCCNuJYrBcwgmLSZ614U0bg47jJf5fKmNw4JGs4Z3QN3fw8Xor+WdXaZmQWztUnLmevlROMU6zSuIT8Pqqd/ee+MjdiBATHrQ/2Z8Z+CvabC7pQdbKGx+jRr3ABNmSl1Gf9NfVqG8ETlJ00QSEGgmyePgHO5odXditydhic7In9UamnHtbh0ORkDqLoKFHmRoxgTMsBwOA/mjlYonXWut5kPfoYj6iA4ueF2XrVahvkCU7S2ANd7fcyfZwnD42o+AAnL3ccjKLn+1wOIsU6bIpHoLieBouo+akVMU6PfQnFz4tg69VqBEmEIss0FM7AgCp77FZEDr5ghD5hIzBmLnJ6/3LXzRBOrQffXA4ixTrsU6yxGYEb1lBrsh57jEeVT/Hz2Bd2cqkAMxxsvVq1zcYspDSI0MdrBCojQAWXzuUSUnEkuIsyR5oygmGtdXiwWPG1JmvUQD32RZ4p9DAdff+tMwKarVertkkzAktjZUYwDHaPBhH6hGcEcoPvvNswvGWvXvZggEvojWOAnyj63owR+Eyg7g2tY4/yiPKtPRVlVjWQeGm2Xq1GkIdZ2LEIF+oTYwTTvtYde+kgiD9cYbsIcAm9HMQHdn5z4ylzemg2YASCP8fsmYDUs9pkjRinx7+E4ueF2Xr1akvxBFOYhcv08WLKvr1nv8W3BH6EfDyX0I0nMzmIfTa5y9yIERAct2F65vBQZd1Vb7Ief5hH1UB93x74Lr5qbXMxC2fpQ7EH+TT/ZsQO67/NUL7PIL4lS+EgqlmwaSOISmfvRVUn67KuFb8b2hYPQZYGbGRGsKyvSNZl+lF3Q9vjabtmyTCCNdVusC4YQRtBhRG0EcdivYARFJM+a8VgFmaVE4VBgW0qAGbhNuNWTasxI6gmFIsagqXBIvlwM4ygjRyAEbQRx2K9gBEUkz5rxTCCrHLurzAYQRsxhxG0EcdivYARFJM+a8Uwgqxy7q8wGEEbMd+UEczlvyFZiWRVz8VrlwmOnvH5xUN3PPnru/HKqrXNxSwMaODMy5Cus84vTAHysQd9Y6S/k+IZbsQIlvHfqk7WCh8oE0OPsS8n9+zs7Tcbxv6mMHC5mIUGqkzTgMpLO7C6rpfsIE1Vg4pEtNvJHqTbEmJPbsQIpKjUsUx+javDMIIEt1ETnfWYrhd296c7lz8UYfP7qtU2F7MwQoM05kBnqm9u2MnL3R/23QNCGT4bgkCzB91tCbMnGzMCm//WS1dtsiaMz7UuNYm6fAraLxM4PccEx9aqbRKhiGIWdoMvpEGMEai6Um37yT6z08crhbKlP/xC7EEv/8DHXRSJ1YYR+PhvopO1Jutagzu+HgPmIW6U61F2sR0MXDZmIZ8Kje8DcGkQNgJdV6ptthHwuu/Pur2ZgRglZ2i+2bHj3ymeoehfG0agZDreaxA/7M0r3VjvAdg3TEu5tow9yD2DimdbSTMCSzJ14DoGsaFByAjMtsybEcSxB51tobiUYuO3OSPY1obW/EGb/057NhAzmKqdbSXsEdg+MK3hT3vzI6bqYmeKIAk5ZllU27x7BE+MXfp4hupMzUcuotmT2zeCfnMrwH+rNlnzj+bZJTq5eWbCbghnzjc2p9mMDjNNYBZGaEDNCNw8Qqpt1GdqeBOWBian0FwqdMVuxAiW8d9gBCF/cG+yamtjUcR2f0egcgENIzC+0zf7OO6RWBpQedl/3+rXlc9cbxjHFhrMQuqzMZSmEQTaEvhNxUaMIJTI9OcwgmX6UXdD2+Npu2bJMII11W6wLhhBG0GFEbQRx2K9gBEUkz5rxTCCrHLurzAYQRsxB7OwjTiiF1BgkQJgFi6SDzdjRtBGDmBp0EYci/UCRlBM+qwVwwiyyrm/wmAEbcQcRtBGHIv1AkZQTPqsFcMIssq5v8JgBG3EHEbQRhyL9QJGUEz6rBVvyggoHluvio/jhmQN5IzG1zN+997dSuletbZHYRZS5wIYuxZHtqXiXk5goG3p94nzE/zwAv+7fmJ/ONZo/PONkY0YQQQbzslx4/2vOlmz+vqMwsyTaNppu7Du9Wobwf6TclEaROkjgK7GtX5OIN22efdJhJlEmjlygRgjGzGC0V89575pjlu9yTpj4Ga+xQ0jUYClQ31+VmS12ibwCCgNPv4yeQQK/49iD/5FcALJts29L2QE9BhpwghCHLdqkzXzoJ5VXIDXx8vcnhHEQFWU+TLj6AIV0ipYBhYoZOIF3L4S0JIBK/bYnTB+Zs9yui6m6mTb5t73lQ0PyZtxaaAvYUJjZPtGMKy1aI4bjIC2CP28/QW76JL3fKTmtmIECvtPeS/DtDroIa3y/3QNfPrYRqDU8f5bZy6HCeumQF2Ocp/RJw2IEjFGNm4E5nTI/eSCEaTMFVwA08ZnBJY8bogrvyxyRjAYgTrLmDQkScVz77PMTbbzjZ3dnzITh27SqDduBG4qyxAvhbgLI0gwAmupsM0ZgY4mm/owDQhCE6cG4nr1M2qtf9rPVD2cQM9gH9o29z79ywHlZTR37OU0zDrcuBGYwcSMIGHIOy71obW2NyPIxizUVDL1CXMRxyev421Gkg7t4imm3se+fGCvt+KNVP28pX8fxcHGz/v2ezZiBCE2nIwWjCDZCEheX1j3qmdbXvZfArMwwDOMZw8avzGIZhZG3me+a9HxDgpq43cjRpCc3toNVSfrsq4VvxvaFg9BlgbACLLIuN9CYARtxB5G0EYci/UCRlBM+qwVwwiyyrm/wmAEbcQczMI24oheQIFFCoBZuEg+3IwZQRs5gKVBG3Es1gsYQTHps1YMI8gq5/4KgxG0EXMYQRtxLNYLGEEx6bNWDCPIKuf+CoMRtBFzGEEbcSzWCxhBMemzVrwpI3Cz88KsNiQrzxkve9DLz9u4tkdnFob16XX3cQK98TDODVw8CBQaL6w73jyCE7pDtspnvv6a5xBGC7keYSwbMQKKnRdCNIFZKE/iHR6e2NXjJdPPolP8vC1ruwazMKzPMHDvu4ND7Ac7XMlBS+WzgSobBrc8zhz67JIdpDGYrEVj/jCYE5vgphsxAtkL1+nCcDAwIyD0I/l5G9Z2DWbhu5A+NCfQeSRYowkNUwD2pXOSs7fuiDFTaVzGZyQ/0XABzVz4Z80YgY/V1ncSRuA3AprtR3PwatZ2DWbh97/MpUEaJ9DHBuBLiX7afsteOw6hOoPzfZbSX3M20IgR6G6nsdrERzCCFCPws/22pK0LCd4PgPszZU2tpA7FbQwzHdU9mHO+7o7gBFJQ2LFOB1fA9Vl8f90YtgZmBObmqd1RGEGKEfhpRBb1p+LZVsoT0swgVz+na2J4hnGcQLcR6OVz82UCgur/zMauu+Poe2dCe0bgWP/ACObuEYTXltVqm7BHYBlBLLPQvHHMvThOoMsIKAObAz3VGY1+E9u8EfwbwWqrNlntR9GR/8X1lPBz92I4ePVqG+YJ3pzbrwSbvmFxLSH0byJick8sGsgX82jf4pgmpGDQv5vvWFA/G16oIt7F0INMHWbmWtrJhNuIERDsvIEIO32vqtKLZSfrTdYjj/ux+AB70MfPi+DgVa3tsZmFEfq4jYCOh74n0QO5J1OiPiP5iQLDPn69aKTeRoxg2YCpOlmXda343dC2eAiyNABGkEXG/RYCI2gj9jCCNuJYrBcwgmLSZ60YRpBVzv0VBiNoI+ZeZmEb3UMvoAAUiFXgcDgMl5786f6G/zg5YeI/Y8vAdVAACmxYgV0sDTYcHzQdCqyiAIxgFZlRCRSoWwEYQd3xQeugwCoKwAhWkRmVQIG6FYAR1B2fxlsnf2o7IbPiO7zk3vha4q+srT3xLTe/IMC3BmnaHelqBwevq+n66Q/73h8mWfvP4OBNv3uX7YwdxK7rYwfPknt9grl+7x/bl75Ms+2xfVk7gHH1YUYQp9OKV6UOsCM2TZrACMfo2/aNvf/dgTdYajtTr1f7teTegBHIvll9Dem67YFv9g5GEIr36p8TSS+SlT08sPObmw6HKZ9g5tNNQWaN93BwqUS69TOMT//0SCzeQeeMQ56uuzaP6zpmLXJAOSm7JvKsq3C4/iP71aG4bp6nJ7F2um6o93Q4wjuh6Pz36qfvpj7ZZQ7ndIdjuz9Gk1MHNkeE3Tyr6DHqeocxLtFd5pyXwpw/3jCC1Qd6qMKwETxrA9d8Msn7RRIPwMsb9szM/5fHW9/4gBhNRWmfmogWMsvRzv76z4z9/P2VvbNmDK5+Gf8mTEQuP/77+wv79fE7+yogoaphSKjH9G+Ogfrhld3evbDTzu3sMo3rZV+F6VmocaNt9tLA6Muo3QzdhxB4+vP7PfumxStPvGEEoXG5+ueuPQIjmdRBaSWo8f6Cj7+4EWjTe9dTWAdvjt02nkoT7yE8XedPYlluvBHYTIn4ezXO/zCexHsALCNzMwGmmZE+EN+0vigDdTRQjxHM1d0RV+4PvD9qPzXTmhlvGMHqAz1UYcSMQJmqO1+SoSbLzMSwWqkYAk9COd3XN9hMcEY37xC8vYjBPOT5tFzp3wYwgECd+xF6eacD2+/ZucRxl2k+cScDlmYwGdlPxj535sn65Uw/2zm+Eci4mku2Y8UbRhAal6t/nmYE1BNiSKJTsTSY+2RS+6/tGci1+2QEZvImzwi0VQkf2H7TcRuBNSPwlimWRMpMwbccYBcX7OK5W5CpbxWyvjXIOyMIvQXJNSNYEm8YweoDPVRhohFYa8l8Cdkn42f2s3v6i2egeFrbT8zu5RvdJS7ibjeJFTMC+bRXlyD2YB7rM6bAuqn0Gob62T3xuz2CT1eP7F72QSvTNgJ/HX195tLJN6MQxuj8xoVakhl7Pdb9vD+3co/AZ+zJ9fJ8hBGExuXqn6caQT8m5IagbKwyXZ+ZGLwke79Cm6pam4lyI4vffX3dvejrx2E0Aq2dzm8NjPrUbyusuuxvHNzMPl+ZoT0CoYBYckz7Fup9flPj7zVI2ZtxfB3pYy4eId4wgtUHOirclAK+TbtNdSLcWBhBWCNcsWMF5EZjsV92rqQ9jGAloVHNBhUYp+ApPz3eYD+xR7DNoKHVUCC3AmAW5lYU5UGBjSpgMQtbI9O21p+a8gza1hSN+W1R4zgeQ24tuK31Z364898JbfNrWqJEGEEJ1RuqE0bQRjDVOP4/EEZqkU5PTAwAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "5b962994",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087f4b2",
   "metadata": {},
   "source": [
    "The experiment-wise error rate determines the probability of type I error (false positive) over a total family of comparisons. Our ANOVA example has 3 groups, which produces 3 comparisons and a family-wise error rate of 0.143. When performing statistical tests, we expect a false positive rate of 0.05 (our set significance level). As shown, when increasing groups from 2 to 3, the error rate is approximately tripled from 0.05 to 0.143. This introduces doubt as to whether we are observing a false positive as opposed to a significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3dec1a",
   "metadata": {},
   "source": [
    "If we use 2-sample t-tests to compare group means, and set a significance level of 0.05 for each test, then the number of comparisons will determine the experiment-wise error rate. In post hoc tests, we set an experiment-wise error rate for all comparisons, and the post hoc test calculates a significance level for individual comparisons to produce the specified familywise error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55754670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import tukey_hsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916a6c4",
   "metadata": {},
   "source": [
    "### ANOVA F-Test\n",
    "\n",
    "    Null: All group means are equal.\n",
    "    Alternative: Not all group means are equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "extend3 = test['rmspe-extend3']\n",
    "extend2 = test['rmspe-extend2']\n",
    "simple = test['rmspe-simple']\n",
    "\n",
    "f_oneway(extend3, extend2, simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759acf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.boxplot([extend3, extend2, simple], showfliers=False)\n",
    "ax.set_xticklabels([\"Extend-3\", \"Extend-2\", \"Simple\"]) \n",
    "ax.set_ylabel(\"mean\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4fe99",
   "metadata": {},
   "source": [
    "Since p-val < 0.05, we have sufficient evidence that the mean values across each group are not equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4d67f",
   "metadata": {},
   "source": [
    "### Tukey's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce119213",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tukey_hsd(extend3, extend2, simple)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea73bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey = pairwise_tukeyhsd(endog=df['score'],groups=df['group'],alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad60a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'rmspe':list(extend3)+list(extend2)+list(simple),\n",
    "                   'model':np.repeat(['extend3','extend2','simple'], repeats=len(extend3))})\n",
    "\n",
    "tukey = pairwise_tukeyhsd(endog=df['rmspe'],groups=df['model'],alpha=0.05)\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3627be7",
   "metadata": {},
   "source": [
    "* P-val for difference in means between extend2 and extend3: 0.9831\n",
    "* P-val for difference in means between extend2 and simple: ~0\n",
    "* P-val for difference in means between extend3 and simple: ~0\n",
    "\n",
    "Thus, we would conclude there is a statistically significant difference between means of extend3 and simple models and extend2 and simple models, but not a statistically significant difference between means of extend3 and extend2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a211fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_melt = pd.melt(test, value_vars=['rmspe-extend3', 'rmspe-extend2', 'rmspe-simple'])\n",
    "\n",
    "test_melt.columns = ['model', 'value']\n",
    "test_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "ax = sns.boxplot(x='model', y='value', data=test_melt, color='#99c2a2', showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822161cd",
   "metadata": {},
   "source": [
    "### Source: https://www.reneshbedre.com/blog/anova.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
